{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Implementing a Positive Control\"\n",
        "date: 06-11-2024\n",
        "date-format: short\n",
        "execute:\n",
        "    error: true\n",
        "---\n",
        "\n",
        "## Introduction \n",
        "\n",
        "Establishing a positive control is necessary to validate that my deep learning model is performing as expected. This validation requires comparing evaluation metrics generated by my model against established benchmarks from the literature using an identical task and dataset. A significant discrepancy would indicate potential model dysfunction. The challenge is finding a paper to which I can compare my model. \n",
        "\n",
        "I did find this paper: ['Pedestrian Segmentation from Complex Background Based on Predefined Pose Fields and Probabilistic Relaxation'](https://www.scielo.br/j/bcg/a/s4LPJYBbNVDQ4ZcWprP4rKw/?lang=en). The paper compares an image segmentation method to CNN-based methods. They use the [Penn Fudan dataset](https://www.cis.upenn.edu/~jshi/ped_html/) - an image database of pedestrians around a campus/urban environment. Each of the 170 samples contains at least one labelled pedestrian (one mask, one bounding box). \n",
        "\n",
        "In section 4.2.1 of the paper's 'Quantitative Evaluation', they compare their method notably to a Mask R-CNN - this model architecture is almost identical to that of my model. For the sake of completeness, I will give an overview here: \n",
        "\n",
        "### Mask R-CNN Structure: \n",
        "\n",
        "Firstly, the network uses a CNN module to extract feature maps using numerous kernels. \n",
        "\n",
        "These feature maps are passed through to the Region Proposal Network (RPN) module that considers pixel k-many 'windows' at evenly spaced 'anchors' over each filter map. This network learns to select windows that most likely contain an object of interest and 'proposes' them to downstream Fully-Connected layers following ROI pooling (to normalise dimensionality anchor windows differing in size and aspect ratio). In this way the network only pays attention to promising regions within the samples, without the need for a selective-search algorithm which is computationally less efficient. Up to here, the Mask R-CNN is identical to the Faster R-CNN architecture. \n",
        "\n",
        "The discrepancy lies in the output heads. A Faster R-CNN network has two output heads: one for classification (kx2 outputs for each ROI), and one for bounding-box regression (kx4 outputs for each ROI). The Mask R-CNN has an additional output head that outputs the object mask for a given input sample. \n",
        "\n",
        "As this is the only discrepancy, I believe its performance in the paper is suitable as a reference for the positive control.\n",
        "\n",
        "### Goal: \n",
        "\n",
        "Validate my model's performance (Average Precision and Recall (AP and AR)) relative to the findings in a sufficiently similar implementation example from the literature.\n",
        "\n",
        "### Hypothesis: \n",
        "\n",
        "If my Faster R-CNN implementation is functioning correctly, it should achieve detection metrics (AP and AR) comparable to the [published Mask R-CNN benchmarks]((https://www.scielo.br/j/bcg/a/s4LPJYBbNVDQ4ZcWprP4rKw/?lang=en)) on the Penn-Fudan dataset.\n",
        "\n",
        "### Rationale:\n",
        "\n",
        "1. The core detection architecture is identical \n",
        "2. The dataset and task (pedestrian detection) are standardized \n",
        "3. The evaluation protocols for AP and AR are consistent \n",
        "4. The segmentation head in Mask R-CNN does not affect detection metrics\n",
        "\n",
        "### Experimental Plan: \n",
        "\n",
        "1. Train Faster R-CNN on Penn-Fudan dataset using: \n",
        "   - ResNet-50 backbone \n",
        "   - Standard detection heads \n",
        "   - Default training parameters\n",
        "   \n",
        "2. Evaluate using COCO metrics: \n",
        "   - Average Precision (AP) \n",
        "   - Average Recall (AR) \n",
        "\n",
        "3. Compare against published benchmarks:\n",
        "  - Mask R-CNN: AP = 79.25%, AR = 92.63%\n",
        "  - Other architectures (for context):\n",
        "    - Yolact++: AP = 92.20%, AR = 94.02%\n",
        "    - DeepLabv3: AP = 78.06%, AR = 92.83%\n",
        "\n",
        "## Reference Selection \n",
        "\n",
        "The paper \"Pedestrian Segmentation from Complex Background Based on Predefined Pose Fields and Probabilistic Relaxation\" (Caisse Amisse, Jij贸n-Palma and Ant贸nio, 2021) provides suitable benchmark metrics for comparison. They evaluate multiple CNN-based architectures on the Penn-Fudan dataset, which contains 170 images of pedestrians in urban environments with pixel-level annotations (masks and bounding boxes).\n",
        "\n",
        "## Architectural Comparison\n",
        "\n",
        "### Base Architecture Similarity \n",
        "\n",
        "The paper benchmarks Mask R-CNN, which shares the same fundamental detection architecture as my Faster R-CNN implementation:\n",
        "\n",
        "1. Backbone: ResNet-50 feature extractor \n",
        "2. Region Proposal Network (RPN) \n",
        "3. ROI Pooling layer \n",
        "4. Classification and bounding box regression heads \n",
        "\n",
        "### Key Differences \n",
        "\n",
        "The main architectural difference is that Mask R-CNN includes an additional segmentation head for mask prediction, while my Faster R-CNN implementation focuses solely on detection. This difference could potentially impact detection performance through:\n",
        "\n",
        "1. Multi-task Learning Effects:\n",
        "   - The additional mask supervision might help the shared layers learn better feature representations \n",
        "   - The model must balance detection and segmentation objectives, which could affect optimization\n",
        "\n",
        "2. Parameter Updates:\n",
        "   - Gradients from the mask head flow back through the shared layers \n",
        "   - This could influence how the detection-related parameters are updated during training\n",
        "\n",
        "However, these implementations still serve as valid reference points because: \n",
        "\n",
        "1. The core detection architecture remains identical \n",
        "2. The published metrics provide a reasonable expected performance range \n",
        "\n",
        "## Multi-Study Validation \n",
        "\n",
        "Two independent studies support the comparison Faster R-CNN and Mask R-CNN metrics in the positive control: \n",
        "\n",
        "1. - Pedestrian Detection Reference Study (Caisse Amisse, Jij贸n-Palma and Ant贸nio, 2021) \n",
        "   - Mask R-CNN: AP = 79.25%, AR = 92.63% \n",
        "   - Similar task (except for mask generation), making it a good reference study for the positive control \n",
        "   - The frozen COCO ResNet50 backbone is identical to the one in my model (they also use transfer learning)\n",
        "\n",
        "2. Vehicle Detection Study (Tahir, Shahbaz Khan and Owais Tariq, 2021) \n",
        "   - Faster R-CNN: AP = 76.3%, AR = 76% \n",
        "   - Mask R-CNN: AP = 74.3%, AR = 74.35% \n",
        "   - Shows consistent relative performance between the two architectures\n",
        "\n",
        "These studies demonstrate that: \n",
        "\n",
        "1. Faster R-CNN and Mask R-CNN may achieve comparable metrics \n",
        "2. My implementation's performance (AP = 87%, AR = 92%) is similar to that of the Mask R-CNN with a ResNet50 backbone\n",
        "\n",
        "## Implementation Details\n",
        "\n",
        "### Establishing a Positive Control:\n",
        "When starting this analysis, I began by following [this](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html) tutorial. Here, they use the PennFudan dataset - images of pedestrians around a campus and urban streets (more info [here](https://airctic.github.io/icedata/pennfudan/)). I will be using this as my positive control.\n",
        "\n",
        "Download here:\n",
        "\n",
        "```{bash}\n",
        "#| eval: false\n",
        "wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip -P data\n",
        "cd data && unzip PennFudanPed.zip\n",
        "```\n",
        "\n",
        "\n",
        "The tutorial predicts both bounding boxes for each pedestrian, as well as masks.For this reason they write their code for a mask R-CNN They organise their dataset as follows: \n",
        "\n",
        "PennFudanPed\n",
        "\n",
        "  PedMasks\n",
        "\n",
        "    FudanPed00001_mask.png\n",
        "    FudanPed00002_mask.png\n",
        "    FudanPed00003_mask.png\n",
        "    FudanPed00004_mask.png\n",
        "    ...\n",
        "\n",
        "  PNGImages\n",
        "\n",
        "    FudanPed00001.png\n",
        "    FudanPed00002.png\n",
        "    FudanPed00003.png\n",
        "    FudanPed00004.png\n",
        "    ...\n",
        "\n",
        "Included in the data are masks that segment out each pedestrian. I will not be using this, given my model does not produce mask predictions in the output. \n",
        "\n",
        "This file structure can be found in raw/pos_control. \n",
        "\n",
        "Firstly, I will define the dataset class, taking the code from the tutorial and adapting it for use with my Faster R-CNN model instead:"
      ],
      "id": "836e928c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from plate_detect import helper_training_functions\n",
        "import torchvision_deps\n",
        "from torchvision.ops.boxes import masks_to_boxes\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "import torch\n",
        "from torchvision.transforms.v2 import functional as F\n",
        "from torchvision import tv_tensors\n",
        "from typing import Dict\n",
        "import torchvision_deps.T_and_utils"
      ],
      "id": "929e0869",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the PennFudanDataset class as in the Pytorch tutorial:"
      ],
      "id": "f90b0f3a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class PennFudanDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms): # I like their use of root, this was something I should have done!\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # load all image files\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\")))) # also, note here they sort the otherwise arbitrary os.listdir return - this was a huge flaw I overlooked in my code!\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images and masks\n",
        "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
        "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
        "        img = read_image(img_path)\n",
        "        mask = read_image(mask_path)\n",
        "        # instances are encoded as different colors\n",
        "        obj_ids = torch.unique(mask)\n",
        "        # first id is the background, so remove it\n",
        "        obj_ids = obj_ids[1:]\n",
        "        num_objs = len(obj_ids)\n",
        "\n",
        "        # split the color-encoded mask into a set\n",
        "        # of binary masks\n",
        "        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        boxes = masks_to_boxes(masks)\n",
        "\n",
        "        # there is only one class\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        image_id = idx\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        # Wrap sample and targets into torchvision tv_tensors:\n",
        "        img = tv_tensors.Image(img)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(img))\n",
        "        #target[\"masks\"] = tv_tensors.Mask(masks) <--- commented out since my model doen't care about masks\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "id": "1e221ffd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, instantiate a dataset object, splitting into validation and training, initiating training for 10 epochs given the dataset is much smaller (the tutorial trains for just 2 epochs):"
      ],
      "id": "a191b34b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "model, optimizer, transforms = helper_training_functions.get_model_instance_object_detection(2)\n",
        "\n",
        "dataset = PennFudanDataset('/Users/cla24mas/Documents/SC_TSL_15092024_plate_detect/raw/pos_control/PennFudanPed', transforms)\n",
        "dataset_test = PennFudanDataset('/Users/cla24mas/Documents/SC_TSL_15092024_plate_detect/raw/pos_control/PennFudanPed', transforms)\n",
        "\n",
        "# split the dataset in train and test set\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
        "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    collate_fn=torchvision_deps.T_and_utils.utils.collate_fn\n",
        ")\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    collate_fn=torchvision_deps.T_and_utils.utils.collate_fn\n",
        ")\n",
        "\n",
        "root_dir = '/Users/cla24mas/Documents/SC_TSL_15092024_plate_detect/'\n",
        "\n",
        "precedent_epoch = 0\n",
        "num_epochs = 9\n",
        "\n",
        "epoch = helper_training_functions.train(model, data_loader, data_loader_test, device, num_epochs, precedent_epoch, root_dir, optimizer)"
      ],
      "id": "f511430b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now checking a prediction:"
      ],
      "id": "f8ec3f72"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "save_dir = '/Users/cla24mas/Documents/SC_TSL_15092024_plate_detect/results/0003_pos_control/predictions'\n",
        "load_dir = '/Users/cla24mas/Documents/SC_TSL_15092024_plate_detect/checkpoints/pos_control/'\n",
        "model, _, _ = helper_training_functions.load_model(load_dir, 2, 'checkpoint_epoch_2')\n",
        "model.eval()  # Set model to evaluation mode\n",
        "for i in range(0, validation_size):\n",
        "    helper_training_functions.plot_prediction(model, dataset_test, device, i, save_dir, 'checkpoint_epoch_2_pos_control')"
      ],
      "id": "1861a811",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This indicates the model is behaving as expected. \n",
        "\n",
        "An example:\n",
        "\n",
        "![example prediction](../results/0003_pos_control/predictions/prediction_normalized_0.png)\n",
        "\n",
        "Based on the evaluation metrics from epoch 0-9, I chose to make predictions using the model saved at the third epoch of training (epoch 2 is the .pth file as epochs are 0-indexed in my code, currently). This point lies between the (potential) over-fitting plateau and the poorer performing model states at epochs 0 and 1. Something good to note here is that the pytorch tutorial only trains for two epochs on this dataset, which is clearly appropriate according to these results.\n",
        "\n",
        "## Results \n",
        "\n",
        "My implementation achieved: \n",
        "\n",
        "- AP @ IoU 0.50:0.95 = 87% (8% > Mask R-CNN in Caisse Amisse, Jij贸n-Palma and Ant贸nio, 2021) \n",
        "- AR @ IoU 0.50:0.95 = 92% (equivalent to Mask R-CNN AR in Caisse Amisse, Jij贸n-Palma and Ant贸nio, 2021) \n",
        "\n",
        "![mAP and mAR](../results/0003_pos_control/evaluation_metrics_epochs_0-9.png) \n",
        "\n",
        "These metrics fall well within the expected range established by the literature benchmarks, validating that my implementation is functioning correctly. The higher discrepancy in AP could be attributed to the additional mask-predictor head in the reference study.\n",
        "\n",
        "## Conclusion \n",
        "\n",
        "The positive control demonstrates that my Faster R-CNN implementation: \n",
        "\n",
        "1. Achieves performance consistent with published benchmarks \n",
        "2. Shows no evidence of implementation errors or dysfunction \n",
        "3. Can be confidently applied to new detection tasks  \n",
        "\n",
        "- Caisse Amisse, Jij贸n-Palma, M.E. and Ant贸nio, J. (2021). PEDESTRIAN SEGMENTATION FROM COMPLEX BACKGROUND BASED ON PREDEFINED POSE FIELDS AND PROBABILISTIC RELAXATION. Boletim de Ci锚ncias Geod茅sicas, [online] 27(3). doi:https://doi.org/10.1590/s1982-21702021000300017.  \n",
        "- Tahir, H., Shahbaz Khan, M. and Owais Tariq, M. (2021). Performance Analysis and Comparison of Faster R-CNN, Mask R-CNN and ResNet50 for the Detection and Counting of Vehicles. 2021 International Conference on Computing, Communication, and Intelligent Systems (ICCCIS). doi:https://doi.org/10.1109/icccis51004.2021.9397079."
      ],
      "id": "0b3dd63f"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/cla24mas/Documents/SC_TSL_15092024_plate_detect/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}