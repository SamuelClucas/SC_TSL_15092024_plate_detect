---
title: "Implementing a Functional Faster R-CNN Model"
date: 09-10-2024
date-format: short
execute:
    error: true
---

## Introduction: \
There are numerous object detection model architectures available. \
Typically, they comprise: a convolutional module used for feature extraction; some mechanism for identifying Regions of Interest (ROIs) (for example, by [selective search](https://link.springer.com/article/10.1007/s11263-013-0620-5), or by a [Region Proposal Network] (RPN) that uses 'anchors' and is integrated within the model); and classification/masking/bounding box regression heads. \
Chosen somewhat arbitrarily, I will first code and train a Faster R-CNN architecture. There are others that might be worth exploring (e.g., You Only Look Once (YOLO), Single-Shot Detector(SSD)), but I think implementing the former is more realistic given my inexperience with deep machine learning frameworks prior to this project. \

### Goal: \
Develop a functional initial object detection pipeline using a Faster R-CNN model trained on the dataset created in [analysis 0001](0001_dataset_creation.md) to facilitate the [imaging system's](https://github.com/SamuelClucas/SC_TSL_06082024_imaging_system_design) recognition of and navigation to growth plates inside the incubator. \

### Hypothesis: \
Can a Faster R-CNN model trained on [this](../raw/) dataset identify and label plates with bounding boxes in a validation subset with a mean Average Precision (mAP) of greater than 50%, based on an Intersection over Union (IoU) metric. \

### Rationale: \
Real-time object detection using the Faster R-CNN architecture has been robustly established in the literature ([Shaoqing Ren et al., 2016](https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf),[Yu Liu, 2018](https://ieeexplore.ieee.org/abstract/document/8695451), [Wenze Li, 2021](https://iopscience.iop.org/article/10.1088/1742-6596/1827/1/012085/meta)). \

Based on reading of the [Universal Approximation Theorem](https://mitliagkas.github.io/ift6085-2020/ift-6085-lecture-10-notes.pdf), I think it is reasonable to posit that given sufficient, representative, and clear training data, the network should be abe to detect plates in images to at least some degree of accuracy. The question is whether or not this accuracy is high enough to be practically useful. \

### Experimental Plan: \
- I will use PyTorch. Firstly, I need to write two custom dataset classes to handle storing and accessing images and their corresponding bounding box vertices for both the positives and negatives datasets (the latter to be used in [hard negatives mining](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590120.pdf) to further optimise the model). \
- Once instantiated, this class is passed to PyTorch's DataLoader for training or evaluation, as outlined in this [pytorch tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html). I would like to write a helper_training_functions src file to be imported in training scripts to keep the latter concise and readable. \
- Training on the cluster requires that scripts don't need to download files at runtime. This will require a local Faster R-CNN class definition file. \
- [ResNet50](https://blog.roboflow.com/what-is-resnet-50/#:~:text=One%20such%20architecture%20is%20called,it%2C%20and%20categorize%20them%20accordingly.)'s COCO or ImageNet pre-trained weights aren't going to be useful. This is common sense. 'Non-lambertian' or transparent objects form a minority within these datasets, which isn't helpful in training a network to recognise an object that is transparent. For this reason, I will train a ResNet backbone (I'm not sure many layers this will have, perhaps 50-layer or 101-layer) on a transparent object dataset. Google's [cleargrasp](https://github.com/Shreeyak/cleargrasp/tree/master) project required the creation of [transparent object datasets](https://sites.google.com/view/transparent-objects). I will train the network on these first, with the hopes that the backbone will then be primed to extract features typical of transparent objects. \
- I will then freeze the backbone weights prior to training of the RPN layer, and the classification and bounding box regression heads. This model will undergo evaluation to answer the hypothesis. \

## Creating a Dataset Class: Plate_Image_Dataset() \
*Note:* \
I used [this](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html) Pytorch tutorial as a launchpoint. As per the tutorial, I downloaded several helper functions available on Pytorch vision's github. \ 

Why not just import them into scripts through the torchvision library? \
- See fmassa's comment [here](https://github.com/pytorch/vision/issues/2254). 

If you install the package as described in the project [README](../README.md), they have already been installed under [src/torchvision_deps/](../src/torchvision_deps/) and can be imported into scripts. \

For reference, I have included wget commands for each below:
```{bash}
#| eval: false
wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py 
wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py
wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py
wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py
wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py
```
### Class Overview:

#### Imports...
```{python}
import os
from torchvision_deps import engine
import re
from glob import glob
import numpy as np
import pandas as pd
from torchvision.io import read_image
import torch
from torchvision.transforms.v2 import functional as F
from torchvision import tv_tensors
from collections.abc import Sequence # for type hints like 'tuple[]': https://docs.python.org/3/library/typing.html
```

#### Defining the constructor...
```{python}
class Plate_Image_Dataset(torch.utils.data.Dataset):
    def __init__(self, annotations_file: str, img_dir: str, transforms=None):
        self.img_labels: pd.Dataframe = pd.read_csv(annotations_file) # bounding box vertices' coordinates
        self.boxes: np.array[int] = None

        self.img_files: list[str] = [f for f in glob(img_dir + "/*.png")]
        self.transforms = transforms
```
**Breakdown:** \
- 'Plate_Image_Dataset' is a custom dataset that represents a map from keys/indices to data samples, hence inherits from PyTorch's provided abstract class '[torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)'. \
- Initialise 'self.img_labels' as as a [pandas DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), reading from the csv file containing bounding box vertex coordinates passed to the constructor as a string in 'annotations_file'. \
- Declare 'self.boxes' as a [numpy array](https://numpy.org/doc/stable/reference/generated/numpy.array.html) of integers, later to be used to store all bounding boxes associated with an image (using an index) in __getitem__. \
- Initialise 'self.img_files' as a list of strings, using [glob](https://docs.python.org/3/library/glob.html) to find all images (pathnames ending in .png) at the image dataset directory passed to the constructor as a string in 'img_dir'. The notation [f for f in...] is a concise way to create lists in python (see [list comprehension](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions)). \
- Initialise 'self.transforms' as transforms passed the constructor. The default is None. See PyTorch's transforms documentation [here](https://pytorch.org/vision/master/transforms.html#transforms). \

#### Defining __len__...
```{python}
class Plate_Image_Dataset(Plate_Image_Dataset):
    def __len__(self) -> int:
        return len(self.img_files)
```
- Just returns the number of image files in the dataset as an integer. \

#### Defining __getitem__...
```{python}
class Plate_Image_Dataset(Plate_Image_Dataset):
    def __getitem__(self, idx: int) -> tuple[torch.Tensor, tv_tensors.Image]: 
        # loads images and bounding boxes
        img: torch.Tensor = read_image(self.img_files[idx]) # uint8
        
        # get digits from self.img_labels eg. ymaxs = '[ymax1, ymax2, ymax3]' returned from pd.DataFrame
        x1 = [int(item) for item in re.findall(r'\d+', str(self.img_labels['xmins'][idx]))]
        y1 = [int(item) for item in re.findall(r'\d+', str(self.img_labels['ymins'][idx]))]
        x2 = [int(item) for item in re.findall(r'\d+', str(self.img_labels['xmaxs'][idx]))]
        y2 = [int(item) for item in re.findall(r'\d+', str(self.img_labels['ymaxs'][idx]))]

        num_objs = len(x1)
        # this class is for the positives dataset, and so every image has at least one labelled bounding box, hence should be tensor of ones shape [N]
        labels = torch.ones((num_objs,), dtype=torch.int64) 

        # boxes expected shape [N, 4] Tensor x1, y1, x2, y2 where 0 <= x1 < x2 same for y1 and y2 [row = boxes, columns: x1y1x2y2]
        self.boxes = [[x1[i], y1[i], x2[i], y2[i]] for i in range(num_objs)]
       
        # one BoundingBoxes instance per sample, "{img": img, "bbox": BoundingBoxes(...)}" where BoundingBoxes contains all the bounding box vertices associated with that image in the form x1, y1,x2, y2
        self.boxes = torch.from_numpy(np.array(self.boxes)) # [N, 4] tensor
        
        image_id = idx

        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)

        area = (self.boxes[:, 3] - self.boxes[:, 1]) * (self.boxes[:, 2] - self.boxes[:, 0]) 

        # tv.tensors is tensor of images with associated metadata
        img = tv_tensors.Image(img)
        target = {}
        target["boxes"] = tv_tensors.BoundingBoxes(self.boxes, format="XYXY", canvas_size=F.get_size(img))
        target["labels"] = labels
        target["image_id"] = image_id
        target["area"] = area
        target["iscrowd"] = iscrowd
        
        if self.transforms is not None:
            img = self.transforms(img)
            target["boxes"] = self.transforms(target["boxes"])
        
        return img, target
```

**Breakdown:** \
- I'm not particularly familiar with indexing pandas DataFrames. I used python's [regular expression operations](https://docs.python.org/3/library/re.html) to extract digits ('\\d') from the dataframe at a given index. Using isdigit() doesn't work (for example, iterating through '200' with isdigit() would return '2', '0', '0'). I struggled to use pandas 'loc' indexing attribute. I may return to this to do so. \
- __getitem__ must return a tuple as specified in the tutorial. Here, img is of subclass of [tv.Tensor](https://pytorch.org/vision/master/tv_tensors.html) ([an image stored as a 3D matrix/tensor](https://discuss.pytorch.org/t/what-is-image-really/151290)). This tensor should be of shape [3, H, W], where 3 is the number of channels. Target is a dict with the following fields: \
    - "boxes": another [tv.Tensor](https://pytorch.org/vision/master/tv_tensors.html) subclass with shape [N, 4] where N is the number of bounding boxes associated with the image at the index being 'got'. Columns are x1, y1, x2, y2, where 0 <= x1 < x2 (same for y1 and y2). This is specified by 'format'. \
    - "labels": class labels (here 0 for background, 1 for 'plate') of type int64. \
    - "image_id": image index in dataset of type int64. \
    - "area": float torch.Tensor of shape [N]. The area of the bounding box. This is used by coco_eval in [src/torchvision_deps](../src/torchvision_deps/) to separate metric scores for small, medium and large boxes. \
    - "iscrowd": int64 torch.Tensor of shape [N]. Instances with iscrowd=True are ignored during evaluation. This doesn't really serve a purpose other than to prevent errors popping up when using the [torchvision_deps](../src/torchvision_deps/). I plan to either extricate it from the dependencies so that it can be removed from the dataset class. For now, it is always initialised as a tensor of zeros using [torch.zeros](https://pytorch.org/docs/stable/generated/torch.zeros.html). \
- wrapping image tensors and bounding boxes in TVTensor subclasses allows for application of torchvision [built-in transformations](https://pytorch.org/vision/stable/transforms.html). Based on the TVTensor subclass wrapping the object, the tranforms dispatch the object to the appropriate implementation (as described [here](https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_getting_started.html#what-are-tvtensors)). \
- in the tutorial, img and target are passed to trainsforms together and returned as a tuple. For some reason, I could not get it to work this way. I suspect it is something to do with the types of the fields of target not all being TVTensors. Regardless, I pass boxes and img to transforms separately, then return them from __getitem__ as a tuple. Hopefully this isn't behaving unexpectedly in the torchvision deps - unit tests necessary to confirm this. \
 
## Creating a 'helper_training_functions' Module:

I created numerous helper functions for import into training/evaluation scripts. They are defined as a module inside [plate_detect](../src/plate_detect/helper_training_functions.py). The most recent version of 'helper_training_functions' can be imported into scripts like so:  \
```{python}
#| execute: false
from plate_detect import helper_training_functions
```

Within this analysis, however, I define each function locally as I iteratively rewrite them during debugging. \
### Programme Overview:

#### Imports:
```{python}
from torchvision_deps.engine import evaluate # in scripts, you should import train_one_epoch also here 
import re
from glob import glob
import numpy as np
import pandas as pd
from torchvision.io import read_image
import torch
from torchvision.transforms.v2 import functional as F
from torchvision import tv_tensors
import matplotlib.pyplot as plt
from pathlib import Path
from typing import Dict, Union
from torchvision.models.detection.faster_rcnn import fasterrcnn_resnet50_fpn_v2, FastRCNNPredictor, FasterRCNN_ResNet50_FPN_V2_Weights
import torchvision_deps.T_and_utils.utils as utils 
import PIL
from torchvision.utils import draw_bounding_boxes
import torchvision
import torch.nn as nn
from torch.utils.tensorboard import SummaryWriter
import matplotlib.pyplot as plt
```

#### Get Faster R-CNN model instance for object detection...
```{python}
def get_model_instance_object_detection(num_class: int) -> fasterrcnn_resnet50_fpn_v2:
    # New weights with accuracy 80.858%
    weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT # alias is .DEFAULT suffix, weights = None is random initialisation, box MAP 46.7, params, 43.7M, GFLOPS 280.37 https://github.com/pytorch/vision/pull/5763
    model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.0001)
    preprocess = weights.transforms()
    # finetuning pretrained model by transfer learning
    # get num of input features for classifier
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    # replace the pre-trained head with a new one
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_class)
    return model, preprocess
```

**Breakdown:** \
- this approach requires permissions to download the model file from the internet (through 'torch.utils.model_zoo.load_url()'). For this reason, it is not amenable to training on the cluster. This will be addressed in analysis 0003, using a local Faster R-CNN with ResNet backbone class definition. Otherwise, this function still works. \
- See pytorch's documentation on [ResNet50 faster R-CNN backbone](https://pytorch.org/vision/stable/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html).

#### Save epoch, model and optimizer state_dict in checkpoint dict:
```{python}
def save_checkpoint(model, optimizer, epoch, root_dir):
    save_path = os.path.join(root_dir, 'checkpoints')
    if not os.path.exists(save_path):
        os.makedirs(save_path)
    
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
    }
    
    filename = f'checkpoint_epoch_{epoch}.pth'
    save_path = os.path.join(save_path, filename)
    torch.save(checkpoint, save_path)
    print(f"Checkpoint saved: {save_path}")
```
**Breakdown:** \
- Creates a directory if it doesn't exist to store the checkpoints. \
- Saves the current epoch number, model parameters (weights), and optimizer state to a dictionary. \
- This allows for resuming training from a specific epoch in case of interruptions. \

#### Training function:
::: {.callout-note collapse="true"}
*Note:* This function uses train_one_epoch() from [engine.py](../src/torchvision_deps/engine.py) in torchvision_deps. Engine.py uses the MetricLogger and SmoothedValue classes defined in [utils.py](../src/torchvision_deps/T_and_utils/utils.py) I may later modify these, and so I define them locally prior to these modifications here preserve their development through time.
```{python}
import datetime
import errno
import os
import time
from collections import defaultdict, deque

import torch
import torch.distributed as dist


class SmoothedValue:
    """Track a series of values and provide access to smoothed values over a
    window or the global series average.
    """

    def __init__(self, window_size=20, fmt=None):
        if fmt is None:
            fmt = "{median:.4f} ({global_avg:.4f})"
        self.deque = deque(maxlen=window_size)
        self.total = 0.0
        self.count = 0
        self.fmt = fmt

    def update(self, value, n=1):
        self.deque.append(value)
        self.count += n
        self.total += value * n

    def synchronize_between_processes(self):
        """
        Warning: does not synchronize the deque!
        """
        if not is_dist_avail_and_initialized():
            return
        t = torch.tensor([self.count, self.total], dtype=torch.float64, device="cuda")
        dist.barrier()
        dist.all_reduce(t)
        t = t.tolist()
        self.count = int(t[0])
        self.total = t[1]

    @property
    def median(self):
        d = torch.tensor(list(self.deque))
        return d.median().item()

    @property
    def avg(self):
        d = torch.tensor(list(self.deque), dtype=torch.float32)
        return d.mean().item()

    @property
    def global_avg(self):
        return self.total / self.count

    @property
    def max(self):
        return max(self.deque)

    @property
    def value(self):
        return self.deque[-1]

    def __str__(self):
        return self.fmt.format(
            median=self.median, avg=self.avg, global_avg=self.global_avg, max=self.max, value=self.value
        )


def all_gather(data):
    """
    Run all_gather on arbitrary picklable data (not necessarily tensors)
    Args:
        data: any picklable object
    Returns:
        list[data]: list of data gathered from each rank
    """
    world_size = get_world_size()
    if world_size == 1:
        return [data]
    data_list = [None] * world_size
    dist.all_gather_object(data_list, data)
    return data_list


def reduce_dict(input_dict, average=True):
    """
    Args:
        input_dict (dict): all the values will be reduced
        average (bool): whether to do average or sum
    Reduce the values in the dictionary from all processes so that all processes
    have the averaged results. Returns a dict with the same fields as
    input_dict, after reduction.
    """
    world_size = get_world_size()
    if world_size < 2:
        return input_dict
    with torch.inference_mode():
        names = []
        values = []
        # sort the keys so that they are consistent across processes
        for k in sorted(input_dict.keys()):
            names.append(k)
            values.append(input_dict[k])
        values = torch.stack(values, dim=0)
        dist.all_reduce(values)
        if average:
            values /= world_size
        reduced_dict = {k: v for k, v in zip(names, values)}
    return reduced_dict

class MetricLogger:
    def __init__(self, delimiter="\t"):
        self.meters = defaultdict(SmoothedValue)
        self.delimiter = delimiter

    def update(self, **kwargs):
        for k, v in kwargs.items():
            if isinstance(v, torch.Tensor):
                v = v.item()
            assert isinstance(v, (float, int))
            self.meters[k].update(v)

    def __getattr__(self, attr):
        if attr in self.meters:
            return self.meters[attr]
        if attr in self.__dict__:
            return self.__dict__[attr]
        raise AttributeError(f"'{type(self).__name__}' object has no attribute '{attr}'")

    def __str__(self):
        loss_str = []
        for name, meter in self.meters.items():
            loss_str.append(f"{name}: {str(meter)}")
        return self.delimiter.join(loss_str)

    def synchronize_between_processes(self):
        for meter in self.meters.values():
            meter.synchronize_between_processes()

    def add_meter(self, name, meter):
        self.meters[name] = meter

    def log_every(self, iterable, print_freq, header=None):
        i = 0
        if not header:
            header = ""
        start_time = time.time()
        end = time.time()
        iter_time = SmoothedValue(fmt="{avg:.4f}")
        data_time = SmoothedValue(fmt="{avg:.4f}")
        space_fmt = ":" + str(len(str(len(iterable)))) + "d"
        if torch.cuda.is_available():
            log_msg = self.delimiter.join(
                [
                    header,
                    "[{0" + space_fmt + "}/{1}]",
                    "eta: {eta}",
                    "{meters}",
                    "time: {time}",
                    "data: {data}",
                    "max mem: {memory:.0f}",
                ]
            )
        else:
            log_msg = self.delimiter.join(
                [header, "[{0" + space_fmt + "}/{1}]", "eta: {eta}", "{meters}", "time: {time}", "data: {data}"]
            )
        MB = 1024.0 * 1024.0
        for obj in iterable:
            data_time.update(time.time() - end)
            yield obj
            iter_time.update(time.time() - end)
            if i % print_freq == 0 or i == len(iterable) - 1:
                eta_seconds = iter_time.global_avg * (len(iterable) - i)
                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))
                if torch.cuda.is_available():
                    print(
                        log_msg.format(
                            i,
                            len(iterable),
                            eta=eta_string,
                            meters=str(self),
                            time=str(iter_time),
                            data=str(data_time),
                            memory=torch.cuda.max_memory_allocated() / MB,
                        )
                    )
                else:
                    print(
                        log_msg.format(
                            i, len(iterable), eta=eta_string, meters=str(self), time=str(iter_time), data=str(data_time)
                        )
                    )
            i += 1
            end = time.time()
        total_time = time.time() - start_time
        total_time_str = str(datetime.timedelta(seconds=int(total_time)))
        print(f"{header} Total time: {total_time_str} ({total_time / len(iterable):.4f} s / it)")
```

```{python}
import math
import sys
import time

import torch
import torchvision.models.detection.mask_rcnn
from torchvision_deps.T_and_utils import utils
from torchvision_deps.coco_eval import CocoEvaluator
from torchvision_deps.coco_utils import get_coco_api_from_dataset


def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq, scaler=None):
    model.train()
    metric_logger = MetricLogger(delimiter="  ") # normally this would be utils.MetricLogger, but I'm using the MetricLogger defined in this quarto document
    metric_logger.add_meter("lr", utils.SmoothedValue(window_size=1, fmt="{value:.6f}"))
    header = f"Epoch: [{epoch}]"

    lr_scheduler = None
    if epoch == 0:
        warmup_factor = 1.0 / 1000
        warmup_iters = min(1000, len(data_loader) - 1)

        lr_scheduler = torch.optim.lr_scheduler.LinearLR(
            optimizer, start_factor=warmup_factor, total_iters=warmup_iters
        )

    for images, targets in metric_logger.log_every(data_loader, print_freq, header):
        images = list(image.to(device) for image in images)
        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]
        with torch.cuda.amp.autocast(enabled=scaler is not None):
            loss_dict = model(images, targets)
            losses = sum(loss for loss in loss_dict.values())

        # reduce losses over all GPUs for logging purposes
        loss_dict_reduced = utils.reduce_dict(loss_dict)
        losses_reduced = sum(loss for loss in loss_dict_reduced.values())

        loss_value = losses_reduced.item()

        if not math.isfinite(loss_value):
            print(f"Loss is {loss_value}, stopping training")
            print(loss_dict_reduced)
            sys.exit(1)

        optimizer.zero_grad()
        if scaler is not None:
            scaler.scale(losses).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            losses.backward()
            optimizer.step()

        if lr_scheduler is not None:
            lr_scheduler.step()

        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)
        metric_logger.update(lr=optimizer.param_groups[0]["lr"])

    return metric_logger


def _get_iou_types(model):
    model_without_ddp = model
    if isinstance(model, torch.nn.parallel.DistributedDataParallel):
        model_without_ddp = model.module
    iou_types = ["bbox"]
    if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN):
        iou_types.append("segm")
    if isinstance(model_without_ddp, torchvision.models.detection.KeypointRCNN):
        iou_types.append("keypoints")
    return iou_types


@torch.inference_mode()
def evaluate(model, data_loader, device):
    n_threads = torch.get_num_threads()
    # FIXME remove this and make paste_masks_in_image run on the GPU
    torch.set_num_threads(1)
    cpu_device = torch.device("cpu")
    model.eval()
    metric_logger = utils.MetricLogger(delimiter="  ")
    header = "Test:"

    coco = get_coco_api_from_dataset(data_loader.dataset)
    iou_types = _get_iou_types(model)
    coco_evaluator = CocoEvaluator(coco, iou_types)

    for images, targets in metric_logger.log_every(data_loader, 100, header):
        images = list(img.to(device) for img in images)

        if torch.cuda.is_available():
            torch.cuda.synchronize()
        model_time = time.time()
        outputs = model(images)

        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]
        model_time = time.time() - model_time

        res = {target["image_id"]: output for target, output in zip(targets, outputs)}
        evaluator_time = time.time()
        coco_evaluator.update(res)
        evaluator_time = time.time() - evaluator_time
        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)

    # gather the stats from all processes
    metric_logger.synchronize_between_processes()
    print("Averaged stats:", metric_logger)
    coco_evaluator.synchronize_between_processes()

    # accumulate predictions from all images
    coco_evaluator.accumulate()
    coco_evaluator.summarize()
    torch.set_num_threads(n_threads)
    return coco_evaluator
```
:::
```{python}
def train(model, data_loader, data_loader_test, device, num_epochs, precedent_epoch, save_dir): 
    model.train()

    # Initialize lists to store metrics
    train_losses = []

     # construct an optimizer
    params = [p for p in model.parameters() if p.requires_grad]
    optimizer = torch.optim.SGD(
        params,
        lr=0.005,
        momentum=0.9,
        weight_decay=0.0005
    )
    # and a learning rate scheduler
    lr_scheduler = torch.optim.lr_scheduler.StepLR(
        optimizer,
        step_size=3,
        gamma=0.1
    )

    for epoch in range(num_epochs):
    # train for one epoch, printing every 10 iterations
        metric_logger = train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)
    
    # Get the average loss for this epoch
        epoch_loss = metric_logger.meters['loss'].global_avg
        train_losses.append(epoch_loss)

        # Save checkpoint
        save_checkpoint(model, optimizer, epoch + precedent_epoch, save_dir)
        
        # Update the learning rate
        lr_scheduler.step()

    return num_epochs + precedent_epoch, train_losses
```

**Breakdown:** \
Setting the Model to Training Mode: \
- model.train() sets the model to training mode. This is crucial because certain layers (like dropout and batch normalization) behave differently during training versus evaluation. \

Optimizer Construction: \
- The optimizer is created using Stochastic Gradient Descent (SGD) with a learning rate of 0.005, momentum of 0.9, and weight decay for regularization (also known as [L2 regularisation](https://arxiv.org/pdf/2310.04415)). \
- params filters the model's parameters to include only those that require gradients (trainable parameters). \

Learning Rate Scheduler: \
- A learning rate scheduler (StepLR) is initialized, which reduces the learning rate by a factor of gamma (0.1) every step_size epochs (3 epochs in this case). This helps improve convergence as training progresses. \

Epoch Loop: \
- The outer loop iterates over the number of specified epochs (num_epochs). \

Training for One Epoch: \
- train_one_epoch(...) is a function from [engine.py](../src/torchvision_deps/engine.py) that handles the training logic for one epoch. It processes batches from the data_loader, computes losses, and updates model weights. \
- The print_freq parameter controls how often training progress is printed (every 10 iterations). \

Logging Loss: \
- The average loss for the epoch is extracted from metric_logger, which tracks various metrics during training. \

Checkpoint Saving: \
- After each epoch, a checkpoint is saved using the save_checkpoint function. This allows for resuming from the last epoch in case of interruption. \

Metrics Plotting: \
- The function plot_eval_metrics is called to visualize training losses over epochs. \

Learning Rate Update: \
- After the epoch, the learning rate is updated according to the scheduler. \

Return Values: \
- The function returns the final epoch number (adjusted for any precedent epochs) and the list of training losses. \

#### Load a model from a .pth file:
```{python}
def load_model(save_dir: str, num_classes: int, model_file_name: str):
    model, preprocess = get_model_instance_object_detection(num_classes)
    checkpoint = torch.load(save_dir + f'{model_file_name}.pth', weights_only=True)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer = model.load_state_dict(checkpoint['optimizer_state_dict'])
    epoch = checkpoint['epoch']
    return model, optimizer, epoch
```
**Breakdown:** \
- Loads a model from a .pth file. See the pytorch documentation [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html). \
- The weights and biases of a model are accessed by model.parameters(). A state_dict is a dictionary object that maps each layer to its parameter tensor. Both the model and optimiser have state_dicts, and both are saved by the save_checkpoint helper function. \
- Returns the model, optimizer and epoch. \

#### Evaluate a model's performance after training:
```{python}
def evaluate_model(model, data_loader_test: torch.utils.data.DataLoader, device: torch.cuda.device): 
    val_metrics = []
    model.eval()

    coco_evaluator = evaluate(model, data_loader_test, device=device)
        
    # Extract evaluation metrics
    eval_metrics = coco_evaluator.coco_eval['bbox'].stats

    return eval_metrics
```
**Breakdown:** \
- Sets the model to evaluation mode using model.eval(), which alters the behavior of certain layers. \
    - In training mode, the model expects input tensors (images) and targets (list of dictionary containing bounding boxes [N, 4], with vertices in the format [x1, y1, x2, y2] and class labels in the format Int64Tensor[N] where N is the number of bounding boxes in a given image, or the number of distinct classes, respectively). \
- Currently using [coco_eval](../src/torchvision_deps/). \
- See device class docs [here](https://pytorch.org/docs/stable/generated/torch.cuda.device.html) \
- Makes a call to [engine's](../src/torchvision_deps/engine.py) evaluator() to perform COCO-style evaluation on the test dataset, shown below: \
```{python}
@torch.inference_mode()
def evaluate(model, data_loader, device):
    n_threads = torch.get_num_threads()
    # FIXME remove this and make paste_masks_in_image run on the GPU
    torch.set_num_threads(1)
    cpu_device = torch.device("cpu")
    model.eval()
    metric_logger = utils.MetricLogger(delimiter="  ") # MetricLogger: https://pytorch.org/tnt/stable/utils/generated/torchtnt.utils.loggers.MetricLogger.html
    header = "Test:"

    coco = get_coco_api_from_dataset(data_loader.dataset)
    iou_types = _get_iou_types(model)
    coco_evaluator = CocoEvaluator(coco, iou_types)

    for images, targets in metric_logger.log_every(data_loader, 100, header):
        images = list(img.to(device) for img in images)

        if torch.cuda.is_available():
            torch.cuda.synchronize()
        model_time = time.time()
        outputs = model(images)

        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]
        model_time = time.time() - model_time

        res = {target["image_id"]: output for target, output in zip(targets, outputs)}
        evaluator_time = time.time()
        coco_evaluator.update(res)
        evaluator_time = time.time() - evaluator_time
        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)

    # gather the stats from all processes
    metric_logger.synchronize_between_processes()
    print("Averaged stats:", metric_logger)
    coco_evaluator.synchronize_between_processes()

    # accumulate predictions from all images
    coco_evaluator.accumulate()
    coco_evaluator.summarize()
    torch.set_num_threads(n_threads)
    return coco_evaluator
```
- COCO provides thorough documentation [here](https://cocodataset.org/#detection-eval).
- coco_evaluator.summarize() computes Average Precision (AP), AP Across Scales, Average Recall (AR), AR Across Scales (see a dicussion of these metrics [here](https://blog.zenggyu.com/posts/en/2018-12-16-an-introduction-to-evaluation-metrics-for-object-detection/index.html)).

#### Using PyTorch hooks to get backbone feature maps:
```{python}
def get_feature_maps(model, input_image, target_layer_name: torch.nn):
    feature_maps = {} # stores activations passed to forward_hook
    
    def hook_fn(module, input, output):
        feature_maps[module] = output.detach()
    return hook_fn # calls handle.remove() to remove the added hook
    
    # Register hooks for the layers to be visualised
    for name, module in model.backbone.named_modules():
        if isinstance(module, target_layer_name): # type examples: nn.Conv2d, nn.BatchNorm2d, nn.ReLU
            module.register_forward_hook(hook_fn(module))
    
    # Forward pass
    with torch.no_grad(): # disables loss gradient calculation: https://discuss.pytorch.org/t/with-torch-no-grad/130146
        model([input_image])
    
    return feature_maps
```
**Breakdown:** \
- In order to extract intermediate activations from model layers, PyTorch provides 'hooks'. [Pytorch documentation on hooks](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_forward_hook) is quite sparse, but [this](https://web.stanford.edu/~nanbhas/blog/forward-hooks-pytorch/) blog is quite useful. \
- Gets the feature maps for every target layer (target_layer_name) in each named module in the model's backbone (e.g., 'Sequential', 'Bottleneck'). Target layers are documented [here](https://pytorch.org/docs/stable/nn.html#convolution-layers). \

#### Plot feature maps:
```{python}
def visualise_feature_maps(feature_maps, num_features=64):
    for layer, feature_map in feature_maps.items():
        # Get the first image in the batch
        feature_map = feature_map[0]
        
        # Plot up to num_features feature maps
        num_features = min(feature_map.size(0), num_features)
        
        fig, axs = plt.subplots(8, 8, figsize=(20, 20))
        fig.suptitle(f'Feature Maps for Layer: {layer}')
        
        for i in range(num_features):
            ax = axs[i // 8, i % 8]
            ax.imshow(feature_map[i].cpu(), cmap='gray')
            ax.axis('off')
        
        plt.tight_layout()
        plt.show()
```

**Breakdown:** \
- This function takes the feature maps collected from the get_feature_maps function and visualizes them. \
- It plots up to num_features from each layer’s output, providing visual insights into what features the model is learning at different levels.

#### Superimpose and plot bounding boxes on image:
```{python}
def plot_prediction(model, dataset, device, index, save_dir: str, model_name):
    img, target = dataset[index]
    num_epochs = 1
    print_freq = 10  

    model = load_model(save_dir, 2, model_name)
    with torch.no_grad():
        image = img
        image = image[:3, ...].to(device)
        predictions = model([image, ])
        pred = predictions[0]

    image = image[:3, ...]
    pred_boxes = pred["boxes"].long()
    output_image = draw_bounding_boxes(image, pred_boxes, colors="red")
    plt.figure(figsize=(12, 12))
    plt.imshow(output_image.permute(1, 2, 0))
   # plt.savefig(f'{index}.png', bbox_inches='tight') # tight removes whitespace
```
**Breakdown:** \
- Loads a specific image from the dataset and runs it through the model to get predictions.\
- Superimposes the predicted bounding boxes on the original image (as a red outlined bounding box) and visualises the result. \

## Debugging 'helper_training_functions' by Implementation in a Script:

Having written the code necessary to instantiate, train, and evaluate a ResNet50 model, I need to evaluate its performance over several epochs. This will help to validate the code functions as intended, and to guide future development. \

Here, I attempt to setup, train, and evaluate a Resnet50 Faster R-CNN model for 10 epochs - using src/Plate_Image_Dataset.py and src/helper_training_functions.py - then evaluate its mean average precision and recall, to be visualised in a line graph. \

The implementation focuses on validating the core functionality of the object detection pipeline before scaling up to full training runs. \

This script uses code from the following src files: \
- [Plate_Image_Dataset.py](../src/plate_detect/Plate_Image_Dataset.py) - custom class that handles storing and accessing images and their corresponding bounding box vertices. \
- [helper_training_functions.py](../src/plate_detect/helper_training_functions.py) - a group of useful helper functions I've written. For example, 'get_model_instance_object_detection' should return a [ResNet50](https://pytorch.org/hub/pytorch_vision_resnet/) backbone ([ImageNet1K_V2 weights](https://pytorch.org/vision/0.18/models/generated/torchvision.models.resnet50.html#:~:text=By%20default%2C%20no%20pre%2Dtrained%20weights%20are%20used.)), with a Region Proposal Network, as well as classifier and bounding box regression heads. \

It may also be helpful to visualise the model architecture. There are libraries for this, one of which is '[pytorchviz](https://github.com/szagoruyko/pytorchviz)'. If useful, I will create such a script [here](docs/scripts/inspect_architecture.qmd). \

For reference, here is a useful [pytorch tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html). \
 
### Programme Overview:

#### Imports:
```{python}
#from plate_detect import Plate_Image_Dataset #, helper_training_functions # in scripts, this is imported. Here, to document the debug process, I am using the iterations of these functions defined locally in the previous section
import torch
from torchvision.transforms.v2 import functional as F
from pathlib import Path
from typing import Dict, Union
from torchvision.models.detection.faster_rcnn import fasterrcnn_resnet50_fpn_v2, FastRCNNPredictor, FasterRCNN_ResNet50_FPN_V2_Weights
from torchvision.transforms import v2 as T
import torchvision_deps.T_and_utils.utils as utils
from torchvision_deps.engine import evaluate # normally, import train_one_epoch from engine here
import os
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import numpy as np
import PIL
from torchvision.utils import draw_bounding_boxes
from torchvision.io import read_image
from torch import nn
```
**Notes:** when running on the cluster, change any import statements for packages defined within the project directory (see [src](../src/)). Here is an example:
```{python}
#| eval: false
from src import Plate_Image_Dataset, helper_training_functions
# to...
from SC_plate_detect.plate_detect import Plate_Image_Dataset, helper_training_functions

```

#### Setting up directories and instantiating model...
```{python}
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

project_dir_root: Path= Path.cwd() # 'SC_TSL_15092024_Plate_Detect/' type PosixPath for UNIX, WindowsPath for windows...
print(f'Project root directory: {str(project_dir_root)}')

annotations_file: Path = project_dir_root.parents[0].joinpath('lib', 'labels.csv')
print(f'Training labels csv file: {annotations_file}')

img_dir: Path= project_dir_root.parents[0].joinpath('raw', 'positives')  # 'SC_TSL_15092024_Plate_Detect/train/images/positives/' on UNIX systems
print(f'Training dataset directory: {img_dir}')

num_class: int = 2 # plate or background
 
# creates resnet50 v2 faster r cnn model with new head for class classification

model, preprocess = get_model_instance_object_detection(num_class) # see next comment for how this function is called when imported 
# model, preprocess = helper_training_functions.get_model_instance_object_detection(num_class)

# move model to the right device
model.to(device)
```
**Breakdown:** \
- Uses 'Compute Unified Device Architecture' ([CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/)) if available, falls back to CPU. \
- Initializes ResNet50 Faster R-CNN with pretrained weights. \
- 'preprocess' stores the input data transforms that take place just before the forward pass through the network. In this case, preprocess converts Tensor image, PIL image, or NumPy ndarray types into FloatTensor and scales pixel intensities in range [0.,1.] \

#### Instantiating Plate_Image_Dataset class and creating training and validation subsets...
```{python}
dataset: Plate_Image_Dataset = Plate_Image_Dataset.Plate_Image_Dataset(
        img_dir=str(img_dir), 
        annotations_file=str(annotations_file),
        transforms=preprocess, 
        )

# split the dataset in train and test set
dataset_size = 20  #normally would be len(dataset), but for debugging purposes this is sufficient
validation_size = 5   # Again, this would normally be higher, for example: min(50, int(dataset_size // 5))  {20% of data for testing, or 50 samples, whichever is smaller}
indices = [int(i) for i in torch.randperm(dataset_size).tolist()]

dataset_validation = torch.utils.data.Subset(dataset, indices[-validation_size:])
dataset_train = torch.utils.data.Subset(dataset, indices[:])
```
**Breakdown:** \
- Plate_Image_Dataset is a custom dataset class used to handle parsing the csv file for bounding box vertices, associating them with the correct image using a dictionary, and handling retrieval of this information. \
- In order to create validation and training subsets of the positive samples (i.e., the images *with* at least one plate labelled), I created a randomly arranged list of indices (where len(indices) == the number of samples in the dataset). The validation subset size is equal to 20% of the superset, or 50 samples, whichever is smallest. When I implement dataset augmentation I will scale this up. \

#### Instantiating Pytorch DataLoaders for train and validation subsets...
```{python}
data_loader = torch.utils.data.DataLoader(
    dataset_train,
    batch_size=1,
    shuffle=True,
    collate_fn=utils.collate_fn
)

data_loader_test = torch.utils.data.DataLoader(
    dataset_validation,
    batch_size=1,
    shuffle=False,
    collate_fn=utils.collate_fn
)
```
**Breakdown:** \
- See [here](https://pytorch.org/docs/stable/data.html) Pytorch's documentation for DataLoader. DataLoader fetches and collates together individual samples into batches - i.e., it acts as a sampler. It does so by squeezing on a batch dimension (typically the first) to Tensors. It also provides an iterable over the given dataset. \
- In this case, the Image_Plate_Dataset class is a map-style dataset, as it implements __getitem__() and __len__(), and stores samples with their associated labels and metadata at a shared index. \
- If shuffle == True, data is reshuffled at every epoch. \

#### Calling train and evaluate_model...
```{python}
save_dir = 'results'
num_epochs = 1
precedent_epoch = 0

epoch, loss_metrics = train(model, data_loader, data_loader_test, device, num_epochs, precedent_epoch, save_dir)

eval_metrics = evaluate_model(model, data_loader_test,device)

print("\n -end-")

```
**Breakdown:** \
- Brief overview: \
    1. Training function handles: \
        - Optimisation using [SGD with momentum](https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d)  \
        - Learning rate scheduling \
        - Checkpoint saving \
        - Loss tracking \
    2. Evaluation produces:  \
        - Mean Average Precision (mAP) \
        - Mean Average Recall (mAR) \
        - Performance metrics across IoU thresholds \
- *Note* I have only set it to train for 1 epoch here just to show the output. I will train for longer on the cluster. Here, my focus is debugging the helper functions, not training the model. \

### Loading a model and implementing 'plot_training_loss':

Training seemed to complete 'successfully' (without any overt error message), as shown by the output from the previous code block. \

Next, I want to try and load the model from the .pth file saved in the previous code block. The'load_model()' function expects the save directory string of the .pth file, the number of classes (2: background, plate), and the model file name. It returns the model, optimizer, and epoch. I then pass the model to evaluate_model(). I will also type- and shape-check the eval_metrics list returned from evaluate_model for clarity when later creating a function to plot these metrics.\

```{python}
#| output: false
model, optimizer, epoch = load_model('../checkpoints/', 2, 'checkpoint_epoch_0')

eval_metrics = evaluate_model(model, data_loader_test, device)

print(eval_metrics, '\n', shape(eval_metrics))

print("\n -end-")
```

This produced an error when loading the model:
```{bash}
{
	"name": "RuntimeError",
	"message": "Error(s) in loading state_dict for FasterRCNN:
\tMissing key(s) in state_dict: ... \"param_groups\". ",
	"stack": "---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
File /Users/cla24mas/Documents/My_Repos/SC_TSL_15092024_plate_detect/analyses/0002_functional_Faster_R-CNN.qmd:1
----> 1 model, optimizer, epoch = helper_training_functions.load_model('checkpoints/', 2, 'checkpoint_epoch_0')
      3 eval_metrics = helper_training_functions.evaluate_model(model, data_loader_test,device)
      5 print(eval_metrics, '\
', shape(eval_metrics))

File ~/Documents/My_Repos/SC_TSL_15092024_plate_detect/src/plate_detect/helper_training_functions.py:101, in load_model(save_dir, num_classes, model_file_name)
     99 checkpoint = torch.load(save_dir + f'{model_file_name}.pth', weights_only=True)
    100 model.load_state_dict(checkpoint['model_state_dict'])
--> 101 optimizer = model.load_state_dict(checkpoint['optimizer_state_dict'])
    102 epoch = checkpoint['epoch']
    103 return model, optimizer, epoch

File ~/Documents/My_Repos/SC_TSL_15092024_plate_detect/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2153, in Module.load_state_dict(self, state_dict, strict, assign)
   2148         error_msgs.insert(
   2149             0, 'Missing key(s) in state_dict: {}. '.format(
   2150                 ', '.join(f'\"{k}\"' for k in missing_keys)))
   2152 if len(error_msgs) > 0:
-> 2153     raise RuntimeError('Error(s) in loading state_dict for {}:\
\\t{}'.format(
   2154                        self.__class__.__name__, \"\
\\t\".join(error_msgs)))
   2155 return _IncompatibleKeys(missing_keys, unexpected_keys)

RuntimeError: Error(s) in loading state_dict for FasterRCNN:
\tMissing key(s) in state_dict: \"backbone.body.conv1.weight\", ... \"roi_heads.box_predictor.bbox_pred.bias\". 
\tUnexpected key(s) in state_dict: \"state\", \"param_groups\". "
}
```
This error is relates to handling loading of the serialised optimiser object by the save function. This function must correctly load the optimizer in instances where I want to recommence training from a saved checkpoint. \

I modififed the load_model function according to [this](https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html) article. \

First, the saved state_dict is [deserialised](https://learn.microsoft.com/en-us/dotnet/standard/serialization/) from the .pth file, then it is passed to load_state_dict. 'load_state_dict' expects a [torch.optim.Optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer) object. In the previous load_model(), it gets a dict object. To solve this, I must call load_state_dict on the SGD optimizer. The optimizer should be initialised and returned by the get_model_instance_object_detection() function, but currently only the train() function initialises an optimizer. I think it would be best to isolate model and optimizer initialisation in the model getter. This way, there isn't a chance that the optimizer gets redefined at some point, causing strange behaviour.\

This will require modifications to get_model_instance_object_detection(), load_model(), and train() as follows:\
```{python}
def get_model_instance_object_detection(num_class: int) -> fasterrcnn_resnet50_fpn_v2:
    # New weights with accuracy 80.858%
    weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT # alias is .DEFAULT suffix, weights = None is random initialisation, box MAP 46.7, params, 43.7M, GFLOPS 280.37 https://github.com/pytorch/vision/pull/5763
    model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.0001)
    preprocess = weights.transforms()
    # finetuning pretrained model by transfer learning
    # get num of input features for classifier
    in_features = model.roi_heads.box_predictor.cls_score.in_features

    params = [p for p in model.parameters() if p.requires_grad]
    optimizer = torch.optim.SGD( # optimizer defined in model getter according to pytorch 'recipes'
        params,
        lr=0.005,
        momentum=0.9,
        weight_decay=0.0005
    )
    # replace the pre-trained head with a new one
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_class)
    return model, optimizer, preprocess
```

```{python}
def train(model, data_loader, data_loader_test, device, num_epochs, precedent_epoch, save_dir, optimizer): 
    model.train()
    # Initialize lists to store metrics
    train_losses = []
    
    # and a learning rate scheduler
    lr_scheduler = torch.optim.lr_scheduler.StepLR(
        optimizer,
        step_size=3,
        gamma=0.1
    )

    for epoch in range(num_epochs):
    # train for one epoch, printing every 10 iterations
        metric_logger = train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)
    
    # Get the average loss for this epoch
        epoch_loss = metric_logger.meters['loss'].global_avg
        train_losses.append(epoch_loss)

        # Save checkpoint
        save_checkpoint(model, optimizer, epoch + precedent_epoch, save_dir)
        
        # Update the learning rate
        lr_scheduler.step()

    return num_epochs + precedent_epoch, train_losses

```

```{python}
def load_model(save_dir: str, num_classes: int, model_file_name: str):
    model, optimizer, preprocess = get_model_instance_object_detection(num_classes)
    checkpoint = torch.load(save_dir + f'{model_file_name}.pth', weights_only=True)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    epoch = checkpoint['epoch']
    return model, optimizer, epoch
```
After the above modifications, I ran the following script:
```{python}
model, optimizer, epoch = load_model('../checkpoints/', num_class, 'checkpoint_epoch_0')

eval_metrics = evaluate_model(model, data_loader_test, device)

print(eval_metrics, '\n', len(eval_metrics))

print("\n -end-")
```
... the evaluator now runs again! \
It may be sensible to tuck data_loader, data_loader_test, as well as validation and training subset and indice permutation lists, into load_model. With that being said, I'm conscious that the function signatures are huge, as well as their return statements. I don't think I should add to that. Further to this, implementation of these helper functions in different object detection contexts will likely be suited to different batching parameters based on variables like dataset size. For this reason, I think it's best load_model() handles loading the optimizer and model deserialisation and variable loading only. \

::: {.callout-note collapse="true"}
I just learned about *args and **kwargs. I think this could help cut down the complexity of the function signatures. I will try to use these from now on, where appropriate.
:::

This output is interesting. Firstly, the loss metrics are missing from eval_metrics, yet are output at the end of the previous section. This is because the two are handled separately! The evaluate_model() function is using [coco_eval](../src/torchvision_deps/coco_eval.py), returning coco_evaluator's 'bbox' stats. The training loss metrics are generated by [engine's](../src/torchvision_deps/engine.py) 'train_one_epoch'. This means to handle plotting of evaluation and training metrics, I will need to modify coco_eval, engine and/or utils in some way. Firstly, to clarify why the call to plot_eval_metrics is raising a type-error, I  will print coco_evaluator as well as the eval_metrics returned from the call to evaluate_model:
```{python}
def evaluate_model(model, data_loader_test,device):
    model.eval()

    coco_evaluator = evaluate(model, data_loader_test, device=device)
    print(coco_evaluator)
    # Extract evaluation metrics
    eval_metrics = coco_evaluator.coco_eval['bbox'].stats
  
    return eval_metrics
```

```{python}
eval_metrics = evaluate_model(model, data_loader_test, device)

print(eval_metrics, '\n', len(eval_metrics))
```
The CocoEvaluator class is defined in [torchvision_deps](../src/torchvision_deps/coco_eval.py). \
- 'eval_metrics' is a one-dimensional array of floats, where each index is one of the 12 metrics outlined [here](https://cocodataset.org/#detection-eval). \

- Currently, I'm trying to plot the average loss. While this would be okay if I were training for many epochs, that won't work when trying to create a line plot for one epoch's training to evaluate batch variability. Instead, I want to plot the metrics as the training or evaluation happens. Currently I am using train_one_epoch() and evaluate() from [engine](../src/torchvision_deps/engine.py) in torchvision_deps. They use PyTorch's [MetricLogger](https://pytorch.org/tnt/stable/utils/generated/torchtnt.utils.loggers.MetricLogger.html) to store metrics, where 'log_every()' method from MetricLogger is handling the print frequency of metrics as training/evaluation progresses through the epoch. I want to plot the data that is currently printed by log_every(). This will require modification to either log_every or the metric loggers in engine's train_one_epoch. This discussion is helpful: https://discuss.pytorch.org/t/object-detection-fine-tuning-interpreting-printout-of-train-one-epoch/193496/2 \

I realised the MetricLogger class is defined locally in [utils.py](../src/torchvision_deps/T_and_utils/utils.py). 'log_every()' is a method of MetricLogger. I want to avoid modifying dependency files as much as possible. \

I realised I can just change the way I handle the metric logger returned from train_one_epoch in my train() function. Before I modify it, I want to retrain on a small batch (not the full epoch) to see print the metric logger, just to check that the granular progress through the epoch survives 'train_one_epoch': \ 

```{python}
def train(model, data_loader, data_loader_test, device, num_epochs, precedent_epoch, save_dir, optimizer): 
    model.train()
    # Initialize lists to store metrics
    train_losses = []
    
    # and a learning rate scheduler
    lr_scheduler = torch.optim.lr_scheduler.StepLR(
        optimizer,
        step_size=3,
        gamma=0.1
    )

    for epoch in range(num_epochs):
    # train for one epoch, printing every 10 iterations
        metric_logger = train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)

        for k, v in metric_logger.meters.items():
            print(f'Key: {k} \t Value: {v}')
    
    # Get the average loss for this epoch
        epoch_loss = metric_logger.meters['loss'].global_avg
        train_losses.append(epoch_loss)

        # Save checkpoint
        save_checkpoint(model, optimizer, epoch + precedent_epoch, (save_dir + 'checkpoints'))
        
        # Update the learning rate
        lr_scheduler.step()

    return num_epochs + precedent_epoch, train_losses
```
Then execute the following script for a short training session, after which all keys in metric_logger are printed:
```{python}
model, optimizer, preprocess = get_model_instance_object_detection(2)

dataset: Plate_Image_Dataset = Plate_Image_Dataset.Plate_Image_Dataset(
        img_dir=str(img_dir), 
        annotations_file=str(annotations_file),
        transforms=preprocess, 
        )

# split the dataset in train and test set
dataset_size = 20
#validation_size = min(50, int(dataset_size // 5))  # Use 20% of data for testing, or 50 samples, whichever is smaller
indices = [int(i) for i in torch.randperm(dataset_size).tolist()]

#dataset_validation = torch.utils.data.Subset(dataset, indices[-validation_size:])
dataset_train = torch.utils.data.Subset(dataset, indices[:])

data_loader = torch.utils.data.DataLoader(
    dataset_train,
    batch_size=1,
    shuffle=True,
    collate_fn=utils.collate_fn
)

num_epochs = 1
precedent_epoch = 0
save_dir = '../'

epoch, loss_metrics = train(model, data_loader, None, device, num_epochs, precedent_epoch, save_dir, optimizer)

print("\n -end-")
```
The keys in metric_logger.meters are 'loss, loss_classifier, loss_box_reg, loss_objectness, loss_rpn_box_reg'. 'train_one_epoch' averages the meters' value throughout training by making a call to MetricLogger.update(). 

Firstly I will modify the MetricLogger class, adding the 'intra_epoch_loss' member. I will store the log_message here for later access, as follows: 
```{python}
from collections import defaultdict

class MetricLogger:
    def __init__(self, delimiter="\t"):
        self.meters = defaultdict(SmoothedValue)
        self.delimiter = delimiter

        # there is definitely a more elegant way to do this, but this should work
        self.intra_epoch_loss = defaultdict(list)

    def update(self, **kwargs):
        for k, v in kwargs.items():
            if isinstance(v, torch.Tensor):
                v = v.item()
            assert isinstance(v, (float, int))
            self.meters[k].update(v)

    def __getattr__(self, attr):
        if attr in self.meters:
            return self.meters[attr]
        if attr in self.__dict__:
            return self.__dict__[attr]
        raise AttributeError(f"'{type(self).__name__}' object has no attribute '{attr}'")

    def __str__(self):
        loss_str = []
        for name, meter in self.meters.items():
            loss_str.append(f"{name}: {str(meter)}")
        return self.delimiter.join(loss_str)

    def synchronize_between_processes(self):
        for meter in self.meters.values():
            meter.synchronize_between_processes()

    def add_meter(self, name, meter):
        self.meters[name] = meter

    def log_every(self, iterable, print_freq, header=None):
        i = 0
        if not header:
            header = ""
        start_time = time.time()
        end = time.time()
        iter_time = SmoothedValue(fmt="{avg:.4f}")
        data_time = SmoothedValue(fmt="{avg:.4f}")
        space_fmt = ":" + str(len(str(len(iterable)))) + "d"
        if torch.cuda.is_available():
            log_msg = self.delimiter.join(
                [
                    header,
                    "[{0" + space_fmt + "}/{1}]",
                    "eta: {eta}",
                    "{meters}",
                    "time: {time}",
                    "data: {data}",
                    "max mem: {memory:.0f}",
                ]
            )
        else:
            log_msg = self.delimiter.join(
                [header, "[{0" + space_fmt + "}/{1}]", "eta: {eta}", "{meters}", "time: {time}", "data: {data}"]
            )
        MB = 1024.0 * 1024.0
        for obj in iterable:
            data_time.update(time.time() - end)
            yield obj
            iter_time.update(time.time() - end)
            if i % print_freq == 0 or i == len(iterable) - 1:
                eta_seconds = iter_time.global_avg * (len(iterable) - i)
                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))
                if torch.cuda.is_available():
                    
                    for k, v in self.meters.items():        # <------ insertion here
                        self.intra_epoch_loss[k].append(v.value)     # <------- here see SmoothedValue class (v is SmoothedValue object)
                    self.intra_epoch_loss['progression'].append((((i+1)/(len(iterable))*100)))  # <----- and here
                    print(
                        log_msg.format(
                            i,
                            len(iterable),
                            eta=eta_string,
                            meters=str(self),
                            time=str(iter_time),
                            data=str(data_time),
                            memory=torch.cuda.max_memory_allocated() / MB,
                        )
                    )
                else:
                    for k, v in self.meters.items():        # <------ insertion here
                        self.intra_epoch_loss[k].append(v.value)     # <------- here
                    self.intra_epoch_loss['progression'].append(((i+1)/(len(iterable))*100))  # <----- and here: could use add_meter in train_one_epoch to remove this line
                    print(
                        log_msg.format(
                            i, len(iterable), eta=eta_string, meters=str(self), time=str(iter_time), data=str(data_time)
                        )
                    )
            i += 1
            end = time.time()
        total_time = time.time() - start_time
        total_time_str = str(datetime.timedelta(seconds=int(total_time)))
        print(f"{header} Total time: {total_time_str} ({total_time / len(iterable):.4f} s / it)")
```

I will pass these to a plot_training_loss() helper function.

```{python}
def plot_training_loss(root_dir: str, epoch: int, **kwargs):
    progression = [v for v in kwargs['progression']] # I'm not sure if list comprehension is neccessary here... perhaps just try progression = kwargs['progression']?
    loss_objectness = [v for v in kwargs['loss_objectness']]
    loss = [v for v in kwargs['loss']]
    loss_rpn_box_reg = [v for v in kwargs['loss_rpn_box_reg']]
    loss_classifier = [v for v in kwargs['loss_classifier']]
    lr = [v for v in kwargs['lr']] 

    # the metrics are passed using list comprehension in train()
    plt.figure(figsize=(10, 6))
    plt.plot(progression, loss, label='Loss')
    plt.plot(progression, loss_objectness, label='Loss Objectness')
    plt.plot(progression, loss_rpn_box_reg, label='Loss RPN Box Reg')
    plt.plot(progression, loss_classifier, label='Loss Classifier')

    plt.xlabel('Progression through Epoch / %')
    plt.ylabel('Loss / Log10')
    plt.yscale("log")
    plt.title(f'Loss Metrics Progression Through Epoch {epoch}')
    plt.legend()
    plt.grid(False)
    plt.savefig(f'{root_dir}/results/loss_metrics_epoch_{epoch}')
    plt.show()
```

Now make a call to it in train() instead:
```{python}
def train(model, data_loader, data_loader_test, device, num_epochs, precedent_epoch, save_dir, optimizer): 
    model.train()
    # Initialize lists to store metrics
    train_losses = []
    
    # and a learning rate scheduler
    lr_scheduler = torch.optim.lr_scheduler.StepLR(
        optimizer,
        step_size=3,
        gamma=0.1
    )

    for epoch in range(num_epochs):
    # train for one epoch, printing every iteration
        metric_logger = train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=1)
        for k, v in metric_logger.intra_epoch_loss.items(): # <----- insertion here
            print(f"\n Key: {k}\n Value: {v}") # <----- insertion here
        # Plot loss throughout *this* epoch's training, save plot in results
        plot_training_loss(save_dir, epoch, **{k: v for k, v in metric_logger.intra_epoch_loss.items()}) # <----- insertion here
        
    # Get the average loss for this epoch
        epoch_loss = metric_logger.meters['loss'].global_avg
        train_losses.append(epoch_loss)

        # Save checkpoint
        save_checkpoint(model, optimizer, epoch + precedent_epoch, (save_dir + 'checkpoints'))
        
        # Update the learning rate
        lr_scheduler.step()

    return num_epochs + precedent_epoch, train_losses
```

Now run the script make sure that plots are saved in results/ which is now in the root of the project:

```{python}
model, optimizer, preprocess = get_model_instance_object_detection(2)

dataset: Plate_Image_Dataset = Plate_Image_Dataset.Plate_Image_Dataset(
        img_dir=str(img_dir), 
        annotations_file=str(annotations_file),
        transforms=preprocess, 
        )

# split the dataset in train and test set
dataset_size = 2
#validation_size = min(50, int(dataset_size // 5))  # Use 20% of data for testing, or 50 samples, whichever is smaller
indices = [int(i) for i in torch.randperm(dataset_size).tolist()]

#dataset_validation = torch.utils.data.Subset(dataset, indices[-validation_size:])
dataset_train = torch.utils.data.Subset(dataset, indices[:])

data_loader = torch.utils.data.DataLoader(
    dataset_train,
    batch_size=1,
    shuffle=True,
    collate_fn=utils.collate_fn
)

num_epochs = 2
precedent_epoch = 0
save_dir = '../'

epoch, loss_metrics = train(model, data_loader, None, device, num_epochs, precedent_epoch, save_dir, optimizer)

print("\n -end-")
```

That's the 'plot_training_loss' sorted for plotting the training loss metrics within an epoch. Now I need to write a function to handle plotting losses accross numerous epochs. This will be called in the train() helper function, after the epoch training loop. 'train_losses' stores the averaged losses at the end of each epoch of training. \

First, I'll define 'plot_train_losses_across_epochs':

```{python}
def plot_train_losses_across_epochs(save_dir: str, precedent_epoch: int, epochs: int, **kwargs):
    progression = [v for v in kwargs['progression']] # I'm not sure if list comprehension is neccessary here... perhaps just try progression = kwargs['progression']?
    loss_objectness = [v for v in kwargs['loss_objectness']]
    loss = [v for v in kwargs['loss']]
    loss_rpn_box_reg = [v for v in kwargs['loss_rpn_box_reg']]
    loss_classifier = [v for v in kwargs['loss_classifier']]
    lr = [v for v in kwargs['lr']] 
    
    # the metrics are passed using list comprehension in train()
    plt.figure(figsize=(10, 6))
    plt.plot(progression, loss, label='Loss')
    plt.plot(progression, loss_objectness, label='Loss Objectness')
    plt.plot(progression, loss_rpn_box_reg, label='Loss RPN Box Reg')
    plt.plot(progression, loss_classifier, label='Loss Classifier')

    plt.xlabel('Epochs')
    plt.ylabel('Loss / Log10')
    plt.yscale("log")
    plt.title(f'Loss Metrics from Epoch {precedent_epoch}-{precedent_epoch + epochs}')
    plt.legend()
    plt.grid(False)
    plt.savefig(f'{save_dir}/results/loss_metrics_epochs_{precedent_epoch}-{precedent_epoch + epochs}')
    plt.show()
```

Next I will call the plot_train_losses_across_epochs() inside train(), and will also modify loss_metrics so that it is a dictionary.

```{python}
def train(model, data_loader, data_loader_test, device, num_epochs, precedent_epoch, save_dir, optimizer): 
    model.train()
    # Initialize lists to store metrics
    losses_across_epochs = defaultdict(list)
    
    # and a learning rate scheduler
    lr_scheduler = torch.optim.lr_scheduler.StepLR(
        optimizer,
        step_size=3,
        gamma=0.1
    )

    for epoch in range(num_epochs):
    # train for one epoch, printing every iteration
        metric_logger = train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=1)
        for k, v in metric_logger.intra_epoch_loss.items(): # <----- insertion here
            print(f"\n Key: {k}\n Value: {v}") # <----- insertion here
            # Get the average loss metrics at this epoch
        for k, v in metric_logger.meters.items():
            losses_across_epochs[k].append(v.value) # see SmoothedValude in utils.py for value as v is a SmoothedValue object
        losses_across_epochs['progression'].append(((epoch+1)/num_epochs)*100)

        # plot loss throughout *this* epoch's training, save plot in results
        plot_training_loss(save_dir, epoch, **{k: v for k, v in metric_logger.intra_epoch_loss.items()})     # <----- insertion here

        # Save checkpoint
        save_checkpoint(model, optimizer, epoch + precedent_epoch, (save_dir + 'checkpoints'))
        
        # Update the learning rate
        lr_scheduler.step()

    # plot average train losses across several epochs
    plot_train_losses_across_epochs(save_dir, precedent_epoch, num_epochs, **{k: v for k, v in losses_across_epochs.items()})
    return num_epochs + precedent_epoch
```
Now running this script to check that this works...
```{python}
model, optimizer, preprocess = get_model_instance_object_detection(2)

dataset: Plate_Image_Dataset = Plate_Image_Dataset.Plate_Image_Dataset(
        img_dir=str(img_dir), 
        annotations_file=str(annotations_file),
        transforms=preprocess, 
        )

# split the dataset in train and test set
dataset_size = 2
#validation_size = min(50, int(dataset_size // 5))  # Use 20% of data for testing, or 50 samples, whichever is smaller
indices = [int(i) for i in torch.randperm(dataset_size).tolist()]

#dataset_validation = torch.utils.data.Subset(dataset, indices[-validation_size:])
dataset_train = torch.utils.data.Subset(dataset, indices[:])

data_loader = torch.utils.data.DataLoader(
    dataset_train,
    batch_size=1,
    shuffle=True,
    collate_fn=utils.collate_fn
)

num_epochs = 2
precedent_epoch = 0
save_dir = '../'

epoch = train(model, data_loader, None, device, num_epochs, precedent_epoch, save_dir, optimizer)

print("\n -end-")
```

Given the y-axis is logarithmic, and log10(0) is -infinity (NaN), and that matplotlib wants the x and y axis to be of equal length, I cannot start the plot x axis from 0%. Just something to keep in mind. \

I think the intra-epoch loss metrics will be useful in evaluating batch to batch variability. Mainly I think it will help for short tests to gain insight into a model's module dysfunction. \
Training plot generation now works. 

Now for handling plotting of evaluation metrics... \

### Considering appropriate visualisation of coco_eval metrics in plots:
Based on earlier outputs from the evaluate() helper function, I know that CocoEvaluator from [coco_eval.py](../src/torchvision_deps/coco_eval.py) produces Average Precision and Recall (AP and AR, respectively) at several IOU thresholds. It is important that these data are visualised in a way that gives insight into the model's performance prior to training on the cluster. I have included a brief discussion of Precision X Recall curves and their use in evaluating object detection networks' performance on a per-sample basis. For now, the AP and AR from coco_eval are sufficient. The idea is to make a call to evaluate() in [helper_training_functions](../src/plate_detect/helper_training_functions.py) after each epoch of training. I will then store the AP and AR metrics at each IOU threshold in a dictionary. Once training is complete, I will make a call to plot_eval_metrics() again from [helper_training_functions](../src/plate_detect/helper_training_functions.py) to plot AP and AR metrics as for each epoch of training. \

::: {.callout-note collapse="true"}
#### Precision X Recall curves 
An interesting discussion [here](https://github.com/rafaelpadilla/Object-Detection-Metrics?tab=readme-ov-file#precision-x-recall-curve) proposes the Precision x Recall curve which is implemented by the [PASCAL VOC Challenge](http://host.robots.ox.ac.uk/pascal/VOC/).

For clarity, I would like to define the following terms: \
- True Positive (TP): a correct detection, where IOU >= threshold. \
- False Positive (FP): an incorrect detection, where IOU < threshold. \
- False Negative (FN): a ground truth is not detected. \
- True Negative (TN): *this isn't used.* TN is any possible bounding boxes that were correctly not detected in an image. \
- Precision: ability of a model to identify only the relevant objects. This is given by TP/(TP + FP). \
- Recall: ability of a model to find all ground truth bounding boxes. It is given by TP/(TP + FN). \

The threshold is typically 50%, 75%, or 95%.

**Precision x Recall Curve:** \
The Precision x Recall curve is based on the rationale that a good object detector should only detect True Positives (0 FP = high precision), and should be able to correctly identify all ground truth objects (0 FN = high recall). On the other hand, a poor detector has to increase the number of detected objects in order to identify all ground truth objects. In this case, the detector compromises precision to improve recall. This is illustrated in such a plot, where as the recall of a detector increases, its precision decreases. \

**Area Under the Curve (AUC) of Precision x Recall Curve:** \
AUC provides a quantitative descriptor for the Precision x Recall curve to allow for comparisons.

Currently, [coco_eval](../src/torchvision_deps/coco_eval.py) is averaging the AP and AR for the whole validation set. As a reminder, this is an output produced by a call to the evaluate() helper function earlier: 
```{bash}
Test:  [ 0/50]  eta: 0:08:31  model_time: 10.2026 (10.2026)  evaluator_time: 0.0037 (0.0037)  time: 10.2212  data: 0.0148
Test:  [49/50]  eta: 0:00:10  model_time: 10.2246 (10.6478)  evaluator_time: 0.0047 (0.0055)  time: 10.4892  data: 0.0122
Test: Total time: 0:08:53 (10.6671 s / it)
Averaged stats: model_time: 10.2246 (10.6478)  evaluator_time: 0.0047 (0.0055)
Accumulating evaluation results...
DONE (t=0.07s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
[array([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.])] 
```

In order to create the plot, I would need to expose the metrics calculated for each sample individually. These are generated by engine and coco_eval, but are currently tucked away. Exposing metrics for each sample would require inspection and modification of these files. \
The [cocoapi github repo](https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocotools/cocoeval.py) is well commented.
:::

Firstly, I need to modify train to make a call to evaluate_model() after each epoch of training. I have added a print statement to confirm this code is behaving as expected.

```{python}
def train(model, data_loader, data_loader_test, device, num_epochs, precedent_epoch, save_dir, optimizer): 
    model.train()
    # Initialize lists to store metrics
    losses_across_epochs = defaultdict(list)
    evaluation_across_epochs = defaultdict(list)
    
    # and a learning rate scheduler
    lr_scheduler = torch.optim.lr_scheduler.StepLR(
        optimizer,
        step_size=3,
        gamma=0.1
    )

    for epoch in range(num_epochs):
    # train for one epoch, printing every iteration
        metric_logger = train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=1)
        for k, v in metric_logger.intra_epoch_loss.items(): # <----- insertion here
            print(f"\n Key: {k}\n Value: {v}") # <----- insertion here
            # Get the average loss metrics at this epoch
        for k, v in metric_logger.meters.items():
            losses_across_epochs[k].append(v.value) # see SmoothedValude in utils.py for value as v is a SmoothedValue object
        losses_across_epochs['progression'].append(((epoch+1)/num_epochs)*100)

        # plot loss throughout *this* epoch's training, save plot in results
        plot_training_loss(save_dir, epoch, **{k: v for k, v in metric_logger.intra_epoch_loss.items()})     # <----- insertion here

        # Save checkpoint
        save_checkpoint(model, optimizer, epoch + precedent_epoch, (save_dir + 'checkpoints'))

        # Evaluate model perfomance on holdout dataset and append results to evaluation metric dictionary with epoch as key
        eval_metrics = evaluate_model(model, data_loader_test, device)
        evaluation_across_epochs[f'{epoch}'] = eval_metrics

        # Update the learning rate
        lr_scheduler.step()

    # adding a print statement here to view evaluation_across_epochs using dict comprehension
    {print(k, '\n', v) for k, v in evaluation_across_epochs.items()}

    # plot average train losses across several epochs
    plot_train_losses_across_epochs(save_dir, precedent_epoch, num_epochs, **{k: v for k, v in losses_across_epochs.items()})
    return num_epochs + precedent_epoch
```
Then make a call to train():
```{python}
model, optimizer, preprocess = get_model_instance_object_detection(2)

dataset: Plate_Image_Dataset = Plate_Image_Dataset(
        img_dir=str(img_dir), 
        annotations_file=str(annotations_file),
        transforms=preprocess, 
        )

# split the dataset in train and test set
dataset_size = 4
validation_size = 2 # Use 20% of data for testing, or 50 samples, whichever is smaller
indices = [int(i) for i in torch.randperm(dataset_size).tolist()]

dataset_validation = torch.utils.data.Subset(dataset, indices[-validation_size:])
dataset_train = torch.utils.data.Subset(dataset, indices[:-validation_size])

data_loader = torch.utils.data.DataLoader(
    dataset_train,
    batch_size=1,
    shuffle=True,
    collate_fn=utils.collate_fn
)

data_loader_test = torch.utils.data.DataLoader(
    dataset_train,
    batch_size=1,
    shuffle=True,
    collate_fn=utils.collate_fn
)

num_epochs = 2
precedent_epoch = 0
save_dir = '../'

epoch = train(model, data_loader, data_loader_test, device, num_epochs, precedent_epoch, save_dir, optimizer)

print("\n -end-")
```

Based on this, it looks like the above code works. I should be able now to create a plot_eval_metrics function to create a line plot visualising the Average Precision (AP) and Average Recall (AR) metrics. You should be able to see that the IoU bbox metric output from the previous code block offers numerous AP and AR metrics. \ 

To make the plot simpler to visually interpret, I will at first only plot the AP and AR at the IoU thresholds averaged from 0.50:0.95 (this approach was introduced by COCO to more comprehensively evaluate model performance in contrast to single threshold evaluations - this is explained in detail [here](https://www.picsellia.com/post/coco-evaluation-metrics-explained)). \

Where there is a value of -1.000 in the bbox stats, there were no ground truth object in those size categories in the provided dataset. Of course, up until this point I have not been training on the entire dataset (given I have been debugging). This plotting function is preliminary, and if after training on a more substantial dataset there are changes to this output, I will investigate and consider their inclusion in the plot. For now, I will include AR for maxDets 100 and area= all (same for AP). When looking at the bbox stats output and the lists stored in the defaultdict evaluation_across_epochs, it is clear that the element index in the list stored for each key-value pair (epoch-bbox stats) corresponds to a line of the metric output from [coco_eval.py](../src/torchvision_deps/coco_eval.py). For each key, I am interested in the metric value at index 0 (AP @ IoU 0.50:0.95, area=all, maxDets=100) and index 8 (AR @ IoU 0.50:0.95, area=all, maxDets=100).

```{python}
def plot_eval_metrics(save_dir, precedent_epoch, num_epochs, **kwargs):
    progression = [k for k in kwargs] # should be equivalent to num_epochs - precedent_epoch

    # store metrics of interest in lists
    AP = [v[0] for k, v in kwargs.items()] 
    AR = [v[8] for k, v in kwargs.items()]
    
    plt.figure(figsize=(10, 6))
    plt.plot(progression, AP, label='AP @ IoU 0.50:0.95, area=all, maxDets=100')
    plt.plot(progression, AR, label='AR @ IoU 0.50:0.95, area=all, maxDets=100')

    plt.xlabel('Epochs')
    plt.ylabel('Value')
  
    plt.title(f'Evaluation Metrics from Epoch {precedent_epoch}-{precedent_epoch + num_epochs}')
    plt.legend()
    plt.grid(False)
    plt.savefig(f'{save_dir}/results/evaluation_metrics_epochs_{precedent_epoch}-{precedent_epoch + num_epochs}')
    plt.show()
```
Next, I will modify train() to call the above function after training:
```{python}
def train(model, data_loader, data_loader_test, device, num_epochs, precedent_epoch, save_dir, optimizer): 
    model.train()
    # Initialize lists to store metrics
    losses_across_epochs = defaultdict(list)
    evaluation_across_epochs = defaultdict(list)
    
    # and a learning rate scheduler
    lr_scheduler = torch.optim.lr_scheduler.StepLR(
        optimizer,
        step_size=3,
        gamma=0.1
    )

    for epoch in range(num_epochs):
    # train for one epoch, printing every iteration
        metric_logger = train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=1)
        for k, v in metric_logger.intra_epoch_loss.items(): # <----- insertion here
            print(f"\n Key: {k}\n Value: {v}") # <----- insertion here
            # Get the average loss metrics at this epoch
        for k, v in metric_logger.meters.items():
            losses_across_epochs[k].append(v.value) # see SmoothedValude in utils.py for value as v is a SmoothedValue object
        losses_across_epochs['progression'].append(((epoch+1)/num_epochs)*100)

        # plot loss throughout *this* epoch's training, save plot in results
        plot_training_loss(save_dir, epoch, **{k: v for k, v in metric_logger.intra_epoch_loss.items()})     # <----- insertion here

        # Save checkpoint
        save_checkpoint(model, optimizer, epoch + precedent_epoch, (save_dir + 'checkpoints'))

        # Evaluate model perfomance on holdout dataset and append results to evaluation metric dictionary with epoch as key
        eval_metrics = evaluate_model(model, data_loader_test, device)
        evaluation_across_epochs[f'{epoch}'] = eval_metrics

        # Update the learning rate
        lr_scheduler.step()

    # adding a print statement here to view evaluation_across_epochs using dict comprehension
    {print(k, '\n', v) for k, v in evaluation_across_epochs.items()}

    # plot average train losses across several epochs
    plot_train_losses_across_epochs(save_dir, precedent_epoch, num_epochs, **{k: v for k, v in losses_across_epochs.items()})
    plot_eval_metrics(save_dir, precedent_epoch, num_epochs, **{k: v for k, v in evaluation_across_epochs.items()})
    return num_epochs + precedent_epoch
```

Now I will make another call to train to verify that these plots are saved and displayed:
```{python}
epoch = train(model, data_loader, data_loader_test, device, num_epochs, precedent_epoch, save_dir, optimizer)

print("\n -end-")
```

This code seems to work without any overt error. Now, I want to train for 15 epochs on the whole positives dataset (495 without augmentation, reduced to 445 after subsetting a holdout set of size 50). At this point, I delete the checkpoints and results from the debugging I have been doing (they should be overwritten anyway).

```{python}
import caffeine # prevents mac from sleeping while running this code block (see here: https://github.com/jpn--/caffeine)

# split the dataset in train and test set
dataset_size = len(dataset) # already defined 
validation_size = 50
indices = [int(i) for i in torch.randperm(dataset_size).tolist()]

dataset_validation = torch.utils.data.Subset(dataset, indices[-validation_size:])
dataset_train = torch.utils.data.Subset(dataset, indices[:-validation_size])

data_loader = torch.utils.data.DataLoader(
    dataset_train,
    batch_size=1,
    shuffle=True,
    collate_fn=utils.collate_fn
)

data_loader_test = torch.utils.data.DataLoader(
    dataset_validation,
    batch_size=1,
    shuffle=True,
    collate_fn=utils.collate_fn
)

num_epochs = 15
precedent_epoch = 0
save_dir = '../'

epoch = train(model, data_loader, data_loader_test, device, num_epochs, precedent_epoch, save_dir, optimizer)

print("\n -end-")
```

## Inspecting Results after 15 Epochs of Training:
- Firstly, it is clear that evaluation is not functioning correctly (see below). The issue could lie in numerous places, but the best place to start looking I think is engine's evaluate function, or indeed coco_eval. **Fixing this is a current priority**, as evaluation metrics will be fundamental in the comparison of models in future analyses. \
![evaluation metrics epochs 0-15](../results/evaluation_metrics_epochs_0-15.png) \

- The loss metrics are more promising in that they actually change throughout training. With this said, it is clear the network is struggling to learn to identify plates. \

### Discussion of Loss Metrics Epoch 0-15:
![Loss Metrics Epoch 0-15](../results/evaluation_metrics_epochs_0-15.png) \

Firstly, loss (blue) appears identical to loss for the Region Proposal Network box regression (green). Loss should be equal to the total loss over the period trained. This is due to the different loss metrics being orders of magnitude apart, where due to the logarithmic y-axis, their summation is decimally more than the loss for the RPN box regression. This could be solved by normalising the total loss, or more simply by separating the metrics into different graphs and displaying them in multiple panels. \

Also, when plotting the loss metrics across several epochs, the x=axis scale units should be epoch count, not percentage progression through training This can be fixed simply by the following modification: \

```{python}
def plot_train_losses_across_epochs(save_dir: str, precedent_epoch: int, epochs: int, **kwargs):
    loss_objectness = [v for v in kwargs['loss_objectness']]
    loss = [v for v in kwargs['loss']]
    loss_rpn_box_reg = [v for v in kwargs['loss_rpn_box_reg']]
    loss_classifier = [v for v in kwargs['loss_classifier']]
    lr = [v for v in kwargs['lr']] 

    training_range =  [i for i in range(precedent_epoch, precedent_epoch + epochs)]  # list of epochs as ints
    
    # the metrics are passed using list comprehension in train()
    plt.figure(figsize=(10, 6))
    plt.plot(training_range, loss, label='Loss')
    plt.plot(training_range, loss_objectness, label='Loss Objectness')
    plt.plot(training_range, loss_rpn_box_reg, label='Loss RPN Box Reg')
    plt.plot(training_range, loss_classifier, label='Loss Classifier')

    plt.xlabel('Epoch Count')
    plt.ylabel('Loss / Log10')
    plt.yscale("log")
    plt.title(f'Loss Metrics from Epoch {precedent_epoch}-{precedent_epoch + epochs}')
    plt.legend()
    plt.grid(False)
    plt.savefig(f'{save_dir}/results/loss_metrics_epochs_{precedent_epoch}-{precedent_epoch + epochs}')
    plt.show()
```

The orders of magnitude difference between the loss for the RPN box regression relative to the loss for the classifier head and the RPN objectness loss is likely due to the fact that both the objectness loss and classifier loss use the Binary Cross Entropy loss function, whereas box regression loss for the RPN uses L1 or 'Huber' loss which measures pixel-wise differences in bounding box coordinates, and can consequently be much larger. \ 

** Explore these loss functions here **

**Discussing Loss Objectness:** \
Loss objectness (orange) initially decreases before ascending slightly and plateauing. Loss objectness evaluates the objectness score produced by the RPN relative to ground truth. Specifically: \
- Objectness score: the probability that an object exists in a given ROI proposed by the RPN. This is a continuous logit (unnormalised probability) passed through a sigmoid function (output between 0 and 1). \
- Loss objectness: in this model it describes the performance of rpn.head.cls_logits which is a [2D convolutional layer](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) of dimensions 256x3, kernel size 1.  This is relative to the ground truth, which is 1 if a given anchor box overlaps a bounding box label (the ground truth) at some IoU threshold (0.50 for example), and is 0 otherwise. The [binary cross entropy loss](https://www.youtube.com/watch?v=Pwgpl9mKars) function is used to quantitatively describe the degree to which the probability is concordant with the ground truth binary value, where loss approaches infinity as these variables diverge, and approaches 0 as they converge (i.e., as they 'agree'). \

Hence, the loss objectness initially decreasing across epochs of training is good, but its later resurgence shows the network is struggling to learn to identify plates.
::: {.callout-note collapse="true"}
## Adding image augmentation to helper functions...
This is definitely something that needs to be implemented in the future. Here are some brief launch-points I can use if I come back to this. My priority is getting some training done for more than just 1 epoch on the HPC, which in itself requires a locally defined Faster R-CNN class.

See [here](https://pdf.sciencedirectassets.com/272206/1-s2.0-S0031320322X00149/1-s2.0-S0031320323000481/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMX%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJIMEYCIQDHGwjyK8inLmJunlMGBkjPI%2B2RQhf29a3PpwUh%2FtcGtQIhAKXEABtOLty%2FeIaoLRArLOo1AG1g5sCAFuBCid9AF0jFKrMFCC4QBRoMMDU5MDAzNTQ2ODY1IgzGC8wcf0hM6U6%2BuigqkAXE2vBrPyLEzEszAdKmPGPE%2Fe2PuWiGsl4asY0mfwrTgBHcCJE%2B7mvDq8J8zDT1gLeiyrumMRHkpG17xRZHf3zGAu%2BV8ElbuBhsVuHZAb%2FnX4n5e7oLaKOO4YW2y4ETsCtwLroPKH4FcWxTretUzrlbhCAS%2BX5d861gT0Y6UTz1LEbIEgq4OWRRXSW5Yn%2Fq1DxzX8wOvw0pw6A%2F5MgVxcz67diPlmC7ph58goHCtHrkUJGqOTWKge%2B8ZCPPPkphqh3I9KbDfhGTKfhDaucbWl1J7eVZoJ0VX8ArJkjW3TGhkS73brzjdN7HR8wRRNBiVXqJhYPofxKyPqKASiRRj7MRIhOgzbkU5d0z4zxNt8Xliwc0bCymJ%2BPepBkjnyJ%2F5%2FCodwL8m5O6m3La5hrVAAjC9x7NIt%2Btecb176q%2BDRB3ELYwD8v0%2F0c8jTUmUeNZatzGPH3WTUaC%2Bo%2B4nKsrpAobYAYOQX8JmcmlT9RO6kTkhAVKsuSN54kl1K%2Fs977nu5B%2FrvePps7UFcN9T580vE%2B2oFa1uGwdbJr1IYZslqT3od3Gz0NmynC1sV%2FRgAYvjk6IBDG7W4FaDmmMYF8vOlD%2FBxZUuLMmw%2BcHlwJIROMn73F77fXo2K0IQMByrmiEg%2FK%2BvKLVLnCrRjpo%2FqnnFipG5FbBforeDYUmN9x6lN%2BH9S%2Bnho8kKXrwJscWXC9Lvs90E8ccWorPjPC52be0W7Y%2FxqEtfvutJ4hnvDT9ilwvpNbu%2BdHsbmwGsTiXzRjwNy4ZhCo7MLSITLM9VR9mVRci4ulnxV2QpXvSBvEYB1%2BF8VcFQIiTavC69L0sUs%2BFYZyZjExT2gW4%2Fc%2Bldptpmdz2V4WIH%2BHKjNX%2BaL0CGKRLejDyj8S4BjqwAe0oxq1p9jccRHXzaAZcTmiwV3fBbPgX4CN%2FuTG8kVpMikl6fAyT4tGm8FD3wT8sPtI2raxqEI0wYIZ33t0t4vYB0NQjFcWSjF%2FocTTGT%2FQITMsLBIP3YFOTJG8SLlRnUZ8sxKsoTfZcs37ToI%2BWZ%2FZtBoj%2Fc0iuU97k3KehPAGwKs8%2BBQRJpXDum8l3y6QN5zGp1tFxI99lg7Nu0RzMg0ZOiDbU0m3pW5rTiMbPZQdn&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20241017T140230Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYS2ZZAFYX%2F20241017%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=b216340b0855727efb00bc68031e19f29813254ab964d60a1685e44e45714c89&hash=dd2c23826d13b059c59a8112432e10407ff621d79cac2d6790b0795c02f0f467&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0031320323000481&tid=spdf-95560c99-4f04-404c-86e1-a3a1717cba5c&sid=6f3a660355f9464580690b17c2898370a5bagxrqb&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=1d04570653560a560e5c52&rr=8d40d3a30a2c63f3&cc=gb) for a comprehensive overview of image augmentation techniques for deep learning. 

There they achieved an accuracy improvement of around 1.5% using [mixup](https://arxiv.org/pdf/1710.09412), which uses 'convex combinations of pairs of examples and their labels' to improve generalisation of network architectures.
:::

Plots of loss metrics within an epoch's training exhibit high variance, which is to be expected:
![loss objectness epoch 13](../results/loss_metrics_epoch_13.png) \

**The more probable issue...** \
The default weights loaded for this model are trained on ImageNet images, which comprises almost exclusively objects that exhibit '[Lambertian reflectance](https://en.wikipedia.org/wiki/Lambertian_reflectance)' (i.e., typical matte objects with consistent surface appearance regardless of the its positional relationship to an observer due to diffuse surface reflectivity). \

In the case of microbe growth plates, which are reflective and transparent 'non-Lambertian' objects, the network parameters are primed to extract features of objects with Lambertian surfaces. As a consequence, the backbone's output feature maps may activate more strongly for Lambertian objects - in other words, it most likely struggles to 'see' and characterise plates in image samples, but rather other objects present like the wireframe rack, or the incubator surface itself. \

The most obvious way to test this is to initialise the backbone parameters randomly or unfreeze these layers (instead of transfer learning) and optimise them also at each backpropagation. This would be explored in the subsequent analysis.

::: {.callout-note collapse="true"}
If it is the case that my current dataset is insufficient in size, one potentially efficient way to train the backbone to extract relevant features is to train it on a much larger publicly available transparent object dataset (with labels provided). For example, google's [ClearGrasp Transparent Object Dataset](https://sites.google.com/view/cleargrasp/home) contains 50,000 photorealistic renders of transparents objects (synthetic dataset) and 286 additional real-world images containing labelled transparent objects. The [Real-World Transparent Objects Dataset](https://sites.google.com/view/keypose/?pli=1) provides 48,000 labeled real-world images of transparent objects at 720p resolution.
:::


### Fixing Evaluation Metric Generation and Plotting:
Firstly, in looking back over my code, I notice in get_model_instance_object_detection I have the bounding box confidence score threshold set to 0.0001 - this is strangely low. I cannot remember why I set it to such a low value as the pytorch tutorial that I was following does not do so. Regardless, I change it here to 0.5.
```{python}
def get_model_instance_object_detection(num_class: int) -> fasterrcnn_resnet50_fpn_v2:
    # New weights with accuracy 80.858%
    weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT # alias is .DEFAULT suffix, weights = None is random initialisation, box MAP 46.7, params, 43.7M, GFLOPS 280.37 https://github.com/pytorch/vision/pull/5763
    model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.5)
    preprocess = weights.transforms()
    # finetuning pretrained model by transfer learning
    # get num of input features for classifier
    in_features = model.roi_heads.box_predictor.cls_score.in_features

    params = [p for p in model.parameters() if p.requires_grad]
    optimizer = torch.optim.SGD( # optimizer defined in model getter according to pytorch 'recipes'
        params,
        lr=0.005,
        momentum=0.9,
        weight_decay=0.0005
    )
    # replace the pre-trained head with a new one
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_class)
    return model, optimizer, preprocess
```

It would also be good to check that the dataset_validation isn't empty and loads correctly.

```{python}
print(len(dataset_validation[1]), '\n', dataset_validation[1])

print("\n -end-")
```

This raises a red-flag I accepted earlier after applying the preprocess transform before the feed-forward through the network. This is definitely something that needs to be ironed out. The pix

When following the [pytorch tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html) when creating the Plate_Image_Dataset class, in the __getitem__ dunder method associated with the class the tutorial applies the transforms passed to the class constructor to both the images and the target (the bounding boxes). I am dubious as to whether I have interpreted this correctly for a few reasons. Firstly, when I tried to do so when writing the class, as such:

```{python}
#| eval: false
if self.transforms is not None:
            img, target = self.transforms(img, target)  # <-- as in the pytorch tutorial
```

I got an error where the transforms function wasn't expecting a tuple. I also got a type error when passing target to the function.

To bypass this, I rewrote the line as:
```{python}
#| eval: false
if self.transforms is not None:
            img = self.transforms(img)
            target["boxes"] = self.transforms(target["boxes"])
```
Passing img and target boxes key through to transforms jumped over the error, but I am sure this is causing problems given the bounding box tensor from dataset_validation[1]'s target from the previous print statement holds incredibly small decimal values. \

My rationale at the time of writing the Plate_Image_Dataset was that whatever transforms I apply to the image tensor elements should also be applied to the bounding box coordinates. My thinking was that if they are downscaled prior to feed forward, the boudning boxes must be resized accordingly so that they bound the same, albeit downsized region. The issue with this is that the elements of the img tensor store completely different information to that of the bounding box tensor. The bounding box tensor stores the bounding box coordinates, but img stores pixel intensities. This is obvious in retrospect, but I think I overlooked it as I familiarised myself with pytorch - silly me. \

Resizing of the img and the bounding boxes does indeed need to be coordinated properly as mentioned before, but the need to normalise img's pixel intensities means I need to be very conscious of what transforms are being applied to what. \

Downscaling of the img tensor is actually handled by the ResNet module. What's interesting is that the torchvision transforms are written for images alone and not their bounding boxes (see [here](https://pseudo-lab.github.io/Tutorial-Book-en/chapters/en/object-detection/Ch3-preprocessing.html)), which again suggests this is causing issues in training and validation. \

I am going to visualise what is happening to the image samples and their bounding boxes prior to the feed forward through the network. To do so I will modify the previously defined plot_prediction function to create a new plot_sample function. This function's signature will include a dataset of type Plate_Image_Dataset and an index (type int).

```{python}
def plot_sample(dataset, index):
    img, target = dataset[index]
   
    output_image = draw_bounding_boxes(img, target["boxes"], colors="red")
    plt.figure(figsize=(12, 12))
    plt.imshow(output_image.permute(1, 2, 0)) # permuting images as pytorch is [channels, height, width], matplotlib is [height, width, channels]
    plt.savefig(f'{index}.png', bbox_inches='tight') # tight removes whitespace
```

```{python}

plot_sample(dataset_validation, 2)


```

There is no red bounding box displayed here. This is most likely because of the transforms being applied by the Plate_Image_Dataset item getter. I will change this now and try again.

** Removing Transforms and Adding Insightful Prints: **
```{python}
class Plate_Image_Dataset(torch.utils.data.Dataset):
    def __init__(self, annotations_file: str, img_dir: str, transforms=None):
        self.img_labels = pd.read_csv(annotations_file)
        self.img_files = [f for f in glob(img_dir + "/*.png")]
        self.transforms = transforms

    def __len__(self) -> int:
        return len(self.img_files)

    def __getitem__(self, idx: int) -> tuple[torch.Tensor, Dict]:
        # Load image
        img = read_image(self.img_files[idx])
        
        # Debug print
        print(f"\nProcessing image {idx}:")
        print(f"Image shape: {img.shape}")
        print(f"Raw coordinates from CSV:")
        print(f"xmins: {self.img_labels['xmins'][idx]}")
        print(f"ymins: {self.img_labels['ymins'][idx]}")
        print(f"xmaxs: {self.img_labels['xmaxs'][idx]}")
        print(f"ymaxs: {self.img_labels['ymaxs'][idx]}")

        # Extract coordinates
        x1 = [int(item) for item in re.findall(r'\d+', str(self.img_labels['xmins'][idx]))]
        y1 = [int(item) for item in re.findall(r'\d+', str(self.img_labels['ymins'][idx]))]
        x2 = [int(item) for item in re.findall(r'\d+', str(self.img_labels['xmaxs'][idx]))]
        y2 = [int(item) for item in re.findall(r'\d+', str(self.img_labels['ymaxs'][idx]))]

        print(f"Extracted coordinates:")
        print(f"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}")

        # Create boxes tensor
        num_objs = len(x1)
        boxes = torch.tensor(
            [[x1[i], y1[i], x2[i], y2[i]] for i in range(num_objs)],
            dtype=torch.float32
        )
        
        print(f"Boxes tensor before TVTensor:")
        print(boxes)

        # Create labels tensor (1 for plate, as 0 is background)
        labels = torch.ones((num_objs,), dtype=torch.int64)

        # Calculate areas
        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])

        # Convert image to TVTensor
        img = tv_tensors.Image(img)

        # Create target dictionary
        target = {
            "boxes": tv_tensors.BoundingBoxes(
                boxes,
                format="XYXY",
                canvas_size=F.get_size(img)
            ),
            "labels": labels,
            "image_id": idx,
            "area": area,
            "iscrowd": torch.zeros((num_objs,), dtype=torch.int64)
        }

        print(f"Final boxes in target:")
        print(target["boxes"])

        # Apply transforms if any
        if self.transforms is not None:
            try:
                img = self.transforms(img)
                # Don't transform boxes directly
            except Exception as e:
                print(f"Transform error: {e}")

        return img, target

    def test_item(self, idx: int):
        """Helper method to test single item processing"""
        print(f"\nTesting item {idx}")
        img, target = self[idx]
        print(f"Image shape: {img.shape}")
        print(f"Number of boxes: {len(target['boxes'])}")
        print(f"Boxes: {target['boxes']}")
        print(f"Labels: {target['labels']}")
        return img, target
```

```{python}
# Create dataset
dataset = Plate_Image_Dataset(
    annotations_file=str(annotations_file),
    img_dir=str(img_dir),
    transforms=preprocess
)

# Test a single item
dataset.test_item(0)

# Create data loaders
train_loader = torch.utils.data.DataLoader(
    dataset_train,
    batch_size=1,
    shuffle=True,
    collate_fn=utils.collate_fn
)

validation_loader = torch.utils.data.DataLoader(
    dataset_validation,
    batch_size=1,
    shuffle=False,
    collate_fn=utils.collate_fn
)

# Test first batch
images, targets = next(iter(train_loader))
print("\nFirst batch:")
print(f"Image shape: {images[0].shape}")
print(f"Target boxes: {targets[0]['boxes']}")
```
This shows the coordinates are being loaded correctly, but they are being messed up by something. This reminded me of an article I read about collate functions and object detection (see [here](https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_e2e.html#sphx-glr-auto-examples-transforms-plot-transforms-e2e-py)). In the data laoding and training loop section, there's a series of comments that states that the default collate function is problematic in object detection and a custom function should be provided instead. This is the problem!


```{python}
def custom_collate_fn(batch):
    """Custom collate function with debugging"""
    images = []
    targets = []
    
    print("\nInside collate_fn:")
    for i, (img, target) in enumerate(batch):
        print(f"Item {i} boxes before collation:", target["boxes"])
        images.append(img)
        
        if isinstance(target["boxes"], tv_tensors.BoundingBoxes):
            boxes_tensor = target["boxes"].as_tensor()
            print(f"Item {i} boxes after conversion:", boxes_tensor)
            target["boxes"] = boxes_tensor
            
        targets.append(target)
    
    print("Final collated boxes:", targets[0]["boxes"])
    return images, targets
```


```{python}
# Then use it in DataLoader:
train_loader = torch.utils.data.DataLoader(
    dataset_train,
    batch_size=1,
    shuffle=True,
    collate_fn=custom_collate_fn  # Use custom collate function
)

validation_loader = torch.utils.data.DataLoader(
    dataset_validation,
    batch_size=1,
    shuffle=False,
    collate_fn=custom_collate_fn  # Use custom collate function
)

    # Test first batch
images, targets = next(iter(train_loader))
print("\nFirst batch:")
print(f"Image shape: {images[0].shape}")
print(f"Target boxes: {targets[0]['boxes']}")

```

Even after trying to address the collate function issue, the cause of the bounding box issue is even further upstream. This narrows it down to the TV Tensor.

Let's look into it! Firstly, I will keep both the bounding boxes and image tensors as pure tensors instead of wrapping them as TVTensor.

```{python}
class Plate_Image_Dataset(torch.utils.data.Dataset):
    def __init__(self, annotations_file: str, img_dir: str, transforms=None):
        self.img_labels = pd.read_csv(annotations_file)
        self.img_files = [f for f in glob(img_dir + "/*.png")]
        self.transforms = transforms

    def __len__(self) -> int:
        return len(self.img_files)

    def __getitem__(self, idx: int) -> tuple[torch.Tensor, Dict]:
        # Load image
        img = read_image(self.img_files[idx])
        print(f"\nProcessing image {idx}")
        
        # Get raw coordinates
        print("Raw coordinates from CSV:")
        x1 = [int(item) for item in re.findall(r'\d+', str(self.img_labels['xmins'][idx]))]
        y1 = [int(item) for item in re.findall(r'\d+', str(self.img_labels['ymins'][idx]))]
        x2 = [int(item) for item in re.findall(r'\d+', str(self.img_labels['xmaxs'][idx]))]
        y2 = [int(item) for item in re.findall(r'\d+', str(self.img_labels['ymaxs'][idx]))]
        print(f"Extracted coordinates: x1={x1}, y1={y1}, x2={x2}, y2={y2}")

        # Create boxes tensor - keeping as regular tensor, not TVTensor
        boxes = torch.tensor([[x1[0], y1[0], x2[0], y2[0]]], dtype=torch.float32)
        print("Boxes after tensor creation:", boxes)

        # Keep image as regular tensor initially, instead of wrapping in TVTensor
        target = {
            "boxes": boxes,  # Regular tensor, not TVTensor
            "labels": torch.ones((1,), dtype=torch.int64),
            "image_id": idx,
            "area": (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),
            "iscrowd": torch.zeros((1,), dtype=torch.int64)
        }

        print("Final boxes in target:", target["boxes"])

        if self.transforms is not None:
            img = self.transforms(img)
            
        return img, target
```

I also want to simplify the custom collate function:
```{python}
def custom_collate_fn(batch):
    images = []
    targets = []
    print("\nCollating batch:")
    
    for img, target in batch:
        print("Box coordinates:", target["boxes"])
        images.append(img)
        targets.append(target)
    
    return images, targets

```

Testing it:
```{python}
# Create dataset
dataset = Plate_Image_Dataset(
    annotations_file=str(annotations_file),
    img_dir=str(img_dir),
    transforms=None  # Let's test without transforms first
)

# Test single item
print("\nTesting single item:")
img, target = dataset[0]
print("Direct access boxes:", target["boxes"])

# Create loader with minimal batch processing
loader = torch.utils.data.DataLoader(
    dataset,
    batch_size=1,
    shuffle=False,
    collate_fn=custom_collate_fn
)

# Test batch
print("\nTesting batch:")
batch = next(iter(loader))
print("Batch boxes:", batch[1][0]["boxes"])
```

Great! The problem has been removed. I will now systematically add them back to identify the cause.

```{python}
class Plate_Image_Dataset(torch.utils.data.Dataset):
    def __init__(self, annotations_file: str, img_dir: str, transforms=None):
        self.img_labels = pd.read_csv(annotations_file)
        self.img_files = [f for f in glob(img_dir + "/*.png")]
        self.transforms = transforms

    def __len__(self) -> int:
        return len(self.img_files)
        
    def __getitem__(self, idx: int) -> tuple[torch.Tensor, Dict]:
        img = read_image(self.img_files[idx])
        img = tv_tensors.Image(img)  # Convert only image to TVTensor

        # Get raw coordinates
        print("Raw coordinates from CSV:")
        x1 = [int(item) for item in re.findall(r'\d+', str(self.img_labels['xmins'][idx]))]
        y1 = [int(item) for item in re.findall(r'\d+', str(self.img_labels['ymins'][idx]))]
        x2 = [int(item) for item in re.findall(r'\d+', str(self.img_labels['xmaxs'][idx]))]
        y2 = [int(item) for item in re.findall(r'\d+', str(self.img_labels['ymaxs'][idx]))]
        print(f"Extracted coordinates: x1={x1}, y1={y1}, x2={x2}, y2={y2}")

        # Rest of your coordinate processing...
        boxes = torch.tensor([[x1[0], y1[0], x2[0], y2[0]]], dtype=torch.float32)

        target = {
            "boxes": boxes,  # Keep as regular tensor
            "labels": torch.ones((1,), dtype=torch.int64),
            "image_id": idx,
            "area": (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),
            "iscrowd": torch.zeros((1,), dtype=torch.int64)
        }
    
        return img, target
```
That worked! Now I will try to add transforms back:

```{python}
class Plate_Image_Dataset(torch.utils.data.Dataset):
    def __init__(self, annotations_file: str, img_dir: str, transforms=None):
        self.img_labels = pd.read_csv(annotations_file)
        self.img_files = [f for f in glob(img_dir + "/*.png")]
        self.transforms = transforms

    def __len__(self) -> int:
        return len(self.img_files)
        
    def __getitem__(self, idx: int) -> tuple[torch.Tensor, Dict]:
        img = read_image(self.img_files[idx])
        img = tv_tensors.Image(img)  # Convert only image to TVTensor

        # Get raw coordinates
        print("Raw coordinates from CSV:")
        x1 = [int(item) for item in re.findall(r'\d+', str(self.img_labels['xmins'][idx]))]
        y1 = [int(item) for item in re.findall(r'\d+', str(self.img_labels['ymins'][idx]))]
        x2 = [int(item) for item in re.findall(r'\d+', str(self.img_labels['xmaxs'][idx]))]
        y2 = [int(item) for item in re.findall(r'\d+', str(self.img_labels['ymaxs'][idx]))]
        print(f"Extracted coordinates: x1={x1}, y1={y1}, x2={x2}, y2={y2}")

        # Rest of your coordinate processing...
        boxes = torch.tensor([[x1[0], y1[0], x2[0], y2[0]]], dtype=torch.float32)

        target = {
            "boxes": boxes,  # Keep as regular tensor
            "labels": torch.ones((1,), dtype=torch.int64),
            "image_id": idx,
            "area": (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),
            "iscrowd": torch.zeros((1,), dtype=torch.int64)
        }

        # Apply transforms to image only
        if self.transforms is not None:
            img = self.transforms(img)
    
        return img, target
```

```{python}
transforms = T.Compose([
    T.ConvertImageDtype(torch.float32),  # Just convert dtype
    T.Normalize(mean=[0.485, 0.456, 0.406],  # Standard ImageNet normalization
               std=[0.229, 0.224, 0.225])
])

dataset = Plate_Image_Dataset(
    annotations_file=str(annotations_file),
    img_dir=str(img_dir),
    transforms=transforms
)
```

More testing now:
```{python}
# Test the dataset
print("\nTesting single item:")
img, target = dataset[0]
print("Image shape:", img.shape)
print("Image dtype:", img.dtype)
print("Direct access boxes:", target["boxes"])

# Test with dataloader
loader = torch.utils.data.DataLoader(
    dataset,
    batch_size=1,
    shuffle=False,
    collate_fn=custom_collate_fn
)

# Check first batch
batch = next(iter(loader))
print("\nBatch test:")
print("Image shape:", batch[0][0].shape)
print("Batch boxes:", batch[1][0]["boxes"])
```

This works! Quickly adding transforms within the model getter to handle normalisation of pixel intensities (according to [ImageNet normalisation](https://discuss.pytorch.org/t/discussion-why-normalise-according-to-imagenet-mean-and-std-dev-for-transfer-learning/115670)).

```{python}
def get_model_instance_object_detection(num_class: int):
    # New weights with accuracy 80.858%
    weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT 
    model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.5)
    
    # Don't use the default preprocess from weights
    # Instead, use our custom transforms we just tested
    transforms = T.Compose([
        T.ConvertImageDtype(torch.float32),
        T.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        )
    ])
    
    # get num of input features for classifier
    in_features = model.roi_heads.box_predictor.cls_score.in_features

    # replace the pre-trained head with a new one
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_class)

    params = [p for p in model.parameters() if p.requires_grad]
    optimizer = torch.optim.SGD(
        params,
        lr=0.005,
        momentum=0.9,
        weight_decay=0.0005
    )

    return model, optimizer, transforms
```

Now I will try to train again for 15 epochs and see if (fingers crossed) the CocoEvaluator class functions correctly now.

```{python}
# Setup data
dataset = Plate_Image_Dataset(
    annotations_file=str(annotations_file),
    img_dir=str(img_dir),
    transforms=transforms  # Using the new transforms 
)

# Split dataset with correct boxes now
dataset_size = len(dataset)
validation_size = 50  # Or whatever size you want
indices = [int(i) for i in torch.randperm(dataset_size).tolist()]

dataset_validation = torch.utils.data.Subset(dataset, indices[-validation_size:])
dataset_train = torch.utils.data.Subset(dataset, indices[:-validation_size])

# Create data loaders with custom_collate_fn
train_loader = torch.utils.data.DataLoader(
    dataset_train,
    batch_size=1,
    shuffle=True,
    collate_fn=custom_collate_fn  # Using new collate function
)

validation_loader = torch.utils.data.DataLoader(
    dataset_validation,
    batch_size=1,
    shuffle=False,
    collate_fn=custom_collate_fn  # Using new collate function
)

# Rest of your training setup
model, optimizer, preprocess = get_model_instance_object_detection(num_class=2)
model.to(device)

# Training loop
num_epochs = 15
precedent_epoch = 0
save_dir = '../'

epoch = train(model, train_loader, validation_loader, device, num_epochs, precedent_epoch, save_dir, optimizer)
```
This gave a type error, because the model is expecting float32 images in range [0,1].

```{python}
class Plate_Image_Dataset(torch.utils.data.Dataset):
    def __init__(self, annotations_file: str, img_dir: str, transforms=None):
        self.img_labels = pd.read_csv(annotations_file)
        self.img_files = [f for f in glob(img_dir + "/*.png")]
        self.transforms = transforms

    def __len__(self) -> int:
        return len(self.img_files)
    
    def load_image(self, idx): # adding this as normalisation will mess up plotting with matplotlib
        print(self.img_files)
        img = read_image(self.img_files[idx])
        return img

    def __getitem__(self, idx: int) -> tuple[torch.Tensor, Dict]:
        # Load image 
        img = read_image(self.img_files[idx])
        # Convert image to TVTensor
        img = tv_tensors.Image(img)
        
        # Get coordinates
        x1 = [int(item) for item in re.findall(r'\d+', str(self.img_labels['xmins'][idx]))]
        y1 = [int(item) for item in re.findall(r'\d+', str(self.img_labels['ymins'][idx]))]
        x2 = [int(item) for item in re.findall(r'\d+', str(self.img_labels['xmaxs'][idx]))]
        y2 = [int(item) for item in re.findall(r'\d+', str(self.img_labels['ymaxs'][idx]))]
        
        # Create boxes tensor
        num_objs = len(x1)
        boxes = torch.tensor(
            [[x1[i], y1[i], x2[i], y2[i]] for i in range(num_objs)],
            dtype=torch.float32
        )
        
        # Create target dictionary
        target = {
            "boxes": tv_tensors.BoundingBoxes(
                boxes,
                format="XYXY",
                canvas_size=F.get_size(img)
            ),
            "labels": torch.ones((num_objs,), dtype=torch.int64),
            "image_id": idx,
            "area": (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),
            "iscrowd": torch.zeros((num_objs,), dtype=torch.int64)
        }

        # Apply transforms only to img
        if self.transforms is not None:
            img, target = self.transforms(img, target)
        
        return img, target

```

```{python}
def plot_sample(dataset, index):
    _, target = dataset[index]
    img = dataset.load_image(index)
   
    output_image = draw_bounding_boxes(img, target["boxes"], colors="red")
    plt.figure(figsize=(12, 12))
    plt.imshow(output_image.permute(1, 2, 0)) # permuting images as pytorch is [channels, height, width], matplotlib is [height, width, channels]

```

Just check the images are being transformed properly before feed forward by plot:
```{python}
# Setup data
model, optimizer, transforms = get_model_instance_object_detection(2)
dataset = Plate_Image_Dataset(
    annotations_file=str(annotations_file),
    img_dir=str(img_dir),
    transforms=transforms  # Using the new transforms 
)
print(dataset[2])
plot_sample(dataset, 2)
```

This revealed another problem - the image being loaded does not correspond correctly to the bounding box coordinates. This is strange as the image at line 4 (index 2) of [labels.csv](../lib/labels.csv) has the same coordinates as the at index 2 of the dataset. The associated image 1_3_0916162509.png clearly is concordant with the bounding box coordinates produced by dataset[2], but is not the one that is loaded at that index in the dataset. \

I will fix this by sorting glob, just as in xml_to_csv in [analysis 0001](analyses/0001_dataset_creation.md):

```{python}
class Plate_Image_Dataset(torch.utils.data.Dataset):
    def __init__(self, annotations_file: str, img_dir: str, transforms=None):
        self.img_labels = pd.read_csv(annotations_file)
        self.img_files = [f for f in sorted(glob(img_dir + "/*.png"))]
        self.transforms = transforms

    def __len__(self) -> int:
        return len(self.img_files)
    
    def load_image(self, idx): # adding this as normalisation will mess up plotting with matplotlib
        print(self.img_files)
        img = read_image(self.img_files[idx])
        return img

    def __getitem__(self, idx: int) -> tuple[torch.Tensor, Dict]:
        # Load image 
        img = read_image(self.img_files[idx])
        # Convert image to TVTensor
        img = tv_tensors.Image(img)
        
        # Get coordinates
        x1 = [int(item) for item in re.findall(r'\d+', str(self.img_labels['xmins'][idx]))]
        y1 = [int(item) for item in re.findall(r'\d+', str(self.img_labels['ymins'][idx]))]
        x2 = [int(item) for item in re.findall(r'\d+', str(self.img_labels['xmaxs'][idx]))]
        y2 = [int(item) for item in re.findall(r'\d+', str(self.img_labels['ymaxs'][idx]))]
        
        # Create boxes tensor
        num_objs = len(x1)
        boxes = torch.tensor(
            [[x1[i], y1[i], x2[i], y2[i]] for i in range(num_objs)],
            dtype=torch.float32
        )
        
        # Create target dictionary
        target = {
            "boxes": tv_tensors.BoundingBoxes(
                boxes,
                format="XYXY",
                canvas_size=F.get_size(img)
            ),
            "labels": torch.ones((num_objs,), dtype=torch.int64),
            "image_id": idx,
            "area": (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),
            "iscrowd": torch.zeros((num_objs,), dtype=torch.int64)
        }

        # Apply transforms only to img
        if self.transforms is not None:
            img, target = self.transforms(img, target)
        
        return img, target

```

Plotting again:
```{python}
# Setup data
model, optimizer, transforms = get_model_instance_object_detection(2)
dataset = Plate_Image_Dataset(
    annotations_file=str(annotations_file),
    img_dir=str(img_dir),
    transforms=transforms  # Using the new transforms 
)
print(dataset[7])
plot_sample(dataset, 7)
```

This is now working! The images and associated bounding boxes are correctly ordered.

Now trying to run the training script again with all these changes integrated together:
```{python}
import caffeine 

# Setup data
dataset = Plate_Image_Dataset(
    annotations_file=str(annotations_file),
    img_dir=str(img_dir),
    transforms=transforms  # Using the new transforms 
)

# Split dataset with correct boxes now
dataset_size = len(dataset)
validation_size = 50  # Or whatever size you want
indices = [int(i) for i in torch.randperm(dataset_size).tolist()]

dataset_validation = torch.utils.data.Subset(dataset, indices[-validation_size:])
dataset_train = torch.utils.data.Subset(dataset, indices[:-validation_size])

# Create data loaders with custom_collate_fn
train_loader = torch.utils.data.DataLoader(
    dataset_train,
    batch_size=1,
    shuffle=True,
    collate_fn=custom_collate_fn  # Using new collate function
)

validation_loader = torch.utils.data.DataLoader(
    dataset_validation,
    batch_size=1,
    shuffle=False,
    collate_fn=custom_collate_fn  # Using new collate function
)

# Rest of your training setup
model, optimizer, preprocess = get_model_instance_object_detection(num_class=2)
model.to(device)

# Training loop
num_epochs = 15
precedent_epoch = 0
save_dir = '../'

epoch = train(model, train_loader, validation_loader, device, num_epochs, precedent_epoch, save_dir, optimizer)
```
These results will be stored in results/analysis_0002_15epochs.

```{python}
save_dir = '/Users/cla24mas/Documents/SC_TSL_15092024_plate_detect/checkpoints/'
plot_prediction(model, dataset, device, 3, save_dir, 'checkpoint_epoch_9')
```

This produced a 'tuple object is not callable error'.

```{python}
def plot_prediction(model, dataset, device, index, save_dir: str, model_name):
    # Get image and target from dataset
    img, target = dataset[index]
    
    with torch.no_grad():
        image = img[:3, ...].to(device) # take the first 3 elements/channels (RGB) - leave the rest as the same (the '...')
        predictions = model([image])
        pred = predictions[0]
    
    # Move image back to CPU for visualization
    image = image.cpu()
    
    # Normalize each channel independently
    normalized_image = torch.zeros_like(image)
    for c in range(3):
        channel = image[c]
        min_val = channel.min()
        max_val = channel.max()
        if max_val > min_val:
            normalized_image[c] = (channel - min_val) / (max_val - min_val)
    
    # Convert to 8-bit format for visualization
    image_uint8 = (normalized_image * 255).byte()
    
    # Get predicted boxes
    pred_boxes = pred["boxes"].cpu().long()
    
    # Draw boxes on image
    output_image = torchvision.utils.draw_bounding_boxes(
        image_uint8,
        pred_boxes,
        colors="red",
        width=3
    )
    
    # Convert tensor to numpy and ensure proper format for matplotlib
    output_image = output_image.permute(1, 2, 0).numpy()
    
    # Create figure and display
    plt.figure(figsize=(15, 10))
    plt.imshow(output_image)
    plt.axis('off')
    
    # Save the figure
    save_path = f'{save_dir}/prediction_normalized_{index}.png'
    plt.savefig(save_path, bbox_inches='tight', pad_inches=0, dpi=300)
    print(f"Saved normalized prediction to: {save_path}")
    
    plt.show()
    plt.close()

    return output_image
```

```{python}
save_dir = '/Users/cla24mas/Documents/SC_TSL_15092024_plate_detect/results/0002_predictions/'
load_dir = '/Users/cla24mas/Documents/SC_TSL_15092024_plate_detect/checkpoints/'
model, _, _ = load_model(load_dir, 2, 'checkpoint_epoch_9')
model.eval()  # Set model to evaluation mode
for i in range(0, validation_size):
    plot_prediction(model, dataset_validation, device, i, save_dir, 'checkpoint_epoch_9')
```

Evaluation Metrics:

![evaluation metrics](../results/0002_15_epochs/02_fix/evaluation_metrics_epochs_0-15.png)

Example prediction:

![example prediction](../results/0002_15_epochs/02_fix/holdout_predictions/prediction_normalized_24.png)

To validate these results, see [analysis 0003](0003_positive_control.md)



