{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Implementing a Functional Faster R-CNN Model\"\n",
        "date: 09-10-2024\n",
        "date-format: short\n",
        "execute:\n",
        "    error: true\n",
        "---\n",
        "\n",
        "\n",
        "## Introduction: \\\n",
        "There are numerous object detection model architectures available. \\\n",
        "Typically, they comprise: a convolutional module used for feature extraction; some mechanism for identifying Regions of Interest (ROIs) (for example, by [selective search](https://link.springer.com/article/10.1007/s11263-013-0620-5), or by a [Region Proposal Network] (RPN) that uses 'anchors' and is integrated within the model); and classification/masking/bounding box regression heads. \\\n",
        "Chosen somewhat arbitrarily, I will first code and train a Faster R-CNN architecture. There are others that might be worth exploring (e.g., You Only Look Once (YOLO), Single-Shot Detector(SSD)), but I think implementing the former is more realistic given my inexperience with deep machine learning frameworks prior to this project. \\\n",
        "\n",
        "### Goal: \\\n",
        "Develop a functional initial object detection pipeline using a Faster R-CNN model trained on the dataset created in [analysis 0001](0001_dataset_creation.md) to facilitate the [imaging system's](https://github.com/SamuelClucas/SC_TSL_06082024_imaging_system_design) recognition of and navigation to growth plates inside the incubator. \\\n",
        "\n",
        "### Hypothesis: \\\n",
        "Can a Faster R-CNN model trained on [this](../raw/) dataset identify and label plates with bounding boxes in a validation subset with a mean Average Precision (mAP) of greater than 50%, based on an Intersection over Union (IoU) metric. \\\n",
        "\n",
        "### Rationale: \\\n",
        "Real-time object detection using the Faster R-CNN architecture has been robustly established in the literature ([Shaoqing Ren et al., 2016](https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf),[Yu Liu, 2018](https://ieeexplore.ieee.org/abstract/document/8695451), [Wenze Li, 2021](https://iopscience.iop.org/article/10.1088/1742-6596/1827/1/012085/meta)). \\\n",
        "\n",
        "Based on reading of the [Universal Approximation Theorem](https://mitliagkas.github.io/ift6085-2020/ift-6085-lecture-10-notes.pdf), I think it is reasonable to posit that given sufficient, representative, and clear training data, the network should be abe to detect plates in images to at least some degree of accuracy. The question is whether or not this accuracy is high enough to be practically useful. \\\n",
        "\n",
        "### Experimental Plan: \\\n",
        "- I will use PyTorch. Firstly, I need to write two custom dataset classes to handle storing and accessing images and their corresponding bounding box vertices for both the positives and negatives datasets (the latter to be used in [hard negatives mining](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590120.pdf) to further optimise the model). \\\n",
        "- Once instantiated, this class is passed to PyTorch's DataLoader for training or evaluation, as outlined in this [pytorch tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html). I would like to write a helper_training_functions src file to be imported in training scripts to keep the latter concise and readable. \\\n",
        "- Training on the cluster requires that scripts don't need to download files at runtime. This will require a local Faster R-CNN class definition file. \\\n",
        "- [ResNet50](https://blog.roboflow.com/what-is-resnet-50/#:~:text=One%20such%20architecture%20is%20called,it%2C%20and%20categorize%20them%20accordingly.)'s COCO or ImageNet pre-trained weights aren't going to be useful. This is common sense. 'Non-lambertian' or transparent objects form a minority within these datasets, which isn't helpful in training a network to recognise an object that is transparent. For this reason, I will train a ResNet backbone (I'm not sure many layers this will have, perhaps 50-layer or 101-layer) on a transparent object dataset. Google's [cleargrasp](https://github.com/Shreeyak/cleargrasp/tree/master) project required the creation of [transparent object datasets](https://sites.google.com/view/transparent-objects). I will train the network on these first, with the hopes that the backbone will then be primed to extract features typical of transparent objects. \\\n",
        "- I will then freeze the backbone weights prior to training of the RPN layer, and the classification and bounding box regression heads. This model will undergo evaluation to answer the hypothesis. \\\n",
        "\n",
        "## Creating a Dataset Class: Plate_Image_Dataset() \\\n",
        "*Note:* \\\n",
        "I used [this](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html) Pytorch tutorial as a launchpoint. As per the tutorial, I downloaded several helper functions available on Pytorch vision's github. \\ \n",
        "\n",
        "Why not just import them into scripts through the torchvision library? \\\n",
        "- See fmassa's comment [here](https://github.com/pytorch/vision/issues/2254). \n",
        "\n",
        "If you install the package as described in the project [README](../README.md), they have already been installed under [src/torchvision_deps/](../src/torchvision_deps/) and can be imported into scripts. \\\n",
        "\n",
        "For reference, I have included wget commands for each below:\n",
        "\n",
        "```{bash}\n",
        "#| eval: false\n",
        "wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py \n",
        "wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\n",
        "wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\n",
        "wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\n",
        "wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\n",
        "```\n",
        "\n",
        "### Class Overview:\n",
        "\n",
        "#### Imports..."
      ],
      "id": "393bdfc3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "from torchvision_deps import engine\n",
        "import re\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "import torch\n",
        "from torchvision.transforms.v2 import functional as F\n",
        "from torchvision import tv_tensors\n",
        "from collections.abc import Sequence # for type hints like 'tuple[]': https://docs.python.org/3/library/typing.html"
      ],
      "id": "ab848be0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Defining the constructor..."
      ],
      "id": "d4e1cce0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Plate_Image_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, annotations_file: str, img_dir: str, transforms=None):\n",
        "        self.img_labels: pd.Dataframe = pd.read_csv(annotations_file) # bounding box vertices' coordinates\n",
        "        self.boxes: np.array[int] = None\n",
        "\n",
        "        self.img_files: list[str] = [f for f in glob(img_dir + \"/*.png\")]\n",
        "        self.transforms = transforms"
      ],
      "id": "50eec941",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Breakdown:** \\\n",
        "- 'Plate_Image_Dataset' is a custom dataset that represents a map from keys/indices to data samples, hence inherits from PyTorch's provided abstract class '[torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)'. \\\n",
        "- Initialise 'self.img_labels' as as a [pandas DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), reading from the csv file containing bounding box vertex coordinates passed to the constructor as a string in 'annotations_file'. \\\n",
        "- Declare 'self.boxes' as a [numpy array](https://numpy.org/doc/stable/reference/generated/numpy.array.html) of integers, later to be used to store all bounding boxes associated with an image (using an index) in __getitem__. \\\n",
        "- Initialise 'self.img_files' as a list of strings, using [glob](https://docs.python.org/3/library/glob.html) to find all images (pathnames ending in .png) at the image dataset directory passed to the constructor as a string in 'img_dir'. The notation [f for f in...] is a concise way to create lists in python (see [list comprehension](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions)). \\\n",
        "- Initialise 'self.transforms' as transforms passed the constructor. The default is None. See PyTorch's transforms documentation [here](https://pytorch.org/vision/master/transforms.html#transforms). \\\n",
        "\n",
        "#### Defining __len__..."
      ],
      "id": "8c1c0d4b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Plate_Image_Dataset(Plate_Image_Dataset):\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.img_files)"
      ],
      "id": "9dc395ef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Just returns the number of image files in the dataset as an integer. \\\n",
        "\n",
        "#### Defining __getitem__..."
      ],
      "id": "3238db31"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Plate_Image_Dataset(Plate_Image_Dataset):\n",
        "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, tv_tensors.Image]: \n",
        "        # loads images and bounding boxes\n",
        "        img: torch.Tensor = read_image(self.img_files[idx]) # uint8\n",
        "        \n",
        "        # get digits from self.img_labels eg. ymaxs = '[ymax1, ymax2, ymax3]' returned from pd.DataFrame\n",
        "        x1 = [int(item) for item in re.findall(r'\\d+', str(self.img_labels['xmins'][idx]))]\n",
        "        y1 = [int(item) for item in re.findall(r'\\d+', str(self.img_labels['ymins'][idx]))]\n",
        "        x2 = [int(item) for item in re.findall(r'\\d+', str(self.img_labels['xmaxs'][idx]))]\n",
        "        y2 = [int(item) for item in re.findall(r'\\d+', str(self.img_labels['ymaxs'][idx]))]\n",
        "\n",
        "        num_objs = len(x1)\n",
        "        # this class is for the positives dataset, and so every image has at least one labelled bounding box, hence should be tensor of ones shape [N]\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64) \n",
        "\n",
        "        # boxes expected shape [N, 4] Tensor x1, y1, x2, y2 where 0 <= x1 < x2 same for y1 and y2 [row = boxes, columns: x1y1x2y2]\n",
        "        self.boxes = [[x1[i], y1[i], x2[i], y2[i]] for i in range(num_objs)]\n",
        "       \n",
        "        # one BoundingBoxes instance per sample, \"{img\": img, \"bbox\": BoundingBoxes(...)}\" where BoundingBoxes contains all the bounding box vertices associated with that image in the form x1, y1,x2, y2\n",
        "        self.boxes = torch.from_numpy(np.array(self.boxes)) # [N, 4] tensor\n",
        "        \n",
        "        image_id = idx\n",
        "\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        area = (self.boxes[:, 3] - self.boxes[:, 1]) * (self.boxes[:, 2] - self.boxes[:, 0]) \n",
        "\n",
        "        # tv.tensors is tensor of images with associated metadata\n",
        "        img = tv_tensors.Image(img)\n",
        "        target = {}\n",
        "        target[\"boxes\"] = tv_tensors.BoundingBoxes(self.boxes, format=\"XYXY\", canvas_size=F.get_size(img))\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "        \n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "            target[\"boxes\"] = self.transforms(target[\"boxes\"])\n",
        "        \n",
        "        return img, target"
      ],
      "id": "bd43ca7d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Breakdown:** \\\n",
        "- I'm not particularly familiar with indexing pandas DataFrames. I used python's [regular expression operations](https://docs.python.org/3/library/re.html) to extract digits ('\\\\d') from the dataframe at a given index. Using isdigit() doesn't work (for example, iterating through '200' with isdigit() would return '2', '0', '0'). I struggled to use pandas 'loc' indexing attribute. I may return to this to do so. \\\n",
        "- __getitem__ must return a tuple as specified in the tutorial. Here, img is of subclass of [tv.Tensor](https://pytorch.org/vision/master/tv_tensors.html) ([an image stored as a 3D matrix/tensor](https://discuss.pytorch.org/t/what-is-image-really/151290)). This tensor should be of shape [3, H, W], where 3 is the number of channels. Target is a dict with the following fields: \\\n",
        "    - \"boxes\": another [tv.Tensor](https://pytorch.org/vision/master/tv_tensors.html) subclass with shape [N, 4] where N is the number of bounding boxes associated with the image at the index being 'got'. Columns are x1, y1, x2, y2, where 0 <= x1 < x2 (same for y1 and y2). This is specified by 'format'. \\\n",
        "    - \"labels\": class labels (here 0 for background, 1 for 'plate') of type int64. \\\n",
        "    - \"image_id\": image index in dataset of type int64. \\\n",
        "    - \"area\": float torch.Tensor of shape [N]. The area of the bounding box. This is used by coco_eval in [src/torchvision_deps](../src/torchvision_deps/) to separate metric scores for small, medium and large boxes. \\\n",
        "    - \"iscrowd\": int64 torch.Tensor of shape [N]. Instances with iscrowd=True are ignored during evaluation. This doesn't really serve a purpose other than to prevent errors popping up when using the [torchvision_deps](../src/torchvision_deps/). I plan to either extricate it from the dependencies so that it can be removed from the dataset class. For now, it is always initialised as a tensor of zeros using [torch.zeros](https://pytorch.org/docs/stable/generated/torch.zeros.html). \\\n",
        "- wrapping image tensors and bounding boxes in TVTensor subclasses allows for application of torchvision [built-in transformations](https://pytorch.org/vision/stable/transforms.html). Based on the TVTensor subclass wrapping the object, the tranforms dispatch the object to the appropriate implementation (as described [here](https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_getting_started.html#what-are-tvtensors)). \\\n",
        "- in the tutorial, img and target are passed to trainsforms together and returned as a tuple. For some reason, I could not get it to work this way. I suspect it is something to do with the types of the fields of target not all being TVTensors. Regardless, I pass boxes and img to transforms separately, then return them from __getitem__ as a tuple. Hopefully this isn't behaving unexpectedly in the torchvision deps - unit tests necessary to confirm this. \\\n",
        " \n",
        "## Creating a 'helper_training_functions' Module:\n",
        "\n",
        "I created numerous helper functions for import into training/evaluation scripts. They are defined as a module inside [plate_detect](../src/plate_detect/helper_training_functions.py). The most recent version of 'helper_training_functions' can be imported into scripts like so:  \\"
      ],
      "id": "f28f38b3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execute": false
      },
      "source": [
        "from plate_detect import helper_training_functions"
      ],
      "id": "16078fb1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Within this analysis, however, I define each function locally as I iteratively rewrite them during debugging. \\\n",
        "### Programme Overview:\n",
        "\n",
        "#### Imports:"
      ],
      "id": "87513853"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from torchvision_deps.engine import evaluate # in scripts, you should import train_one_epoch also here \n",
        "import re\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "import torch\n",
        "from torchvision.transforms.v2 import functional as F\n",
        "from torchvision import tv_tensors\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from typing import Dict, Union\n",
        "from torchvision.models.detection.faster_rcnn import fasterrcnn_resnet50_fpn_v2, FastRCNNPredictor, FasterRCNN_ResNet50_FPN_V2_Weights\n",
        "import torchvision_deps.T_and_utils.utils as utils \n",
        "import PIL\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "bbadf4b4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Get Faster R-CNN model instance for object detection..."
      ],
      "id": "4a78915b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_model_instance_object_detection(num_class: int) -> fasterrcnn_resnet50_fpn_v2:\n",
        "    # New weights with accuracy 80.858%\n",
        "    weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT # alias is .DEFAULT suffix, weights = None is random initialisation, box MAP 46.7, params, 43.7M, GFLOPS 280.37 https://github.com/pytorch/vision/pull/5763\n",
        "    model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.0001)\n",
        "    preprocess = weights.transforms()\n",
        "    # finetuning pretrained model by transfer learning\n",
        "    # get num of input features for classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_class)\n",
        "    return model, preprocess"
      ],
      "id": "1822a80a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Breakdown:** \\\n",
        "- this approach requires permissions to download the model file from the internet (through 'torch.utils.model_zoo.load_url()'). For this reason, it is not amenable to training on the cluster. This will be addressed in analysis 0003, using a local Faster R-CNN with ResNet backbone class definition. Otherwise, this function still works. \\\n",
        "- See pytorch's documentation on [ResNet50 faster R-CNN backbone](https://pytorch.org/vision/stable/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html).\n",
        "\n",
        "#### Save epoch, model and optimizer state_dict in checkpoint dict:"
      ],
      "id": "e253c1e3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def save_checkpoint(model, optimizer, epoch, root_dir):\n",
        "    save_path = os.path.join(root_dir, 'checkpoints')\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "    \n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }\n",
        "    \n",
        "    filename = f'checkpoint_epoch_{epoch}.pth'\n",
        "    save_path = os.path.join(save_path, filename)\n",
        "    torch.save(checkpoint, save_path)\n",
        "    print(f\"Checkpoint saved: {save_path}\")"
      ],
      "id": "dbefbb8b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Breakdown:** \\\n",
        "- Creates a directory if it doesn't exist to store the checkpoints. \\\n",
        "- Saves the current epoch number, model parameters (weights), and optimizer state to a dictionary. \\\n",
        "- This allows for resuming training from a specific epoch in case of interruptions. \\\n",
        "\n",
        "#### Training function:\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "*Note:* This function uses train_one_epoch() from [engine.py](../src/torchvision_deps/engine.py) in torchvision_deps. Engine.py uses the MetricLogger and SmoothedValue classes defined in [utils.py](../src/torchvision_deps/T_and_utils/utils.py) I may later modify these, and so I define them locally prior to these modifications here preserve their development through time."
      ],
      "id": "97c176d8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import datetime\n",
        "import errno\n",
        "import os\n",
        "import time\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "\n",
        "\n",
        "class SmoothedValue:\n",
        "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
        "    window or the global series average.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, window_size=20, fmt=None):\n",
        "        if fmt is None:\n",
        "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
        "        self.deque = deque(maxlen=window_size)\n",
        "        self.total = 0.0\n",
        "        self.count = 0\n",
        "        self.fmt = fmt\n",
        "\n",
        "    def update(self, value, n=1):\n",
        "        self.deque.append(value)\n",
        "        self.count += n\n",
        "        self.total += value * n\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        \"\"\"\n",
        "        Warning: does not synchronize the deque!\n",
        "        \"\"\"\n",
        "        if not is_dist_avail_and_initialized():\n",
        "            return\n",
        "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device=\"cuda\")\n",
        "        dist.barrier()\n",
        "        dist.all_reduce(t)\n",
        "        t = t.tolist()\n",
        "        self.count = int(t[0])\n",
        "        self.total = t[1]\n",
        "\n",
        "    @property\n",
        "    def median(self):\n",
        "        d = torch.tensor(list(self.deque))\n",
        "        return d.median().item()\n",
        "\n",
        "    @property\n",
        "    def avg(self):\n",
        "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
        "        return d.mean().item()\n",
        "\n",
        "    @property\n",
        "    def global_avg(self):\n",
        "        return self.total / self.count\n",
        "\n",
        "    @property\n",
        "    def max(self):\n",
        "        return max(self.deque)\n",
        "\n",
        "    @property\n",
        "    def value(self):\n",
        "        return self.deque[-1]\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.fmt.format(\n",
        "            median=self.median, avg=self.avg, global_avg=self.global_avg, max=self.max, value=self.value\n",
        "        )\n",
        "\n",
        "\n",
        "def all_gather(data):\n",
        "    \"\"\"\n",
        "    Run all_gather on arbitrary picklable data (not necessarily tensors)\n",
        "    Args:\n",
        "        data: any picklable object\n",
        "    Returns:\n",
        "        list[data]: list of data gathered from each rank\n",
        "    \"\"\"\n",
        "    world_size = get_world_size()\n",
        "    if world_size == 1:\n",
        "        return [data]\n",
        "    data_list = [None] * world_size\n",
        "    dist.all_gather_object(data_list, data)\n",
        "    return data_list\n",
        "\n",
        "\n",
        "def reduce_dict(input_dict, average=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        input_dict (dict): all the values will be reduced\n",
        "        average (bool): whether to do average or sum\n",
        "    Reduce the values in the dictionary from all processes so that all processes\n",
        "    have the averaged results. Returns a dict with the same fields as\n",
        "    input_dict, after reduction.\n",
        "    \"\"\"\n",
        "    world_size = get_world_size()\n",
        "    if world_size < 2:\n",
        "        return input_dict\n",
        "    with torch.inference_mode():\n",
        "        names = []\n",
        "        values = []\n",
        "        # sort the keys so that they are consistent across processes\n",
        "        for k in sorted(input_dict.keys()):\n",
        "            names.append(k)\n",
        "            values.append(input_dict[k])\n",
        "        values = torch.stack(values, dim=0)\n",
        "        dist.all_reduce(values)\n",
        "        if average:\n",
        "            values /= world_size\n",
        "        reduced_dict = {k: v for k, v in zip(names, values)}\n",
        "    return reduced_dict\n",
        "\n",
        "class MetricLogger:\n",
        "    def __init__(self, delimiter=\"\\t\"):\n",
        "        self.meters = defaultdict(SmoothedValue)\n",
        "        self.delimiter = delimiter\n",
        "\n",
        "    def update(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                v = v.item()\n",
        "            assert isinstance(v, (float, int))\n",
        "            self.meters[k].update(v)\n",
        "\n",
        "    def __getattr__(self, attr):\n",
        "        if attr in self.meters:\n",
        "            return self.meters[attr]\n",
        "        if attr in self.__dict__:\n",
        "            return self.__dict__[attr]\n",
        "        raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{attr}'\")\n",
        "\n",
        "    def __str__(self):\n",
        "        loss_str = []\n",
        "        for name, meter in self.meters.items():\n",
        "            loss_str.append(f\"{name}: {str(meter)}\")\n",
        "        return self.delimiter.join(loss_str)\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        for meter in self.meters.values():\n",
        "            meter.synchronize_between_processes()\n",
        "\n",
        "    def add_meter(self, name, meter):\n",
        "        self.meters[name] = meter\n",
        "\n",
        "    def log_every(self, iterable, print_freq, header=None):\n",
        "        i = 0\n",
        "        if not header:\n",
        "            header = \"\"\n",
        "        start_time = time.time()\n",
        "        end = time.time()\n",
        "        iter_time = SmoothedValue(fmt=\"{avg:.4f}\")\n",
        "        data_time = SmoothedValue(fmt=\"{avg:.4f}\")\n",
        "        space_fmt = \":\" + str(len(str(len(iterable)))) + \"d\"\n",
        "        if torch.cuda.is_available():\n",
        "            log_msg = self.delimiter.join(\n",
        "                [\n",
        "                    header,\n",
        "                    \"[{0\" + space_fmt + \"}/{1}]\",\n",
        "                    \"eta: {eta}\",\n",
        "                    \"{meters}\",\n",
        "                    \"time: {time}\",\n",
        "                    \"data: {data}\",\n",
        "                    \"max mem: {memory:.0f}\",\n",
        "                ]\n",
        "            )\n",
        "        else:\n",
        "            log_msg = self.delimiter.join(\n",
        "                [header, \"[{0\" + space_fmt + \"}/{1}]\", \"eta: {eta}\", \"{meters}\", \"time: {time}\", \"data: {data}\"]\n",
        "            )\n",
        "        MB = 1024.0 * 1024.0\n",
        "        for obj in iterable:\n",
        "            data_time.update(time.time() - end)\n",
        "            yield obj\n",
        "            iter_time.update(time.time() - end)\n",
        "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
        "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
        "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
        "                if torch.cuda.is_available():\n",
        "                    print(\n",
        "                        log_msg.format(\n",
        "                            i,\n",
        "                            len(iterable),\n",
        "                            eta=eta_string,\n",
        "                            meters=str(self),\n",
        "                            time=str(iter_time),\n",
        "                            data=str(data_time),\n",
        "                            memory=torch.cuda.max_memory_allocated() / MB,\n",
        "                        )\n",
        "                    )\n",
        "                else:\n",
        "                    print(\n",
        "                        log_msg.format(\n",
        "                            i, len(iterable), eta=eta_string, meters=str(self), time=str(iter_time), data=str(data_time)\n",
        "                        )\n",
        "                    )\n",
        "            i += 1\n",
        "            end = time.time()\n",
        "        total_time = time.time() - start_time\n",
        "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "        print(f\"{header} Total time: {total_time_str} ({total_time / len(iterable):.4f} s / it)\")"
      ],
      "id": "137a5063",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import math\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torchvision.models.detection.mask_rcnn\n",
        "from torchvision_deps.T_and_utils import utils\n",
        "from torchvision_deps.coco_eval import CocoEvaluator\n",
        "from torchvision_deps.coco_utils import get_coco_api_from_dataset\n",
        "\n",
        "\n",
        "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq, scaler=None):\n",
        "    model.train()\n",
        "    metric_logger = MetricLogger(delimiter=\"  \") # normally this would be utils.MetricLogger, but I'm using the MetricLogger defined in this quarto document\n",
        "    metric_logger.add_meter(\"lr\", utils.SmoothedValue(window_size=1, fmt=\"{value:.6f}\"))\n",
        "    header = f\"Epoch: [{epoch}]\"\n",
        "\n",
        "    lr_scheduler = None\n",
        "    if epoch == 0:\n",
        "        warmup_factor = 1.0 / 1000\n",
        "        warmup_iters = min(1000, len(data_loader) - 1)\n",
        "\n",
        "        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
        "            optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n",
        "        )\n",
        "\n",
        "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
        "        with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # reduce losses over all GPUs for logging purposes\n",
        "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
        "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
        "\n",
        "        loss_value = losses_reduced.item()\n",
        "\n",
        "        if not math.isfinite(loss_value):\n",
        "            print(f\"Loss is {loss_value}, stopping training\")\n",
        "            print(loss_dict_reduced)\n",
        "            sys.exit(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        if scaler is not None:\n",
        "            scaler.scale(losses).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if lr_scheduler is not None:\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
        "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "    return metric_logger\n",
        "\n",
        "\n",
        "def _get_iou_types(model):\n",
        "    model_without_ddp = model\n",
        "    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
        "        model_without_ddp = model.module\n",
        "    iou_types = [\"bbox\"]\n",
        "    if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN):\n",
        "        iou_types.append(\"segm\")\n",
        "    if isinstance(model_without_ddp, torchvision.models.detection.KeypointRCNN):\n",
        "        iou_types.append(\"keypoints\")\n",
        "    return iou_types\n",
        "\n",
        "\n",
        "@torch.inference_mode()\n",
        "def evaluate(model, data_loader, device):\n",
        "    n_threads = torch.get_num_threads()\n",
        "    # FIXME remove this and make paste_masks_in_image run on the GPU\n",
        "    torch.set_num_threads(1)\n",
        "    cpu_device = torch.device(\"cpu\")\n",
        "    model.eval()\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    header = \"Test:\"\n",
        "\n",
        "    coco = get_coco_api_from_dataset(data_loader.dataset)\n",
        "    iou_types = _get_iou_types(model)\n",
        "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
        "\n",
        "    for images, targets in metric_logger.log_every(data_loader, 100, header):\n",
        "        images = list(img.to(device) for img in images)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "        model_time = time.time()\n",
        "        outputs = model(images)\n",
        "\n",
        "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
        "        model_time = time.time() - model_time\n",
        "\n",
        "        res = {target[\"image_id\"]: output for target, output in zip(targets, outputs)}\n",
        "        evaluator_time = time.time()\n",
        "        coco_evaluator.update(res)\n",
        "        evaluator_time = time.time() - evaluator_time\n",
        "        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
        "\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger)\n",
        "    coco_evaluator.synchronize_between_processes()\n",
        "\n",
        "    # accumulate predictions from all images\n",
        "    coco_evaluator.accumulate()\n",
        "    coco_evaluator.summarize()\n",
        "    torch.set_num_threads(n_threads)\n",
        "    return coco_evaluator"
      ],
      "id": "5f82b325",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::"
      ],
      "id": "f6026a77"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def train(model, data_loader, data_loader_test, device, num_epochs, precedent_epoch, save_dir): \n",
        "    model.train()\n",
        "\n",
        "    # Initialize lists to store metrics\n",
        "    train_losses = []\n",
        "\n",
        "     # construct an optimizer\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.SGD(\n",
        "        params,\n",
        "        lr=0.005,\n",
        "        momentum=0.9,\n",
        "        weight_decay=0.0005\n",
        "    )\n",
        "    # and a learning rate scheduler\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "        optimizer,\n",
        "        step_size=3,\n",
        "        gamma=0.1\n",
        "    )\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "        metric_logger = train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "    \n",
        "    # Get the average loss for this epoch\n",
        "        epoch_loss = metric_logger.meters['loss'].global_avg\n",
        "        train_losses.append(epoch_loss)\n",
        "\n",
        "        # Save checkpoint\n",
        "        save_checkpoint(model, optimizer, epoch + precedent_epoch, save_dir)\n",
        "        \n",
        "        # Update the learning rate\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    return num_epochs + precedent_epoch, train_losses"
      ],
      "id": "d22fb3e5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Breakdown:** \\\n",
        "Setting the Model to Training Mode: \\\n",
        "- model.train() sets the model to training mode. This is crucial because certain layers (like dropout and batch normalization) behave differently during training versus evaluation. \\\n",
        "\n",
        "Optimizer Construction: \\\n",
        "- The optimizer is created using Stochastic Gradient Descent (SGD) with a learning rate of 0.005, momentum of 0.9, and weight decay for regularization (also known as [L2 regularisation](https://arxiv.org/pdf/2310.04415)). \\\n",
        "- params filters the model's parameters to include only those that require gradients (trainable parameters). \\\n",
        "\n",
        "Learning Rate Scheduler: \\\n",
        "- A learning rate scheduler (StepLR) is initialized, which reduces the learning rate by a factor of gamma (0.1) every step_size epochs (3 epochs in this case). This helps improve convergence as training progresses. \\\n",
        "\n",
        "Epoch Loop: \\\n",
        "- The outer loop iterates over the number of specified epochs (num_epochs). \\\n",
        "\n",
        "Training for One Epoch: \\\n",
        "- train_one_epoch(...) is a function from [engine.py](../src/torchvision_deps/engine.py) that handles the training logic for one epoch. It processes batches from the data_loader, computes losses, and updates model weights. \\\n",
        "- The print_freq parameter controls how often training progress is printed (every 10 iterations). \\\n",
        "\n",
        "Logging Loss: \\\n",
        "- The average loss for the epoch is extracted from metric_logger, which tracks various metrics during training. \\\n",
        "\n",
        "Checkpoint Saving: \\\n",
        "- After each epoch, a checkpoint is saved using the save_checkpoint function. This allows for resuming from the last epoch in case of interruption. \\\n",
        "\n",
        "Metrics Plotting: \\\n",
        "- The function plot_eval_metrics is called to visualize training losses over epochs. \\\n",
        "\n",
        "Learning Rate Update: \\\n",
        "- After the epoch, the learning rate is updated according to the scheduler. \\\n",
        "\n",
        "Return Values: \\\n",
        "- The function returns the final epoch number (adjusted for any precedent epochs) and the list of training losses. \\\n",
        "\n",
        "#### Load a model from a .pth file:"
      ],
      "id": "7493d281"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def load_model(save_dir: str, num_classes: int, model_file_name: str):\n",
        "    model, preprocess = get_model_instance_object_detection(num_classes)\n",
        "    checkpoint = torch.load(save_dir + f'{model_file_name}.pth', weights_only=True)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer = model.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    return model, optimizer, epoch"
      ],
      "id": "8e137072",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Breakdown:** \\\n",
        "- Loads a model from a .pth file. See the pytorch documentation [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html). \\\n",
        "- The weights and biases of a model are accessed by model.parameters(). A state_dict is a dictionary object that maps each layer to its parameter tensor. Both the model and optimiser have state_dicts, and both are saved by the save_checkpoint helper function. \\\n",
        "- Returns the model, optimizer and epoch. \\\n",
        "\n",
        "#### Evaluate a model's performance after training:"
      ],
      "id": "bec6fd60"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def evaluate_model(model, data_loader_test: torch.utils.data.DataLoader, device: torch.cuda.device): \n",
        "    val_metrics = []\n",
        "    model.eval()\n",
        "\n",
        "    coco_evaluator = evaluate(model, data_loader_test, device=device)\n",
        "        \n",
        "    # Extract evaluation metrics\n",
        "    eval_metrics = coco_evaluator.coco_eval['bbox'].stats\n",
        "\n",
        "    return eval_metrics"
      ],
      "id": "a9f8c8c6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Breakdown:** \\\n",
        "- Sets the model to evaluation mode using model.eval(), which alters the behavior of certain layers. \\\n",
        "    - In training mode, the model expects input tensors (images) and targets (list of dictionary containing bounding boxes [N, 4], with vertices in the format [x1, y1, x2, y2] and class labels in the format Int64Tensor[N] where N is the number of bounding boxes in a given image, or the number of distinct classes, respectively). \\\n",
        "- Currently using [coco_eval](../src/torchvision_deps/). \\\n",
        "- See device class docs [here](https://pytorch.org/docs/stable/generated/torch.cuda.device.html) \\\n",
        "- Makes a call to [engine's](../src/torchvision_deps/engine.py) evaluator() to perform COCO-style evaluation on the test dataset, shown below: \\"
      ],
      "id": "a952096c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "@torch.inference_mode()\n",
        "def evaluate(model, data_loader, device):\n",
        "    n_threads = torch.get_num_threads()\n",
        "    # FIXME remove this and make paste_masks_in_image run on the GPU\n",
        "    torch.set_num_threads(1)\n",
        "    cpu_device = torch.device(\"cpu\")\n",
        "    model.eval()\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \") # MetricLogger: https://pytorch.org/tnt/stable/utils/generated/torchtnt.utils.loggers.MetricLogger.html\n",
        "    header = \"Test:\"\n",
        "\n",
        "    coco = get_coco_api_from_dataset(data_loader.dataset)\n",
        "    iou_types = _get_iou_types(model)\n",
        "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
        "\n",
        "    for images, targets in metric_logger.log_every(data_loader, 100, header):\n",
        "        images = list(img.to(device) for img in images)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "        model_time = time.time()\n",
        "        outputs = model(images)\n",
        "\n",
        "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
        "        model_time = time.time() - model_time\n",
        "\n",
        "        res = {target[\"image_id\"]: output for target, output in zip(targets, outputs)}\n",
        "        evaluator_time = time.time()\n",
        "        coco_evaluator.update(res)\n",
        "        evaluator_time = time.time() - evaluator_time\n",
        "        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
        "\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger)\n",
        "    coco_evaluator.synchronize_between_processes()\n",
        "\n",
        "    # accumulate predictions from all images\n",
        "    coco_evaluator.accumulate()\n",
        "    coco_evaluator.summarize()\n",
        "    torch.set_num_threads(n_threads)\n",
        "    return coco_evaluator"
      ],
      "id": "38df0848",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- COCO provides thorough documentation [here](https://cocodataset.org/#detection-eval).\n",
        "- coco_evaluator.summarize() computes Average Precision (AP), AP Across Scales, Average Recall (AR), AR Across Scales (see a dicussion of these metrics [here](https://blog.zenggyu.com/posts/en/2018-12-16-an-introduction-to-evaluation-metrics-for-object-detection/index.html)).\n",
        "\n",
        "#### Using PyTorch hooks to get backbone feature maps:"
      ],
      "id": "92994bad"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_feature_maps(model, input_image, target_layer_name: torch.nn):\n",
        "    feature_maps = {} # stores activations passed to forward_hook\n",
        "    \n",
        "    def hook_fn(module, input, output):\n",
        "        feature_maps[module] = output.detach()\n",
        "    return hook_fn # calls handle.remove() to remove the added hook\n",
        "    \n",
        "    # Register hooks for the layers to be visualised\n",
        "    for name, module in model.backbone.named_modules():\n",
        "        if isinstance(module, target_layer_name): # type examples: nn.Conv2d, nn.BatchNorm2d, nn.ReLU\n",
        "            module.register_forward_hook(hook_fn(module))\n",
        "    \n",
        "    # Forward pass\n",
        "    with torch.no_grad(): # disables loss gradient calculation: https://discuss.pytorch.org/t/with-torch-no-grad/130146\n",
        "        model([input_image])\n",
        "    \n",
        "    return feature_maps"
      ],
      "id": "40d01e51",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Breakdown:** \\\n",
        "- In order to extract intermediate activations from model layers, PyTorch provides 'hooks'. [Pytorch documentation on hooks](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_forward_hook) is quite sparse, but [this](https://web.stanford.edu/~nanbhas/blog/forward-hooks-pytorch/) blog is quite useful. \\\n",
        "- Gets the feature maps for every target layer (target_layer_name) in each named module in the model's backbone (e.g., 'Sequential', 'Bottleneck'). Target layers are documented [here](https://pytorch.org/docs/stable/nn.html#convolution-layers). \\\n",
        "\n",
        "#### Plot feature maps:"
      ],
      "id": "b8bf16cf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def visualise_feature_maps(feature_maps, num_features=64):\n",
        "    for layer, feature_map in feature_maps.items():\n",
        "        # Get the first image in the batch\n",
        "        feature_map = feature_map[0]\n",
        "        \n",
        "        # Plot up to num_features feature maps\n",
        "        num_features = min(feature_map.size(0), num_features)\n",
        "        \n",
        "        fig, axs = plt.subplots(8, 8, figsize=(20, 20))\n",
        "        fig.suptitle(f'Feature Maps for Layer: {layer}')\n",
        "        \n",
        "        for i in range(num_features):\n",
        "            ax = axs[i // 8, i % 8]\n",
        "            ax.imshow(feature_map[i].cpu(), cmap='gray')\n",
        "            ax.axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "id": "586362c8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Breakdown:** \\\n",
        "- This function takes the feature maps collected from the get_feature_maps function and visualizes them. \\\n",
        "- It plots up to num_features from each layer’s output, providing visual insights into what features the model is learning at different levels.\n",
        "\n",
        "#### Superimpose and plot bounding boxes on image:"
      ],
      "id": "308da067"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_prediction(model, dataset, device, index, save_dir: str, model_name):\n",
        "    img, target = dataset[index]\n",
        "    num_epochs = 1\n",
        "    print_freq = 10  \n",
        "\n",
        "    model = load_model(save_dir, 2, model_name)\n",
        "    with torch.no_grad():\n",
        "        image = img\n",
        "        image = image[:3, ...].to(device)\n",
        "        predictions = model([image, ])\n",
        "        pred = predictions[0]\n",
        "\n",
        "    image = image[:3, ...]\n",
        "    pred_boxes = pred[\"boxes\"].long()\n",
        "    output_image = draw_bounding_boxes(image, pred_boxes, colors=\"red\")\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    plt.imshow(output_image.permute(1, 2, 0))\n",
        "   # plt.savefig(f'{index}.png', bbox_inches='tight') # tight removes whitespace"
      ],
      "id": "60eb388a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Breakdown:** \\\n",
        "- Loads a specific image from the dataset and runs it through the model to get predictions.\\\n",
        "- Superimposes the predicted bounding boxes on the original image (as a red outlined bounding box) and visualises the result. \\\n",
        "\n",
        "## Debugging 'helper_training_functions' by Implementation in a Script:\n",
        "\n",
        "Having written the code necessary to instantiate, train, and evaluate a ResNet50 model, I need to evaluate its performance over several epochs. This will help to validate the code functions as intended, and to guide future development. \\\n",
        "\n",
        "Here, I attempt to setup, train, and evaluate a Resnet50 Faster R-CNN model for 10 epochs - using src/Plate_Image_Dataset.py and src/helper_training_functions.py - then evaluate its mean average precision and recall, to be visualised in a line graph. \\\n",
        "\n",
        "The implementation focuses on validating the core functionality of the object detection pipeline before scaling up to full training runs. \\\n",
        "\n",
        "This script uses code from the following src files: \\\n",
        "- [Plate_Image_Dataset.py](../src/plate_detect/Plate_Image_Dataset.py) - custom class that handles storing and accessing images and their corresponding bounding box vertices. \\\n",
        "- [helper_training_functions.py](../src/plate_detect/helper_training_functions.py) - a group of useful helper functions I've written. For example, 'get_model_instance_object_detection' should return a [ResNet50](https://pytorch.org/hub/pytorch_vision_resnet/) backbone ([ImageNet1K_V2 weights](https://pytorch.org/vision/0.18/models/generated/torchvision.models.resnet50.html#:~:text=By%20default%2C%20no%20pre%2Dtrained%20weights%20are%20used.)), with a Region Proposal Network, as well as classifier and bounding box regression heads. \\\n",
        "\n",
        "It may also be helpful to visualise the model architecture. There are libraries for this, one of which is '[pytorchviz](https://github.com/szagoruyko/pytorchviz)'. If useful, I will create such a script [here](docs/scripts/inspect_architecture.qmd). \\\n",
        "\n",
        "For reference, here is a useful [pytorch tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html). \\\n",
        " \n",
        "### Programme Overview:\n",
        "\n",
        "#### Imports:"
      ],
      "id": "a4f43b2a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#from plate_detect import Plate_Image_Dataset #, helper_training_functions # in scripts, this is imported. Here, to document the debug process, I am using the iterations of these functions defined locally in the previous section\n",
        "import torch\n",
        "from torchvision.transforms.v2 import functional as F\n",
        "from pathlib import Path\n",
        "from typing import Dict, Union\n",
        "from torchvision.models.detection.faster_rcnn import fasterrcnn_resnet50_fpn_v2, FastRCNNPredictor, FasterRCNN_ResNet50_FPN_V2_Weights\n",
        "from torchvision.transforms import v2 as T\n",
        "import torchvision_deps.T_and_utils.utils as utils\n",
        "from torchvision_deps.engine import evaluate # normally, import train_one_epoch from engine here\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import PIL\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "from torchvision.io import read_image\n",
        "from torch import nn"
      ],
      "id": "e6e328b1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Notes:** when running on the cluster, change any import statements for packages defined within the project directory (see [src](../src/)). Here is an example:"
      ],
      "id": "29b17fb5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "from src import Plate_Image_Dataset, helper_training_functions\n",
        "# to...\n",
        "from SC_plate_detect.plate_detect import Plate_Image_Dataset, helper_training_functions"
      ],
      "id": "8cbd0d0e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Setting up directories and instantiating model..."
      ],
      "id": "20144387"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "project_dir_root: Path= Path.cwd() # 'SC_TSL_15092024_Plate_Detect/' type PosixPath for UNIX, WindowsPath for windows...\n",
        "print(f'Project root directory: {str(project_dir_root)}')\n",
        "\n",
        "annotations_file: Path = project_dir_root.parents[0].joinpath('lib', 'labels.csv')\n",
        "print(f'Training labels csv file: {annotations_file}')\n",
        "\n",
        "img_dir: Path= project_dir_root.parents[0].joinpath('raw', 'positives')  # 'SC_TSL_15092024_Plate_Detect/train/images/positives/' on UNIX systems\n",
        "print(f'Training dataset directory: {img_dir}')\n",
        "\n",
        "num_class: int = 2 # plate or background\n",
        " \n",
        "# creates resnet50 v2 faster r cnn model with new head for class classification\n",
        "\n",
        "model, preprocess = get_model_instance_object_detection(num_class) # see next comment for how this function is called when imported \n",
        "# model, preprocess = helper_training_functions.get_model_instance_object_detection(num_class)\n",
        "\n",
        "# move model to the right device\n",
        "model.to(device)"
      ],
      "id": "1fe0cf2c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Breakdown:** \\\n",
        "- Uses 'Compute Unified Device Architecture' ([CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/)) if available, falls back to CPU. \\\n",
        "- Initializes ResNet50 Faster R-CNN with pretrained weights. \\\n",
        "- 'preprocess' stores the input data transforms that take place just before the forward pass through the network. In this case, preprocess converts Tensor image, PIL image, or NumPy ndarray types into FloatTensor and scales pixel intensities in range [0.,1.] \\\n",
        "\n",
        "#### Instantiating Plate_Image_Dataset class and creating training and validation subsets..."
      ],
      "id": "8fe723c5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset: Plate_Image_Dataset = Plate_Image_Dataset.Plate_Image_Dataset(\n",
        "        img_dir=str(img_dir), \n",
        "        annotations_file=str(annotations_file),\n",
        "        transforms=preprocess, \n",
        "        )\n",
        "\n",
        "# split the dataset in train and test set\n",
        "dataset_size = 20  #normally would be len(dataset), but for debugging purposes this is sufficient\n",
        "validation_size = 5   # Again, this would normally be higher, for example: min(50, int(dataset_size // 5))  {20% of data for testing, or 50 samples, whichever is smaller}\n",
        "indices = [int(i) for i in torch.randperm(dataset_size).tolist()]\n",
        "\n",
        "dataset_validation = torch.utils.data.Subset(dataset, indices[-validation_size:])\n",
        "dataset_train = torch.utils.data.Subset(dataset, indices[:])"
      ],
      "id": "40b880ee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Breakdown:** \\\n",
        "- Plate_Image_Dataset is a custom dataset class used to handle parsing the csv file for bounding box vertices, associating them with the correct image using a dictionary, and handling retrieval of this information. \\\n",
        "- In order to create validation and training subsets of the positive samples (i.e., the images *with* at least one plate labelled), I created a randomly arranged list of indices (where len(indices) == the number of samples in the dataset). The validation subset size is equal to 20% of the superset, or 50 samples, whichever is smallest. When I implement dataset augmentation I will scale this up. \\\n",
        "\n",
        "#### Instantiating Pytorch DataLoaders for train and validation subsets..."
      ],
      "id": "1cc98316"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset_train,\n",
        "    batch_size=1,\n",
        "    shuffle=True,\n",
        "    collate_fn=utils.collate_fn\n",
        ")\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_validation,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    collate_fn=utils.collate_fn\n",
        ")"
      ],
      "id": "2da7aa2e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Breakdown:** \\\n",
        "- See [here](https://pytorch.org/docs/stable/data.html) Pytorch's documentation for DataLoader. DataLoader fetches and collates together individual samples into batches - i.e., it acts as a sampler. It does so by squeezing on a batch dimension (typically the first) to Tensors. It also provides an iterable over the given dataset. \\\n",
        "- In this case, the Image_Plate_Dataset class is a map-style dataset, as it implements __getitem__() and __len__(), and stores samples with their associated labels and metadata at a shared index. \\\n",
        "- If shuffle == True, data is reshuffled at every epoch. \\\n",
        "\n",
        "#### Calling train and evaluate_model..."
      ],
      "id": "70af6956"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "save_dir = 'results'\n",
        "num_epochs = 1\n",
        "precedent_epoch = 0\n",
        "\n",
        "epoch, loss_metrics = train(model, data_loader, data_loader_test, device, num_epochs, precedent_epoch, save_dir)\n",
        "\n",
        "eval_metrics = evaluate_model(model, data_loader_test,device)\n",
        "\n",
        "print(\"\\n -end-\")"
      ],
      "id": "1122203e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Breakdown:** \\\n",
        "- Brief overview: \\\n",
        "    1. Training function handles: \\\n",
        "        - Optimisation using [SGD with momentum](https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d)  \\\n",
        "        - Learning rate scheduling \\\n",
        "        - Checkpoint saving \\\n",
        "        - Loss tracking \\\n",
        "    2. Evaluation produces:  \\\n",
        "        - Mean Average Precision (mAP) \\\n",
        "        - Mean Average Recall (mAR) \\\n",
        "        - Performance metrics across IoU thresholds \\\n",
        "- *Note* I have only set it to train for 1 epoch here just to show the output. I will train for longer on the cluster. Here, my focus is debugging the helper functions, not training the model. \\\n",
        "\n",
        "### Loading a model and implementing 'plot_training_loss':\n",
        "\n",
        "Training seemed to complete 'successfully' (without any overt error message), as shown by the output from the previous code block. \\\n",
        "\n",
        "Next, I want to try and load the model from the .pth file saved in the previous code block. The'load_model()' function expects the save directory string of the .pth file, the number of classes (2: background, plate), and the model file name. It returns the model, optimizer, and epoch. I then pass the model to evaluate_model(). I will also type- and shape-check the eval_metrics list returned from evaluate_model for clarity when later creating a function to plot these metrics.\\\n"
      ],
      "id": "7a88d1cb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: false\n",
        "model, optimizer, epoch = load_model('../checkpoints/', 2, 'checkpoint_epoch_0')\n",
        "\n",
        "eval_metrics = evaluate_model(model, data_loader_test, device)\n",
        "\n",
        "print(eval_metrics, '\\n', shape(eval_metrics))\n",
        "\n",
        "print(\"\\n -end-\")"
      ],
      "id": "0094cbdd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This produced an error when loading the model:\n",
        "\n",
        "```{bash}\n",
        "{\n",
        "\t\"name\": \"RuntimeError\",\n",
        "\t\"message\": \"Error(s) in loading state_dict for FasterRCNN:\n",
        "\\tMissing key(s) in state_dict: ... \\\"param_groups\\\". \",\n",
        "\t\"stack\": \"---------------------------------------------------------------------------\n",
        "RuntimeError                              Traceback (most recent call last)\n",
        "File /Users/cla24mas/Documents/My_Repos/SC_TSL_15092024_plate_detect/analyses/0002_functional_Faster_R-CNN.qmd:1\n",
        "----> 1 model, optimizer, epoch = helper_training_functions.load_model('checkpoints/', 2, 'checkpoint_epoch_0')\n",
        "      3 eval_metrics = helper_training_functions.evaluate_model(model, data_loader_test,device)\n",
        "      5 print(eval_metrics, '\\\n",
        "', shape(eval_metrics))\n",
        "\n",
        "File ~/Documents/My_Repos/SC_TSL_15092024_plate_detect/src/plate_detect/helper_training_functions.py:101, in load_model(save_dir, num_classes, model_file_name)\n",
        "     99 checkpoint = torch.load(save_dir + f'{model_file_name}.pth', weights_only=True)\n",
        "    100 model.load_state_dict(checkpoint['model_state_dict'])\n",
        "--> 101 optimizer = model.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    102 epoch = checkpoint['epoch']\n",
        "    103 return model, optimizer, epoch\n",
        "\n",
        "File ~/Documents/My_Repos/SC_TSL_15092024_plate_detect/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2153, in Module.load_state_dict(self, state_dict, strict, assign)\n",
        "   2148         error_msgs.insert(\n",
        "   2149             0, 'Missing key(s) in state_dict: {}. '.format(\n",
        "   2150                 ', '.join(f'\\\"{k}\\\"' for k in missing_keys)))\n",
        "   2152 if len(error_msgs) > 0:\n",
        "-> 2153     raise RuntimeError('Error(s) in loading state_dict for {}:\\\n",
        "\\\\t{}'.format(\n",
        "   2154                        self.__class__.__name__, \\\"\\\n",
        "\\\\t\\\".join(error_msgs)))\n",
        "   2155 return _IncompatibleKeys(missing_keys, unexpected_keys)\n",
        "\n",
        "RuntimeError: Error(s) in loading state_dict for FasterRCNN:\n",
        "\\tMissing key(s) in state_dict: \\\"backbone.body.conv1.weight\\\", ... \\\"roi_heads.box_predictor.bbox_pred.bias\\\". \n",
        "\\tUnexpected key(s) in state_dict: \\\"state\\\", \\\"param_groups\\\". \"\n",
        "}\n",
        "```\n",
        "\n",
        "This error is relates to handling loading of the serialised optimiser object by the save function. This function must correctly load the optimizer in instances where I want to recommence training from a saved checkpoint. \\\n",
        "\n",
        "I modififed the load_model function according to [this](https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html) article. \\\n",
        "\n",
        "First, the saved state_dict is [deserialised](https://learn.microsoft.com/en-us/dotnet/standard/serialization/) from the .pth file, then it is passed to load_state_dict. 'load_state_dict' expects a [torch.optim.Optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer) object. In the previous load_model(), it gets a dict object. To solve this, I must call load_state_dict on the SGD optimizer. The optimizer should be initialised and returned by the get_model_instance_object_detection() function, but currently only the train() function initialises an optimizer. I think it would be best to isolate model and optimizer initialisation in the model getter. This way, there isn't a chance that the optimizer gets redefined at some point, causing strange behaviour.\\\n",
        "\n",
        "This will require modifications to get_model_instance_object_detection(), load_model(), and train() as follows:\\"
      ],
      "id": "cf2a2694"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_model_instance_object_detection(num_class: int) -> fasterrcnn_resnet50_fpn_v2:\n",
        "    # New weights with accuracy 80.858%\n",
        "    weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT # alias is .DEFAULT suffix, weights = None is random initialisation, box MAP 46.7, params, 43.7M, GFLOPS 280.37 https://github.com/pytorch/vision/pull/5763\n",
        "    model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.0001)\n",
        "    preprocess = weights.transforms()\n",
        "    # finetuning pretrained model by transfer learning\n",
        "    # get num of input features for classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.SGD( # optimizer defined in model getter according to pytorch 'recipes'\n",
        "        params,\n",
        "        lr=0.005,\n",
        "        momentum=0.9,\n",
        "        weight_decay=0.0005\n",
        "    )\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_class)\n",
        "    return model, optimizer, preprocess"
      ],
      "id": "81fa589a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def train(model, data_loader, data_loader_test, device, num_epochs, precedent_epoch, save_dir, optimizer): \n",
        "    model.train()\n",
        "    # Initialize lists to store metrics\n",
        "    train_losses = []\n",
        "    \n",
        "    # and a learning rate scheduler\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "        optimizer,\n",
        "        step_size=3,\n",
        "        gamma=0.1\n",
        "    )\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "        metric_logger = train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "    \n",
        "    # Get the average loss for this epoch\n",
        "        epoch_loss = metric_logger.meters['loss'].global_avg\n",
        "        train_losses.append(epoch_loss)\n",
        "\n",
        "        # Save checkpoint\n",
        "        save_checkpoint(model, optimizer, epoch + precedent_epoch, save_dir)\n",
        "        \n",
        "        # Update the learning rate\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    return num_epochs + precedent_epoch, train_losses"
      ],
      "id": "97f3d582",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def load_model(save_dir: str, num_classes: int, model_file_name: str):\n",
        "    model, optimizer, preprocess = get_model_instance_object_detection(num_classes)\n",
        "    checkpoint = torch.load(save_dir + f'{model_file_name}.pth', weights_only=True)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    return model, optimizer, epoch"
      ],
      "id": "b0c593c0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After the above modifications, I ran the following script:"
      ],
      "id": "722b7301"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model, optimizer, epoch = load_model('../checkpoints/', num_class, 'checkpoint_epoch_0')\n",
        "\n",
        "eval_metrics = evaluate_model(model, data_loader_test, device)\n",
        "\n",
        "print(eval_metrics, '\\n', len(eval_metrics))\n",
        "\n",
        "print(\"\\n -end-\")"
      ],
      "id": "c9877110",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "... the evaluator now runs again! \\\n",
        "It may be sensible to tuck data_loader, data_loader_test, as well as validation and training subset and indice permutation lists, into load_model. With that being said, I'm conscious that the function signatures are huge, as well as their return statements. I don't think I should add to that. Further to this, implementation of these helper functions in different object detection contexts will likely be suited to different batching parameters based on variables like dataset size. For this reason, I think it's best load_model() handles loading the optimizer and model deserialisation and variable loading only. \\\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "I just learned about *args and **kwargs. I think this could help cut down the complexity of the function signatures. I will try to use these from now on, where appropriate.\n",
        ":::\n",
        "\n",
        "This output is interesting. Firstly, the loss metrics are missing from eval_metrics, yet are output at the end of the previous section. This is because the two are handled separately! The evaluate_model() function is using [coco_eval](../src/torchvision_deps/coco_eval.py), returning coco_evaluator's 'bbox' stats. The training loss metrics are generated by [engine's](../src/torchvision_deps/engine.py) 'train_one_epoch'. This means to handle plotting of evaluation and training metrics, I will need to modify coco_eval, engine and/or utils in some way. Firstly, to clarify why the call to plot_eval_metrics is raising a type-error, I  will print coco_evaluator as well as the eval_metrics returned from the call to evaluate_model:"
      ],
      "id": "5a2244ac"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def evaluate_model(model, data_loader_test,device):\n",
        "    model.eval()\n",
        "\n",
        "    coco_evaluator = evaluate(model, data_loader_test, device=device)\n",
        "    print(coco_evaluator)\n",
        "    # Extract evaluation metrics\n",
        "    eval_metrics = coco_evaluator.coco_eval['bbox'].stats\n",
        "  \n",
        "    return eval_metrics"
      ],
      "id": "eab645c0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "eval_metrics = evaluate_model(model, data_loader_test, device)\n",
        "\n",
        "print(eval_metrics, '\\n', len(eval_metrics))"
      ],
      "id": "9e45d765",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The CocoEvaluator class is defined in [torchvision_deps](../src/torchvision_deps/coco_eval.py). \\\n",
        "- 'eval_metrics' is a one-dimensional array of floats, where each index is one of the 12 metrics outlined [here](https://cocodataset.org/#detection-eval). \\\n",
        "\n",
        "- Currently, I'm trying to plot the average loss. While this would be okay if I were training for many epochs, that won't work when trying to create a line plot for one epoch's training to evaluate batch variability. Instead, I want to plot the metrics as the training or evaluation happens. Currently I am using train_one_epoch() and evaluate() from [engine](../src/torchvision_deps/engine.py) in torchvision_deps. They use PyTorch's [MetricLogger](https://pytorch.org/tnt/stable/utils/generated/torchtnt.utils.loggers.MetricLogger.html) to store metrics, where 'log_every()' method from MetricLogger is handling the print frequency of metrics as training/evaluation progresses through the epoch. I want to plot the data that is currently printed by log_every(). This will require modification to either log_every or the metric loggers in engine's train_one_epoch. This discussion is helpful: https://discuss.pytorch.org/t/object-detection-fine-tuning-interpreting-printout-of-train-one-epoch/193496/2 \\\n",
        "\n",
        "I realised the MetricLogger class is defined locally in [utils.py](../src/torchvision_deps/T_and_utils/utils.py). 'log_every()' is a method of MetricLogger. I want to avoid modifying dependency files as much as possible. \\\n",
        "\n",
        "I realised I can just change the way I handle the metric logger returned from train_one_epoch in my train() function. Before I modify it, I want to retrain on a small batch (not the full epoch) to see print the metric logger, just to check that the granular progress through the epoch survives 'train_one_epoch': \\ \n"
      ],
      "id": "5e25e8ee"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def train(model, data_loader, data_loader_test, device, num_epochs, precedent_epoch, save_dir, optimizer): \n",
        "    model.train()\n",
        "    # Initialize lists to store metrics\n",
        "    train_losses = []\n",
        "    \n",
        "    # and a learning rate scheduler\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "        optimizer,\n",
        "        step_size=3,\n",
        "        gamma=0.1\n",
        "    )\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "        metric_logger = train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "\n",
        "        for k, v in metric_logger.meters.items():\n",
        "            print(f'Key: {k} \\t Value: {v}')\n",
        "    \n",
        "    # Get the average loss for this epoch\n",
        "        epoch_loss = metric_logger.meters['loss'].global_avg\n",
        "        train_losses.append(epoch_loss)\n",
        "\n",
        "        # Save checkpoint\n",
        "        save_checkpoint(model, optimizer, epoch + precedent_epoch, (save_dir + 'checkpoints'))\n",
        "        \n",
        "        # Update the learning rate\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    return num_epochs + precedent_epoch, train_losses"
      ],
      "id": "ffc4e74b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then execute the following script for a short training session, after which all keys in metric_logger are printed:"
      ],
      "id": "4b4b2d51"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model, optimizer, preprocess = get_model_instance_object_detection(2)\n",
        "\n",
        "dataset: Plate_Image_Dataset = Plate_Image_Dataset.Plate_Image_Dataset(\n",
        "        img_dir=str(img_dir), \n",
        "        annotations_file=str(annotations_file),\n",
        "        transforms=preprocess, \n",
        "        )\n",
        "\n",
        "# split the dataset in train and test set\n",
        "dataset_size = 20\n",
        "#validation_size = min(50, int(dataset_size // 5))  # Use 20% of data for testing, or 50 samples, whichever is smaller\n",
        "indices = [int(i) for i in torch.randperm(dataset_size).tolist()]\n",
        "\n",
        "#dataset_validation = torch.utils.data.Subset(dataset, indices[-validation_size:])\n",
        "dataset_train = torch.utils.data.Subset(dataset, indices[:])\n",
        "\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset_train,\n",
        "    batch_size=1,\n",
        "    shuffle=True,\n",
        "    collate_fn=utils.collate_fn\n",
        ")\n",
        "\n",
        "num_epochs = 1\n",
        "precedent_epoch = 0\n",
        "save_dir = '../'\n",
        "\n",
        "epoch, loss_metrics = train(model, data_loader, None, device, num_epochs, precedent_epoch, save_dir, optimizer)\n",
        "\n",
        "print(\"\\n -end-\")"
      ],
      "id": "e8cdd0fd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The keys in metric_logger.meters are 'loss, loss_classifier, loss_box_reg, loss_objectness, loss_rpn_box_reg'. 'train_one_epoch' averages the meters' value throughout training by making a call to MetricLogger.update(). \n",
        "\n",
        "Firstly I will modify the MetricLogger class, adding the 'intra_epoch_loss' member. I will store the log_message here for later access, as follows: "
      ],
      "id": "de21a407"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "class MetricLogger:\n",
        "    def __init__(self, delimiter=\"\\t\"):\n",
        "        self.meters = defaultdict(SmoothedValue)\n",
        "        self.delimiter = delimiter\n",
        "\n",
        "        # there is definitely a more elegant way to do this, but this should work\n",
        "        self.intra_epoch_loss = defaultdict(list)\n",
        "\n",
        "    def update(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                v = v.item()\n",
        "            assert isinstance(v, (float, int))\n",
        "            self.meters[k].update(v)\n",
        "\n",
        "    def __getattr__(self, attr):\n",
        "        if attr in self.meters:\n",
        "            return self.meters[attr]\n",
        "        if attr in self.__dict__:\n",
        "            return self.__dict__[attr]\n",
        "        raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{attr}'\")\n",
        "\n",
        "    def __str__(self):\n",
        "        loss_str = []\n",
        "        for name, meter in self.meters.items():\n",
        "            loss_str.append(f\"{name}: {str(meter)}\")\n",
        "        return self.delimiter.join(loss_str)\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        for meter in self.meters.values():\n",
        "            meter.synchronize_between_processes()\n",
        "\n",
        "    def add_meter(self, name, meter):\n",
        "        self.meters[name] = meter\n",
        "\n",
        "    def log_every(self, iterable, print_freq, header=None):\n",
        "        i = 0\n",
        "        if not header:\n",
        "            header = \"\"\n",
        "        start_time = time.time()\n",
        "        end = time.time()\n",
        "        iter_time = SmoothedValue(fmt=\"{avg:.4f}\")\n",
        "        data_time = SmoothedValue(fmt=\"{avg:.4f}\")\n",
        "        space_fmt = \":\" + str(len(str(len(iterable)))) + \"d\"\n",
        "        if torch.cuda.is_available():\n",
        "            log_msg = self.delimiter.join(\n",
        "                [\n",
        "                    header,\n",
        "                    \"[{0\" + space_fmt + \"}/{1}]\",\n",
        "                    \"eta: {eta}\",\n",
        "                    \"{meters}\",\n",
        "                    \"time: {time}\",\n",
        "                    \"data: {data}\",\n",
        "                    \"max mem: {memory:.0f}\",\n",
        "                ]\n",
        "            )\n",
        "        else:\n",
        "            log_msg = self.delimiter.join(\n",
        "                [header, \"[{0\" + space_fmt + \"}/{1}]\", \"eta: {eta}\", \"{meters}\", \"time: {time}\", \"data: {data}\"]\n",
        "            )\n",
        "        MB = 1024.0 * 1024.0\n",
        "        for obj in iterable:\n",
        "            data_time.update(time.time() - end)\n",
        "            yield obj\n",
        "            iter_time.update(time.time() - end)\n",
        "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
        "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
        "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
        "                if torch.cuda.is_available():\n",
        "                    \n",
        "                    for k, v in self.meters.items():        # <------ insertion here\n",
        "                        self.intra_epoch_loss[k].append(v.value)     # <------- here see SmoothedValue class (v is SmoothedValue object)\n",
        "                    self.intra_epoch_loss['progression'].append((((i+1)/(len(iterable))*100)))  # <----- and here\n",
        "                    print(\n",
        "                        log_msg.format(\n",
        "                            i,\n",
        "                            len(iterable),\n",
        "                            eta=eta_string,\n",
        "                            meters=str(self),\n",
        "                            time=str(iter_time),\n",
        "                            data=str(data_time),\n",
        "                            memory=torch.cuda.max_memory_allocated() / MB,\n",
        "                        )\n",
        "                    )\n",
        "                else:\n",
        "                    for k, v in self.meters.items():        # <------ insertion here\n",
        "                        self.intra_epoch_loss[k].append(v.value)     # <------- here\n",
        "                    self.intra_epoch_loss['progression'].append(((i+1)/(len(iterable))*100))  # <----- and here: could use add_meter in train_one_epoch to remove this line\n",
        "                    print(\n",
        "                        log_msg.format(\n",
        "                            i, len(iterable), eta=eta_string, meters=str(self), time=str(iter_time), data=str(data_time)\n",
        "                        )\n",
        "                    )\n",
        "            i += 1\n",
        "            end = time.time()\n",
        "        total_time = time.time() - start_time\n",
        "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "        print(f\"{header} Total time: {total_time_str} ({total_time / len(iterable):.4f} s / it)\")"
      ],
      "id": "5999fb12",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I will pass these to a plot_training_loss() helper function.\n"
      ],
      "id": "2dd5a5ba"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_training_loss(root_dir: str, epoch: int, **kwargs):\n",
        "    progression = [v for v in kwargs['progression']] # I'm not sure if list comprehension is neccessary here... perhaps just try progression = kwargs['progression']?\n",
        "    loss_objectness = [v for v in kwargs['loss_objectness']]\n",
        "    loss = [v for v in kwargs['loss']]\n",
        "    loss_rpn_box_reg = [v for v in kwargs['loss_rpn_box_reg']]\n",
        "    loss_classifier = [v for v in kwargs['loss_classifier']]\n",
        "    lr = [v for v in kwargs['lr']] \n",
        "\n",
        "    # the metrics are passed using list comprehension in train()\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(progression, loss, label='Loss')\n",
        "    plt.plot(progression, loss_objectness, label='Loss Objectness')\n",
        "    plt.plot(progression, loss_rpn_box_reg, label='Loss RPN Box Reg')\n",
        "    plt.plot(progression, loss_classifier, label='Loss Classifier')\n",
        "\n",
        "    plt.xlabel('Progression through Epoch / %')\n",
        "    plt.ylabel('Loss / Log10')\n",
        "    plt.yscale(\"log\")\n",
        "    plt.title(f'Loss Metrics Progression Through Epoch {epoch}')\n",
        "    plt.legend()\n",
        "    plt.grid(False)\n",
        "    plt.savefig(f'{root_dir}/results/loss_metrics_epoch_{epoch}')\n",
        "    plt.show()"
      ],
      "id": "3b8dbed9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now make a call to it in train() instead:"
      ],
      "id": "d0e45a87"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def train(model, data_loader, data_loader_test, device, num_epochs, precedent_epoch, save_dir, optimizer): \n",
        "    model.train()\n",
        "    # Initialize lists to store metrics\n",
        "    train_losses = []\n",
        "    \n",
        "    # and a learning rate scheduler\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "        optimizer,\n",
        "        step_size=3,\n",
        "        gamma=0.1\n",
        "    )\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every iteration\n",
        "        metric_logger = train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=1)\n",
        "        for k, v in metric_logger.intra_epoch_loss.items(): # <----- insertion here\n",
        "            print(f\"\\n Key: {k}\\n Value: {v}\") # <----- insertion here\n",
        "        # Plot loss throughout *this* epoch's training, save plot in results\n",
        "        plot_training_loss(save_dir, epoch, **{k: v for k, v in metric_logger.intra_epoch_loss.items()}) # <----- insertion here\n",
        "        \n",
        "    # Get the average loss for this epoch\n",
        "        epoch_loss = metric_logger.meters['loss'].global_avg\n",
        "        train_losses.append(epoch_loss)\n",
        "\n",
        "        # Save checkpoint\n",
        "        save_checkpoint(model, optimizer, epoch + precedent_epoch, (save_dir + 'checkpoints'))\n",
        "        \n",
        "        # Update the learning rate\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    return num_epochs + precedent_epoch, train_losses"
      ],
      "id": "568c2e51",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now run the script make sure that plots are saved in results/ which is now in the root of the project:\n"
      ],
      "id": "272f05fc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model, optimizer, preprocess = get_model_instance_object_detection(2)\n",
        "\n",
        "dataset: Plate_Image_Dataset = Plate_Image_Dataset.Plate_Image_Dataset(\n",
        "        img_dir=str(img_dir), \n",
        "        annotations_file=str(annotations_file),\n",
        "        transforms=preprocess, \n",
        "        )\n",
        "\n",
        "# split the dataset in train and test set\n",
        "dataset_size = 2\n",
        "#validation_size = min(50, int(dataset_size // 5))  # Use 20% of data for testing, or 50 samples, whichever is smaller\n",
        "indices = [int(i) for i in torch.randperm(dataset_size).tolist()]\n",
        "\n",
        "#dataset_validation = torch.utils.data.Subset(dataset, indices[-validation_size:])\n",
        "dataset_train = torch.utils.data.Subset(dataset, indices[:])\n",
        "\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset_train,\n",
        "    batch_size=1,\n",
        "    shuffle=True,\n",
        "    collate_fn=utils.collate_fn\n",
        ")\n",
        "\n",
        "num_epochs = 2\n",
        "precedent_epoch = 0\n",
        "save_dir = '../'\n",
        "\n",
        "epoch, loss_metrics = train(model, data_loader, None, device, num_epochs, precedent_epoch, save_dir, optimizer)\n",
        "\n",
        "print(\"\\n -end-\")"
      ],
      "id": "707e6fe5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That's the 'plot_training_loss' sorted for plotting the training loss metrics within an epoch. Now I need to write a function to handle plotting losses accross numerous epochs. This will be called in the train() helper function, after the epoch training loop. 'train_losses' stores the averaged losses at the end of each epoch of training. \\\n",
        "\n",
        "First, I'll define 'plot_train_losses_across_epochs':\n"
      ],
      "id": "975a1286"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_train_losses_across_epochs(save_dir: str, precedent_epoch: int, epochs: int, **kwargs):\n",
        "    progression = [v for v in kwargs['progression']] # I'm not sure if list comprehension is neccessary here... perhaps just try progression = kwargs['progression']?\n",
        "    loss_objectness = [v for v in kwargs['loss_objectness']]\n",
        "    loss = [v for v in kwargs['loss']]\n",
        "    loss_rpn_box_reg = [v for v in kwargs['loss_rpn_box_reg']]\n",
        "    loss_classifier = [v for v in kwargs['loss_classifier']]\n",
        "    lr = [v for v in kwargs['lr']] \n",
        "    \n",
        "    # the metrics are passed using list comprehension in train()\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(progression, loss, label='Loss')\n",
        "    plt.plot(progression, loss_objectness, label='Loss Objectness')\n",
        "    plt.plot(progression, loss_rpn_box_reg, label='Loss RPN Box Reg')\n",
        "    plt.plot(progression, loss_classifier, label='Loss Classifier')\n",
        "\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss / Log10')\n",
        "    plt.yscale(\"log\")\n",
        "    plt.title(f'Loss Metrics from Epoch {precedent_epoch}-{precedent_epoch + epochs}')\n",
        "    plt.legend()\n",
        "    plt.grid(False)\n",
        "    plt.savefig(f'{save_dir}/results/loss_metrics_epochs_{precedent_epoch}-{precedent_epoch + epochs}')\n",
        "    plt.show()"
      ],
      "id": "2a1e0aa1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next I will call the plot_train_losses_across_epochs() inside train(), and will also modify loss_metrics so that it is a dictionary.\n"
      ],
      "id": "078bd3a5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def train(model, data_loader, data_loader_test, device, num_epochs, precedent_epoch, save_dir, optimizer): \n",
        "    model.train()\n",
        "    # Initialize lists to store metrics\n",
        "    losses_across_epochs = defaultdict(list)\n",
        "    \n",
        "    # and a learning rate scheduler\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "        optimizer,\n",
        "        step_size=3,\n",
        "        gamma=0.1\n",
        "    )\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every iteration\n",
        "        metric_logger = train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=1)\n",
        "        for k, v in metric_logger.intra_epoch_loss.items(): # <----- insertion here\n",
        "            print(f\"\\n Key: {k}\\n Value: {v}\") # <----- insertion here\n",
        "            # Get the average loss metrics at this epoch\n",
        "        for k, v in metric_logger.meters.items():\n",
        "            losses_across_epochs[k].append(v.value) # see SmoothedValude in utils.py for value as v is a SmoothedValue object\n",
        "        losses_across_epochs['progression'].append(((epoch+1)/num_epochs)*100)\n",
        "\n",
        "        # plot loss throughout *this* epoch's training, save plot in results\n",
        "        plot_training_loss(save_dir, epoch, **{k: v for k, v in metric_logger.intra_epoch_loss.items()})     # <----- insertion here\n",
        "\n",
        "        # Save checkpoint\n",
        "        save_checkpoint(model, optimizer, epoch + precedent_epoch, (save_dir + 'checkpoints'))\n",
        "        \n",
        "        # Update the learning rate\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    # plot average train losses across several epochs\n",
        "    plot_train_losses_across_epochs(save_dir, precedent_epoch, num_epochs, **{k: v for k, v in losses_across_epochs.items()})\n",
        "    return num_epochs + precedent_epoch"
      ],
      "id": "eda64642",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now running this script to check that this works..."
      ],
      "id": "671fdbed"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model, optimizer, preprocess = get_model_instance_object_detection(2)\n",
        "\n",
        "dataset: Plate_Image_Dataset = Plate_Image_Dataset.Plate_Image_Dataset(\n",
        "        img_dir=str(img_dir), \n",
        "        annotations_file=str(annotations_file),\n",
        "        transforms=preprocess, \n",
        "        )\n",
        "\n",
        "# split the dataset in train and test set\n",
        "dataset_size = 2\n",
        "#validation_size = min(50, int(dataset_size // 5))  # Use 20% of data for testing, or 50 samples, whichever is smaller\n",
        "indices = [int(i) for i in torch.randperm(dataset_size).tolist()]\n",
        "\n",
        "#dataset_validation = torch.utils.data.Subset(dataset, indices[-validation_size:])\n",
        "dataset_train = torch.utils.data.Subset(dataset, indices[:])\n",
        "\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset_train,\n",
        "    batch_size=1,\n",
        "    shuffle=True,\n",
        "    collate_fn=utils.collate_fn\n",
        ")\n",
        "\n",
        "num_epochs = 2\n",
        "precedent_epoch = 0\n",
        "save_dir = '../'\n",
        "\n",
        "epoch = train(model, data_loader, None, device, num_epochs, precedent_epoch, save_dir, optimizer)\n",
        "\n",
        "print(\"\\n -end-\")"
      ],
      "id": "e80a328d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given the y-axis is logarithmic, and log10(0) is -infinity (NaN), and that matplotlib wants the x and y axis to be of equal length, I cannot start the plot x axis from 0%. Just something to keep in mind. \\\n",
        "\n",
        "I think the intra-epoch loss metrics will be useful in evaluating batch to batch variability. Mainly I think it will help for short tests to gain insight into a model's module dysfunction. \\\n",
        "Training plot generation now works. \n",
        "\n",
        "Now for handling plotting of evaluation metrics... \\\n",
        "\n",
        "### Considering appropriate visualisation of coco_eval metrics in plots:\n",
        "Based on earlier outputs from the evaluate() helper function, I know that CocoEvaluator from [coco_eval.py](../src/torchvision_deps/coco_eval.py) produces Average Precision and Recall (AP and AR, respectively) at several IOU thresholds. It is important that these data are visualised in a way that gives insight into the model's performance prior to training on the cluster. I have included a brief discussion of Precision X Recall curves and their use in evaluating object detection networks' performance on a per-sample basis. For now, the AP and AR from coco_eval are sufficient. The idea is to make a call to evaluate() in [helper_training_functions](../src/plate_detect/helper_training_functions.py) after each epoch of training. I will then store the AP and AR metrics at each IOU threshold in a dictionary. Once training is complete, I will make a call to plot_eval_metrics() again from [helper_training_functions](../src/plate_detect/helper_training_functions.py) to plot AP and AR metrics as for each epoch of training. \\\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "#### Precision X Recall curves \n",
        "An interesting discussion [here](https://github.com/rafaelpadilla/Object-Detection-Metrics?tab=readme-ov-file#precision-x-recall-curve) proposes the Precision x Recall curve which is implemented by the [PASCAL VOC Challenge](http://host.robots.ox.ac.uk/pascal/VOC/).\n",
        "\n",
        "For clarity, I would like to define the following terms: \\\n",
        "- True Positive (TP): a correct detection, where IOU >= threshold. \\\n",
        "- False Positive (FP): an incorrect detection, where IOU < threshold. \\\n",
        "- False Negative (FN): a ground truth is not detected. \\\n",
        "- True Negative (TN): *this isn't used.* TN is any possible bounding boxes that were correctly not detected in an image. \\\n",
        "- Precision: ability of a model to identify only the relevant objects. This is given by TP/(TP + FP). \\\n",
        "- Recall: ability of a model to find all ground truth bounding boxes. It is given by TP/(TP + FN). \\\n",
        "\n",
        "The threshold is typically 50%, 75%, or 95%.\n",
        "\n",
        "**Precision x Recall Curve:** \\\n",
        "The Precision x Recall curve is based on the rationale that a good object detector should only detect True Positives (0 FP = high precision), and should be able to correctly identify all ground truth objects (0 FN = high recall). On the other hand, a poor detector has to increase the number of detected objects in order to identify all ground truth objects. In this case, the detector compromises precision to improve recall. This is illustrated in such a plot, where as the recall of a detector increases, its precision decreases. \\\n",
        "\n",
        "**Area Under the Curve (AUC) of Precision x Recall Curve:** \\\n",
        "AUC provides a quantitative descriptor for the Precision x Recall curve to allow for comparisons.\n",
        "\n",
        "Currently, [coco_eval](../src/torchvision_deps/coco_eval.py) is averaging the AP and AR for the whole validation set. As a reminder, this is an output produced by a call to the evaluate() helper function earlier: \n",
        "\n",
        "```{bash}\n",
        "Test:  [ 0/50]  eta: 0:08:31  model_time: 10.2026 (10.2026)  evaluator_time: 0.0037 (0.0037)  time: 10.2212  data: 0.0148\n",
        "Test:  [49/50]  eta: 0:00:10  model_time: 10.2246 (10.6478)  evaluator_time: 0.0047 (0.0055)  time: 10.4892  data: 0.0122\n",
        "Test: Total time: 0:08:53 (10.6671 s / it)\n",
        "Averaged stats: model_time: 10.2246 (10.6478)  evaluator_time: 0.0047 (0.0055)\n",
        "Accumulating evaluation results...\n",
        "DONE (t=0.07s).\n",
        "IoU metric: bbox\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
        " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
        " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
        " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
        " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
        "[array([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.])] \n",
        "```\n",
        "\n",
        "\n",
        "In order to create the plot, I would need to expose the metrics calculated for each sample individually. These are generated by engine and coco_eval, but are currently tucked away. Exposing metrics for each sample would require inspection and modification of these files. \\\n",
        "The [cocoapi github repo](https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocotools/cocoeval.py) is well commented.\n",
        ":::\n",
        "\n",
        "Firstly, I need to modify train to make a call to evaluate_model() after each epoch of training. I have added a print statement to confirm this code is behaving as expected.\n"
      ],
      "id": "4f109789"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def train(model, data_loader, data_loader_test, device, num_epochs, precedent_epoch, save_dir, optimizer): \n",
        "    model.train()\n",
        "    # Initialize lists to store metrics\n",
        "    losses_across_epochs = defaultdict(list)\n",
        "    evaluation_across_epochs = defaultdict(list)\n",
        "    \n",
        "    # and a learning rate scheduler\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "        optimizer,\n",
        "        step_size=3,\n",
        "        gamma=0.1\n",
        "    )\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every iteration\n",
        "        metric_logger = train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=1)\n",
        "        for k, v in metric_logger.intra_epoch_loss.items(): # <----- insertion here\n",
        "            print(f\"\\n Key: {k}\\n Value: {v}\") # <----- insertion here\n",
        "            # Get the average loss metrics at this epoch\n",
        "        for k, v in metric_logger.meters.items():\n",
        "            losses_across_epochs[k].append(v.value) # see SmoothedValude in utils.py for value as v is a SmoothedValue object\n",
        "        losses_across_epochs['progression'].append(((epoch+1)/num_epochs)*100)\n",
        "\n",
        "        # plot loss throughout *this* epoch's training, save plot in results\n",
        "        plot_training_loss(save_dir, epoch, **{k: v for k, v in metric_logger.intra_epoch_loss.items()})     # <----- insertion here\n",
        "\n",
        "        # Save checkpoint\n",
        "        save_checkpoint(model, optimizer, epoch + precedent_epoch, (save_dir + 'checkpoints'))\n",
        "\n",
        "        # Evaluate model perfomance on holdout dataset and append results to evaluation metric dictionary with epoch as key\n",
        "        eval_metrics = evaluate_model(model, data_loader_test, device)\n",
        "        evaluation_across_epochs[f'{epoch}'] = eval_metrics\n",
        "\n",
        "        # Update the learning rate\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    # adding a print statement here to view evaluation_across_epochs using dict comprehension\n",
        "    {print(k, '\\n', v) for k, v in evaluation_across_epochs.items()}\n",
        "\n",
        "    # plot average train losses across several epochs\n",
        "    plot_train_losses_across_epochs(save_dir, precedent_epoch, num_epochs, **{k: v for k, v in losses_across_epochs.items()})\n",
        "    return num_epochs + precedent_epoch"
      ],
      "id": "1e5fd162",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then make a call to train():"
      ],
      "id": "e5cf3244"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model, optimizer, preprocess = get_model_instance_object_detection(2)\n",
        "\n",
        "dataset: Plate_Image_Dataset = Plate_Image_Dataset(\n",
        "        img_dir=str(img_dir), \n",
        "        annotations_file=str(annotations_file),\n",
        "        transforms=preprocess, \n",
        "        )\n",
        "\n",
        "# split the dataset in train and test set\n",
        "dataset_size = 4\n",
        "validation_size = 2 # Use 20% of data for testing, or 50 samples, whichever is smaller\n",
        "indices = [int(i) for i in torch.randperm(dataset_size).tolist()]\n",
        "\n",
        "dataset_validation = torch.utils.data.Subset(dataset, indices[-validation_size:])\n",
        "dataset_train = torch.utils.data.Subset(dataset, indices[:-validation_size])\n",
        "\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset_train,\n",
        "    batch_size=1,\n",
        "    shuffle=True,\n",
        "    collate_fn=utils.collate_fn\n",
        ")\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_train,\n",
        "    batch_size=1,\n",
        "    shuffle=True,\n",
        "    collate_fn=utils.collate_fn\n",
        ")\n",
        "\n",
        "num_epochs = 2\n",
        "precedent_epoch = 0\n",
        "save_dir = '../'\n",
        "\n",
        "epoch = train(model, data_loader, data_loader_test, device, num_epochs, precedent_epoch, save_dir, optimizer)\n",
        "\n",
        "print(\"\\n -end-\")"
      ],
      "id": "0dc21fa1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on this, it looks like the above code works. I should be able now to create a plot_eval_metrics function to create a line plot visualising the Average Precision (AP) and Average Recall (AR) metrics. You should be able to see that the IoU bbox metric output from the previous code block offers numerous AP and AR metrics. \\ \n",
        "\n",
        "To make the plot simpler to visually interpret, I will at first only plot the AP and AR at the IoU thresholds averaged from 0.50:0.95 (this approach was introduced by COCO to more comprehensively evaluate model performance in contrast to single threshold evaluations - this is explained in detail [here](https://www.picsellia.com/post/coco-evaluation-metrics-explained)). \\\n",
        "\n",
        "Where there is a value of -1.000 in the bbox stats, there were no ground truth object in those size categories in the provided dataset. Of course, up until this point I have not been training on the entire dataset (given I have been debugging). This plotting function is preliminary, and if after training on a more substantial dataset there are changes to this output, I will investigate and consider their inclusion in the plot. For now, I will include AR for maxDets 100 and area= all (same for AP). When looking at the bbox stats output and the lists stored in the defaultdict evaluation_across_epochs, it is clear that the element index in the list stored for each key-value pair (epoch-bbox stats) corresponds to a line of the metric output from [coco_eval.py](../src/torchvision_deps/coco_eval.py). For each key, I am interested in the metric value at index 0 (AP @ IoU 0.50:0.95, area=all, maxDets=100) and index 8 (AR @ IoU 0.50:0.95, area=all, maxDets=100).\n"
      ],
      "id": "74cd8567"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_eval_metrics(save_dir, precedent_epoch, num_epochs, **kwargs):\n",
        "    progression = [k for k in kwargs] # should be equivalent to num_epochs - precedent_epoch\n",
        "\n",
        "    # store metrics of interest in lists\n",
        "    AP = [v[0] for k, v in kwargs.items()] \n",
        "    AR = [v[8] for k, v in kwargs.items()]\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(progression, AP, label='AP @ IoU 0.50:0.95, area=all, maxDets=100')\n",
        "    plt.plot(progression, AR, label='AR @ IoU 0.50:0.95, area=all, maxDets=100')\n",
        "\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Value')\n",
        "  \n",
        "    plt.title(f'Evaluation Metrics from Epoch {precedent_epoch}-{precedent_epoch + num_epochs}')\n",
        "    plt.legend()\n",
        "    plt.grid(False)\n",
        "    plt.savefig(f'{save_dir}/results/evaluation_metrics_epochs_{precedent_epoch}-{precedent_epoch + num_epochs}')\n",
        "    plt.show()"
      ],
      "id": "a0c862c4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, I will modify train() to call the above function after training:"
      ],
      "id": "7ee0cc44"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def train(model, data_loader, data_loader_test, device, num_epochs, precedent_epoch, save_dir, optimizer): \n",
        "    model.train()\n",
        "    # Initialize lists to store metrics\n",
        "    losses_across_epochs = defaultdict(list)\n",
        "    evaluation_across_epochs = defaultdict(list)\n",
        "    \n",
        "    # and a learning rate scheduler\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "        optimizer,\n",
        "        step_size=3,\n",
        "        gamma=0.1\n",
        "    )\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every iteration\n",
        "        metric_logger = train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=1)\n",
        "        for k, v in metric_logger.intra_epoch_loss.items(): # <----- insertion here\n",
        "            print(f\"\\n Key: {k}\\n Value: {v}\") # <----- insertion here\n",
        "            # Get the average loss metrics at this epoch\n",
        "        for k, v in metric_logger.meters.items():\n",
        "            losses_across_epochs[k].append(v.value) # see SmoothedValude in utils.py for value as v is a SmoothedValue object\n",
        "        losses_across_epochs['progression'].append(((epoch+1)/num_epochs)*100)\n",
        "\n",
        "        # plot loss throughout *this* epoch's training, save plot in results\n",
        "        plot_training_loss(save_dir, epoch, **{k: v for k, v in metric_logger.intra_epoch_loss.items()})     # <----- insertion here\n",
        "\n",
        "        # Save checkpoint\n",
        "        save_checkpoint(model, optimizer, epoch + precedent_epoch, (save_dir + 'checkpoints'))\n",
        "\n",
        "        # Evaluate model perfomance on holdout dataset and append results to evaluation metric dictionary with epoch as key\n",
        "        eval_metrics = evaluate_model(model, data_loader_test, device)\n",
        "        evaluation_across_epochs[f'{epoch}'] = eval_metrics\n",
        "\n",
        "        # Update the learning rate\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    # adding a print statement here to view evaluation_across_epochs using dict comprehension\n",
        "    {print(k, '\\n', v) for k, v in evaluation_across_epochs.items()}\n",
        "\n",
        "    # plot average train losses across several epochs\n",
        "    plot_train_losses_across_epochs(save_dir, precedent_epoch, num_epochs, **{k: v for k, v in losses_across_epochs.items()})\n",
        "    plot_eval_metrics(save_dir, precedent_epoch, num_epochs, **{k: v for k, v in evaluation_across_epochs.items()})\n",
        "    return num_epochs + precedent_epoch"
      ],
      "id": "d503affa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now I will make another call to train to verify that these plots are saved and displayed:"
      ],
      "id": "9c4596ae"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "epoch = train(model, data_loader, data_loader_test, device, num_epochs, precedent_epoch, save_dir, optimizer)\n",
        "\n",
        "print(\"\\n -end-\")"
      ],
      "id": "6e8d4b35",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code seems to work without any overt error. Now, I want to train for 15 epochs on the whole positives dataset (495 without augmentation, reduced to 445 after subsetting a holdout set of size 50). At this point, I delete the checkpoints and results from the debugging I have been doing (they should be overwritten anyway).\n"
      ],
      "id": "51e50e7c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import caffeine # prevents mac from sleeping while running this code block (see here: https://github.com/jpn--/caffeine)\n",
        "\n",
        "# split the dataset in train and test set\n",
        "dataset_size = len(dataset) # already defined \n",
        "validation_size = 50\n",
        "indices = [int(i) for i in torch.randperm(dataset_size).tolist()]\n",
        "\n",
        "dataset_validation = torch.utils.data.Subset(dataset, indices[-validation_size:])\n",
        "dataset_train = torch.utils.data.Subset(dataset, indices[:-validation_size])\n",
        "\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset_train,\n",
        "    batch_size=1,\n",
        "    shuffle=True,\n",
        "    collate_fn=utils.collate_fn\n",
        ")\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_validation,\n",
        "    batch_size=1,\n",
        "    shuffle=True,\n",
        "    collate_fn=utils.collate_fn\n",
        ")\n",
        "\n",
        "num_epochs = 15\n",
        "precedent_epoch = 0\n",
        "save_dir = '../'\n",
        "\n",
        "epoch = train(model, data_loader, data_loader_test, device, num_epochs, precedent_epoch, save_dir, optimizer)\n",
        "\n",
        "print(\"\\n -end-\")"
      ],
      "id": "3af8e49a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspecting Results after 15 Epochs of Training:\n",
        "- Firstly, it is clear that evaluation is not functioning correctly (see below). The issue could lie in numerous places, but the best place to start looking I think is engine's evaluate function, or indeed coco_eval. **Fixing this is a current priority**, as evaluation metrics will be fundamental in the comparison of models in future analyses. \\\n",
        "![evaluation metrics epochs 0-15](../results/evaluation_metrics_epochs_0-15.png) \\\n",
        "\n",
        "- The loss metrics are more promising in that they actually change throughout training. With this said, it is clear the network is struggling to learn to identify plates. \\\n",
        "\n",
        "### Discussion of Loss Metrics Epoch 0-15:\n",
        "![Loss Metrics Epoch 0-15](../results/evaluation_metrics_epochs_0-15.png) \\\n",
        "\n",
        "Firstly, loss (blue) appears identical to loss for the Region Proposal Network box regression (green). Loss should be equal to the total loss over the period trained. This is due to the different loss metrics being orders of magnitude apart, where due to the logarithmic y-axis, their summation is decimally more than the loss for the RPN box regression. This could be solved by normalising the total loss, or more simply by separating the metrics into different graphs and displaying them in multiple panels. \\\n",
        "\n",
        "Also, when plotting the loss metrics across several epochs, the x=axis scale units should be epoch count, not percentage progression through training This can be fixed simply by the following modification: \\\n"
      ],
      "id": "d20faec5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_train_losses_across_epochs(save_dir: str, precedent_epoch: int, epochs: int, **kwargs):\n",
        "    loss_objectness = [v for v in kwargs['loss_objectness']]\n",
        "    loss = [v for v in kwargs['loss']]\n",
        "    loss_rpn_box_reg = [v for v in kwargs['loss_rpn_box_reg']]\n",
        "    loss_classifier = [v for v in kwargs['loss_classifier']]\n",
        "    lr = [v for v in kwargs['lr']] \n",
        "\n",
        "    training_range =  [i for i in range(precedent_epoch, precedent_epoch + epochs)]  # list of epochs as ints\n",
        "    \n",
        "    # the metrics are passed using list comprehension in train()\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(training_range, loss, label='Loss')\n",
        "    plt.plot(training_range, loss_objectness, label='Loss Objectness')\n",
        "    plt.plot(training_range, loss_rpn_box_reg, label='Loss RPN Box Reg')\n",
        "    plt.plot(training_range, loss_classifier, label='Loss Classifier')\n",
        "\n",
        "    plt.xlabel('Epoch Count')\n",
        "    plt.ylabel('Loss / Log10')\n",
        "    plt.yscale(\"log\")\n",
        "    plt.title(f'Loss Metrics from Epoch {precedent_epoch}-{precedent_epoch + epochs}')\n",
        "    plt.legend()\n",
        "    plt.grid(False)\n",
        "    plt.savefig(f'{save_dir}/results/loss_metrics_epochs_{precedent_epoch}-{precedent_epoch + epochs}')\n",
        "    plt.show()"
      ],
      "id": "aa261891",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The orders of magnitude difference between the loss for the RPN box regression relative to the loss for the classifier head and the RPN objectness loss is likely due to the fact that both the objectness loss and classifier loss use the Binary Cross Entropy loss function, whereas box regression loss for the RPN uses L1 or 'Huber' loss which measures pixel-wise differences in bounding box coordinates, and can consequently be much larger. \\ \n",
        "\n",
        "** Explore these loss functions here **\n",
        "\n",
        "**Discussing Loss Objectness:** \\\n",
        "Loss objectness (orange) initially decreases before ascending slightly and plateauing. Loss objectness evaluates the objectness score produced by the RPN relative to ground truth. Specifically: \\\n",
        "- Objectness score: the probability that an object exists in a given ROI proposed by the RPN. This is a continuous logit (unnormalised probability) passed through a sigmoid function (output between 0 and 1). \\\n",
        "- Loss objectness: in this model it describes the performance of rpn.head.cls_logits which is a [2D convolutional layer](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) of dimensions 256x3, kernel size 1.  This is relative to the ground truth, which is 1 if a given anchor box overlaps a bounding box label (the ground truth) at some IoU threshold (0.50 for example), and is 0 otherwise. The [binary cross entropy loss](https://www.youtube.com/watch?v=Pwgpl9mKars) function is used to quantitatively describe the degree to which the probability is concordant with the ground truth binary value, where loss approaches infinity as these variables diverge, and approaches 0 as they converge (i.e., as they 'agree'). \\\n",
        "\n",
        "Hence, the loss objectness initially decreasing across epochs of training is good, but its later resurgence shows the network is struggling to learn to identify plates.\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Adding image augmentation to helper functions...\n",
        "This is definitely something that needs to be implemented in the future. Here are some brief launch-points I can use if I come back to this. My priority is getting some training done for more than just 1 epoch on the HPC, which in itself requires a locally defined Faster R-CNN class.\n",
        "\n",
        "See [here](https://pdf.sciencedirectassets.com/272206/1-s2.0-S0031320322X00149/1-s2.0-S0031320323000481/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEMX%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJIMEYCIQDHGwjyK8inLmJunlMGBkjPI%2B2RQhf29a3PpwUh%2FtcGtQIhAKXEABtOLty%2FeIaoLRArLOo1AG1g5sCAFuBCid9AF0jFKrMFCC4QBRoMMDU5MDAzNTQ2ODY1IgzGC8wcf0hM6U6%2BuigqkAXE2vBrPyLEzEszAdKmPGPE%2Fe2PuWiGsl4asY0mfwrTgBHcCJE%2B7mvDq8J8zDT1gLeiyrumMRHkpG17xRZHf3zGAu%2BV8ElbuBhsVuHZAb%2FnX4n5e7oLaKOO4YW2y4ETsCtwLroPKH4FcWxTretUzrlbhCAS%2BX5d861gT0Y6UTz1LEbIEgq4OWRRXSW5Yn%2Fq1DxzX8wOvw0pw6A%2F5MgVxcz67diPlmC7ph58goHCtHrkUJGqOTWKge%2B8ZCPPPkphqh3I9KbDfhGTKfhDaucbWl1J7eVZoJ0VX8ArJkjW3TGhkS73brzjdN7HR8wRRNBiVXqJhYPofxKyPqKASiRRj7MRIhOgzbkU5d0z4zxNt8Xliwc0bCymJ%2BPepBkjnyJ%2F5%2FCodwL8m5O6m3La5hrVAAjC9x7NIt%2Btecb176q%2BDRB3ELYwD8v0%2F0c8jTUmUeNZatzGPH3WTUaC%2Bo%2B4nKsrpAobYAYOQX8JmcmlT9RO6kTkhAVKsuSN54kl1K%2Fs977nu5B%2FrvePps7UFcN9T580vE%2B2oFa1uGwdbJr1IYZslqT3od3Gz0NmynC1sV%2FRgAYvjk6IBDG7W4FaDmmMYF8vOlD%2FBxZUuLMmw%2BcHlwJIROMn73F77fXo2K0IQMByrmiEg%2FK%2BvKLVLnCrRjpo%2FqnnFipG5FbBforeDYUmN9x6lN%2BH9S%2Bnho8kKXrwJscWXC9Lvs90E8ccWorPjPC52be0W7Y%2FxqEtfvutJ4hnvDT9ilwvpNbu%2BdHsbmwGsTiXzRjwNy4ZhCo7MLSITLM9VR9mVRci4ulnxV2QpXvSBvEYB1%2BF8VcFQIiTavC69L0sUs%2BFYZyZjExT2gW4%2Fc%2Bldptpmdz2V4WIH%2BHKjNX%2BaL0CGKRLejDyj8S4BjqwAe0oxq1p9jccRHXzaAZcTmiwV3fBbPgX4CN%2FuTG8kVpMikl6fAyT4tGm8FD3wT8sPtI2raxqEI0wYIZ33t0t4vYB0NQjFcWSjF%2FocTTGT%2FQITMsLBIP3YFOTJG8SLlRnUZ8sxKsoTfZcs37ToI%2BWZ%2FZtBoj%2Fc0iuU97k3KehPAGwKs8%2BBQRJpXDum8l3y6QN5zGp1tFxI99lg7Nu0RzMg0ZOiDbU0m3pW5rTiMbPZQdn&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20241017T140230Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYS2ZZAFYX%2F20241017%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=b216340b0855727efb00bc68031e19f29813254ab964d60a1685e44e45714c89&hash=dd2c23826d13b059c59a8112432e10407ff621d79cac2d6790b0795c02f0f467&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0031320323000481&tid=spdf-95560c99-4f04-404c-86e1-a3a1717cba5c&sid=6f3a660355f9464580690b17c2898370a5bagxrqb&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=1d04570653560a560e5c52&rr=8d40d3a30a2c63f3&cc=gb) for a comprehensive overview of image augmentation techniques for deep learning. \n",
        "\n",
        "There they achieved an accuracy improvement of around 1.5% using [mixup](https://arxiv.org/pdf/1710.09412), which uses 'convex combinations of pairs of examples and their labels' to improve generalisation of network architectures.\n",
        ":::\n",
        "\n",
        "Plots of loss metrics within an epoch's training exhibit high variance, which is to be expected:\n",
        "![loss objectness epoch 13](../results/loss_metrics_epoch_13.png) \\\n",
        "\n",
        "**The more probable issue...** \\\n",
        "The default weights loaded for this model are trained on ImageNet images, which comprises almost exclusively objects that exhibit '[Lambertian reflectance](https://en.wikipedia.org/wiki/Lambertian_reflectance)' (i.e., typical matte objects with consistent surface appearance regardless of the its positional relationship to an observer due to diffuse surface reflectivity). \\\n",
        "\n",
        "In the case of microbe growth plates, which are reflective and transparent 'non-Lambertian' objects, the network parameters are primed to extract features of objects with Lambertian surfaces. As a consequence, the backbone's output feature maps may activate more strongly for Lambertian objects - in other words, it most likely struggles to 'see' and characterise plates in image samples, but rather other objects present like the wireframe rack, or the incubator surface itself. \\\n",
        "\n",
        "The most obvious way to test this is to initialise the backbone parameters randomly or unfreeze these layers (instead of transfer learning) and optimise them also at each backpropagation. This would be explored in the subsequent analysis.\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "If it is the case that my current dataset is insufficient in size, one potentially efficient way to train the backbone to extract relevant features is to train it on a much larger publicly available transparent object dataset (with labels provided). For example, google's [ClearGrasp Transparent Object Dataset](https://sites.google.com/view/cleargrasp/home) contains 50,000 photorealistic renders of transparents objects (synthetic dataset) and 286 additional real-world images containing labelled transparent objects. The [Real-World Transparent Objects Dataset](https://sites.google.com/view/keypose/?pli=1) provides 48,000 labeled real-world images of transparent objects at 720p resolution.\n",
        ":::\n",
        "\n",
        "\n",
        "### Fixing Evaluation Metric Generation and Plotting:\n",
        "Firstly, in looking back over my code, I notice in get_model_instance_object_detection I have the bounding box confidence score threshold set to 0.0001 - this is strangely low. I cannot remember why I set it to such a low value as the pytorch tutorial that I was following does not do so. Regardless, I change it here to 0.5."
      ],
      "id": "12b19b51"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_model_instance_object_detection(num_class: int) -> fasterrcnn_resnet50_fpn_v2:\n",
        "    # New weights with accuracy 80.858%\n",
        "    weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT # alias is .DEFAULT suffix, weights = None is random initialisation, box MAP 46.7, params, 43.7M, GFLOPS 280.37 https://github.com/pytorch/vision/pull/5763\n",
        "    model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.5)\n",
        "    preprocess = weights.transforms()\n",
        "    # finetuning pretrained model by transfer learning\n",
        "    # get num of input features for classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.SGD( # optimizer defined in model getter according to pytorch 'recipes'\n",
        "        params,\n",
        "        lr=0.005,\n",
        "        momentum=0.9,\n",
        "        weight_decay=0.0005\n",
        "    )\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_class)\n",
        "    return model, optimizer, preprocess"
      ],
      "id": "2dd124de",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It would also be good to check that the dataset_validation isn't empty and loads correctly.\n"
      ],
      "id": "148bfa97"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(len(dataset_validation[1]), '\\n', dataset_validation[1])\n",
        "\n",
        "print(\"\\n -end-\")"
      ],
      "id": "6af11ae0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This raises a red-flag I accepted earlier after applying the preprocess transform before the feed-forward through the network. This is definitely something that needs to be ironed out. The pix\n",
        "\n",
        "When following the [pytorch tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html) when creating the Plate_Image_Dataset class, in the __getitem__ dunder method associated with the class the tutorial applies the transforms passed to the class constructor to both the images and the target (the bounding boxes). I am dubious as to whether I have interpreted this correctly for a few reasons. Firstly, when I tried to do so when writing the class, as such:\n"
      ],
      "id": "a1832c2a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)  # <-- as in the pytorch tutorial"
      ],
      "id": "60bc33bd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I got an error where the transforms function wasn't expecting a tuple. I also got a type error when passing target to the function.\n",
        "\n",
        "To bypass this, I rewrote the line as:"
      ],
      "id": "74ae15c9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "            target[\"boxes\"] = self.transforms(target[\"boxes\"])"
      ],
      "id": "63562e5b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Passing img and target boxes key through to transforms jumped over the error, but I am sure this is causing problems given the bounding box tensor from dataset_validation[1]'s target from the previous print statement holds incredibly small decimal values. \\\n",
        "\n",
        "My rationale at the time of writing the Plate_Image_Dataset was that whatever transforms I apply to the image tensor elements should also be applied to the bounding box coordinates. My thinking was that if they are downscaled prior to feed forward, the boudning boxes must be resized accordingly so that they bound the same, albeit downsized region. The issue with this is that the elements of the img tensor store completely different information to that of the bounding box tensor. The bounding box tensor stores the bounding box coordinates, but img stores pixel intensities. This is obvious in retrospect, but I think I overlooked it as I familiarised myself with pytorch - silly me. \\\n",
        "\n",
        "Resizing of the img and the bounding boxes does indeed need to be coordinated properly as mentioned before, but the need to normalise img's pixel intensities means I need to be very conscious of what transforms are being applied to what. \\\n",
        "\n",
        "Downscaling of the img tensor is actually handled by the ResNet module. What's interesting is that the torchvision transforms are written for images alone and not their bounding boxes (see [here](https://pseudo-lab.github.io/Tutorial-Book-en/chapters/en/object-detection/Ch3-preprocessing.html)), which again suggests this is causing issues in training and validation. \\\n",
        "\n",
        "I am going to visualise what is happening to the image samples and their bounding boxes prior to the feed forward through the network. To do so I will modify the previously defined plot_prediction function to create a new plot_sample function. This function's signature will include a dataset of type Plate_Image_Dataset and an index (type int).\n"
      ],
      "id": "7727bd8d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_sample(dataset, index):\n",
        "    img, target = dataset[index]\n",
        "   \n",
        "    output_image = draw_bounding_boxes(img, target[\"boxes\"], colors=\"red\")\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    plt.imshow(output_image.permute(1, 2, 0)) # permuting images as pytorch is [channels, height, width], matplotlib is [height, width, channels]\n",
        "    plt.savefig(f'{index}.png', bbox_inches='tight') # tight removes whitespace"
      ],
      "id": "62981734",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_sample(dataset_validation, 2)\n"
      ],
      "id": "0b4acd36",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is no red bounding box displayed here. This is most likely because of the transforms being applied by the Plate_Image_Dataset item getter. I will change this now and try again.\n",
        "\n",
        "** Removing Transforms and Adding Insightful Prints: **"
      ],
      "id": "3187d12e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Plate_Image_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, annotations_file: str, img_dir: str, transforms=None):\n",
        "        self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_files = [f for f in glob(img_dir + \"/*.png\")]\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.img_files)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, Dict]:\n",
        "        # Load image\n",
        "        img = read_image(self.img_files[idx])\n",
        "        \n",
        "        # Debug print\n",
        "        print(f\"\\nProcessing image {idx}:\")\n",
        "        print(f\"Image shape: {img.shape}\")\n",
        "        print(f\"Raw coordinates from CSV:\")\n",
        "        print(f\"xmins: {self.img_labels['xmins'][idx]}\")\n",
        "        print(f\"ymins: {self.img_labels['ymins'][idx]}\")\n",
        "        print(f\"xmaxs: {self.img_labels['xmaxs'][idx]}\")\n",
        "        print(f\"ymaxs: {self.img_labels['ymaxs'][idx]}\")\n",
        "\n",
        "        # Extract coordinates\n",
        "        x1 = [int(item) for item in re.findall(r'\\d+', str(self.img_labels['xmins'][idx]))]\n",
        "        y1 = [int(item) for item in re.findall(r'\\d+', str(self.img_labels['ymins'][idx]))]\n",
        "        x2 = [int(item) for item in re.findall(r'\\d+', str(self.img_labels['xmaxs'][idx]))]\n",
        "        y2 = [int(item) for item in re.findall(r'\\d+', str(self.img_labels['ymaxs'][idx]))]\n",
        "\n",
        "        print(f\"Extracted coordinates:\")\n",
        "        print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
        "\n",
        "        # Create boxes tensor\n",
        "        num_objs = len(x1)\n",
        "        boxes = torch.tensor(\n",
        "            [[x1[i], y1[i], x2[i], y2[i]] for i in range(num_objs)],\n",
        "            dtype=torch.float32\n",
        "        )\n",
        "        \n",
        "        print(f\"Boxes tensor before TVTensor:\")\n",
        "        print(boxes)\n",
        "\n",
        "        # Create labels tensor (1 for plate, as 0 is background)\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        # Calculate areas\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "\n",
        "        # Convert image to TVTensor\n",
        "        img = tv_tensors.Image(img)\n",
        "\n",
        "        # Create target dictionary\n",
        "        target = {\n",
        "            \"boxes\": tv_tensors.BoundingBoxes(\n",
        "                boxes,\n",
        "                format=\"XYXY\",\n",
        "                canvas_size=F.get_size(img)\n",
        "            ),\n",
        "            \"labels\": labels,\n",
        "            \"image_id\": idx,\n",
        "            \"area\": area,\n",
        "            \"iscrowd\": torch.zeros((num_objs,), dtype=torch.int64)\n",
        "        }\n",
        "\n",
        "        print(f\"Final boxes in target:\")\n",
        "        print(target[\"boxes\"])\n",
        "\n",
        "        # Apply transforms if any\n",
        "        if self.transforms is not None:\n",
        "            try:\n",
        "                img = self.transforms(img)\n",
        "                # Don't transform boxes directly\n",
        "            except Exception as e:\n",
        "                print(f\"Transform error: {e}\")\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def test_item(self, idx: int):\n",
        "        \"\"\"Helper method to test single item processing\"\"\"\n",
        "        print(f\"\\nTesting item {idx}\")\n",
        "        img, target = self[idx]\n",
        "        print(f\"Image shape: {img.shape}\")\n",
        "        print(f\"Number of boxes: {len(target['boxes'])}\")\n",
        "        print(f\"Boxes: {target['boxes']}\")\n",
        "        print(f\"Labels: {target['labels']}\")\n",
        "        return img, target"
      ],
      "id": "773e7f9d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create dataset\n",
        "dataset = Plate_Image_Dataset(\n",
        "    annotations_file=str(annotations_file),\n",
        "    img_dir=str(img_dir),\n",
        "    transforms=preprocess\n",
        ")\n",
        "\n",
        "# Test a single item\n",
        "dataset.test_item(0)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset_train,\n",
        "    batch_size=1,\n",
        "    shuffle=True,\n",
        "    collate_fn=utils.collate_fn\n",
        ")\n",
        "\n",
        "validation_loader = torch.utils.data.DataLoader(\n",
        "    dataset_validation,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    collate_fn=utils.collate_fn\n",
        ")\n",
        "\n",
        "# Test first batch\n",
        "images, targets = next(iter(train_loader))\n",
        "print(\"\\nFirst batch:\")\n",
        "print(f\"Image shape: {images[0].shape}\")\n",
        "print(f\"Target boxes: {targets[0]['boxes']}\")"
      ],
      "id": "81600871",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This shows the coordinates are being loaded correctly, but they are being messed up by something. This reminded me of an article I read about collate functions and object detection (see [here](https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_e2e.html#sphx-glr-auto-examples-transforms-plot-transforms-e2e-py)). In the data laoding and training loop section, there's a series of comments that states that the default collate function is problematic in object detection and a custom function should be provided instead. This is the problem!\n"
      ],
      "id": "b34a6d8e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def custom_collate_fn(batch):\n",
        "    \"\"\"Custom collate function with debugging\"\"\"\n",
        "    images = []\n",
        "    targets = []\n",
        "    \n",
        "    print(\"\\nInside collate_fn:\")\n",
        "    for i, (img, target) in enumerate(batch):\n",
        "        print(f\"Item {i} boxes before collation:\", target[\"boxes\"])\n",
        "        images.append(img)\n",
        "        \n",
        "        if isinstance(target[\"boxes\"], tv_tensors.BoundingBoxes):\n",
        "            boxes_tensor = target[\"boxes\"].as_tensor()\n",
        "            print(f\"Item {i} boxes after conversion:\", boxes_tensor)\n",
        "            target[\"boxes\"] = boxes_tensor\n",
        "            \n",
        "        targets.append(target)\n",
        "    \n",
        "    print(\"Final collated boxes:\", targets[0][\"boxes\"])\n",
        "    return images, targets"
      ],
      "id": "94501fb7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Then use it in DataLoader:\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset_train,\n",
        "    batch_size=1,\n",
        "    shuffle=True,\n",
        "    collate_fn=custom_collate_fn  # Use custom collate function\n",
        ")\n",
        "\n",
        "validation_loader = torch.utils.data.DataLoader(\n",
        "    dataset_validation,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    collate_fn=custom_collate_fn  # Use custom collate function\n",
        ")\n",
        "\n",
        "    # Test first batch\n",
        "images, targets = next(iter(train_loader))\n",
        "print(\"\\nFirst batch:\")\n",
        "print(f\"Image shape: {images[0].shape}\")\n",
        "print(f\"Target boxes: {targets[0]['boxes']}\")"
      ],
      "id": "9678ccc9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Even after trying to address the collate function issue, the cause of the bounding box issue is even further upstream. This narrows it down to the TV Tensor.\n",
        "\n",
        "Let's look into it! Firstly, I will keep both the bounding boxes and image tensors as pure tensors instead of wrapping them as TVTensor.\n"
      ],
      "id": "d0f82e75"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Plate_Image_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, annotations_file: str, img_dir: str, transforms=None):\n",
        "        self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_files = [f for f in glob(img_dir + \"/*.png\")]\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.img_files)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, Dict]:\n",
        "        # Load image\n",
        "        img = read_image(self.img_files[idx])\n",
        "        print(f\"\\nProcessing image {idx}\")\n",
        "        \n",
        "        # Get raw coordinates\n",
        "        print(\"Raw coordinates from CSV:\")\n",
        "        x1 = [int(item) for item in re.findall(r'\\d+', str(self.img_labels['xmins'][idx]))]\n",
        "        y1 = [int(item) for item in re.findall(r'\\d+', str(self.img_labels['ymins'][idx]))]\n",
        "        x2 = [int(item) for item in re.findall(r'\\d+', str(self.img_labels['xmaxs'][idx]))]\n",
        "        y2 = [int(item) for item in re.findall(r'\\d+', str(self.img_labels['ymaxs'][idx]))]\n",
        "        print(f\"Extracted coordinates: x1={x1}, y1={y1}, x2={x2}, y2={y2}\")\n",
        "\n",
        "        # Create boxes tensor - keeping as regular tensor, not TVTensor\n",
        "        boxes = torch.tensor([[x1[0], y1[0], x2[0], y2[0]]], dtype=torch.float32)\n",
        "        print(\"Boxes after tensor creation:\", boxes)\n",
        "\n",
        "        # Keep image as regular tensor initially, instead of wrapping in TVTensor\n",
        "        target = {\n",
        "            \"boxes\": boxes,  # Regular tensor, not TVTensor\n",
        "            \"labels\": torch.ones((1,), dtype=torch.int64),\n",
        "            \"image_id\": idx,\n",
        "            \"area\": (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),\n",
        "            \"iscrowd\": torch.zeros((1,), dtype=torch.int64)\n",
        "        }\n",
        "\n",
        "        print(\"Final boxes in target:\", target[\"boxes\"])\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "            \n",
        "        return img, target"
      ],
      "id": "abd8c141",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I also want to simplify the custom collate function:"
      ],
      "id": "fb21307c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def custom_collate_fn(batch):\n",
        "    images = []\n",
        "    targets = []\n",
        "    print(\"\\nCollating batch:\")\n",
        "    \n",
        "    for img, target in batch:\n",
        "        print(\"Box coordinates:\", target[\"boxes\"])\n",
        "        images.append(img)\n",
        "        targets.append(target)\n",
        "    \n",
        "    return images, targets"
      ],
      "id": "37f66bcd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testing it:"
      ],
      "id": "5d22334a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create dataset\n",
        "dataset = Plate_Image_Dataset(\n",
        "    annotations_file=str(annotations_file),\n",
        "    img_dir=str(img_dir),\n",
        "    transforms=None  # Let's test without transforms first\n",
        ")\n",
        "\n",
        "# Test single item\n",
        "print(\"\\nTesting single item:\")\n",
        "img, target = dataset[0]\n",
        "print(\"Direct access boxes:\", target[\"boxes\"])\n",
        "\n",
        "# Create loader with minimal batch processing\n",
        "loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    collate_fn=custom_collate_fn\n",
        ")\n",
        "\n",
        "# Test batch\n",
        "print(\"\\nTesting batch:\")\n",
        "batch = next(iter(loader))\n",
        "print(\"Batch boxes:\", batch[1][0][\"boxes\"])"
      ],
      "id": "85f7b546",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great! The problem has been removed. I will now systematically add them back to identify the cause.\n"
      ],
      "id": "65ec6326"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Plate_Image_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, annotations_file: str, img_dir: str, transforms=None):\n",
        "        self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_files = [f for f in glob(img_dir + \"/*.png\")]\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.img_files)\n",
        "        \n",
        "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, Dict]:\n",
        "        img = read_image(self.img_files[idx])\n",
        "        img = tv_tensors.Image(img)  # Convert only image to TVTensor\n",
        "\n",
        "        # Get raw coordinates\n",
        "        print(\"Raw coordinates from CSV:\")\n",
        "        x1 = [int(item) for item in re.findall(r'\\d+', str(self.img_labels['xmins'][idx]))]\n",
        "        y1 = [int(item) for item in re.findall(r'\\d+', str(self.img_labels['ymins'][idx]))]\n",
        "        x2 = [int(item) for item in re.findall(r'\\d+', str(self.img_labels['xmaxs'][idx]))]\n",
        "        y2 = [int(item) for item in re.findall(r'\\d+', str(self.img_labels['ymaxs'][idx]))]\n",
        "        print(f\"Extracted coordinates: x1={x1}, y1={y1}, x2={x2}, y2={y2}\")\n",
        "\n",
        "        # Rest of your coordinate processing...\n",
        "        boxes = torch.tensor([[x1[0], y1[0], x2[0], y2[0]]], dtype=torch.float32)\n",
        "\n",
        "        target = {\n",
        "            \"boxes\": boxes,  # Keep as regular tensor\n",
        "            \"labels\": torch.ones((1,), dtype=torch.int64),\n",
        "            \"image_id\": idx,\n",
        "            \"area\": (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),\n",
        "            \"iscrowd\": torch.zeros((1,), dtype=torch.int64)\n",
        "        }\n",
        "    \n",
        "        return img, target"
      ],
      "id": "e50ff538",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That worked! Now I will try to add transforms back:\n"
      ],
      "id": "b7fc86d3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Plate_Image_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, annotations_file: str, img_dir: str, transforms=None):\n",
        "        self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_files = [f for f in glob(img_dir + \"/*.png\")]\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.img_files)\n",
        "        \n",
        "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, Dict]:\n",
        "        img = read_image(self.img_files[idx])\n",
        "        img = tv_tensors.Image(img)  # Convert only image to TVTensor\n",
        "\n",
        "        # Get raw coordinates\n",
        "        print(\"Raw coordinates from CSV:\")\n",
        "        x1 = [int(item) for item in re.findall(r'\\d+', str(self.img_labels['xmins'][idx]))]\n",
        "        y1 = [int(item) for item in re.findall(r'\\d+', str(self.img_labels['ymins'][idx]))]\n",
        "        x2 = [int(item) for item in re.findall(r'\\d+', str(self.img_labels['xmaxs'][idx]))]\n",
        "        y2 = [int(item) for item in re.findall(r'\\d+', str(self.img_labels['ymaxs'][idx]))]\n",
        "        print(f\"Extracted coordinates: x1={x1}, y1={y1}, x2={x2}, y2={y2}\")\n",
        "\n",
        "        # Rest of your coordinate processing...\n",
        "        boxes = torch.tensor([[x1[0], y1[0], x2[0], y2[0]]], dtype=torch.float32)\n",
        "\n",
        "        target = {\n",
        "            \"boxes\": boxes,  # Keep as regular tensor\n",
        "            \"labels\": torch.ones((1,), dtype=torch.int64),\n",
        "            \"image_id\": idx,\n",
        "            \"area\": (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),\n",
        "            \"iscrowd\": torch.zeros((1,), dtype=torch.int64)\n",
        "        }\n",
        "\n",
        "        # Apply transforms to image only\n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "    \n",
        "        return img, target"
      ],
      "id": "0c742e91",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "transforms = T.Compose([\n",
        "    T.ConvertImageDtype(torch.float32),  # Just convert dtype\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406],  # Standard ImageNet normalization\n",
        "               std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "dataset = Plate_Image_Dataset(\n",
        "    annotations_file=str(annotations_file),\n",
        "    img_dir=str(img_dir),\n",
        "    transforms=transforms\n",
        ")"
      ],
      "id": "01abd377",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "More testing now:"
      ],
      "id": "825d2add"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test the dataset\n",
        "print(\"\\nTesting single item:\")\n",
        "img, target = dataset[0]\n",
        "print(\"Image shape:\", img.shape)\n",
        "print(\"Image dtype:\", img.dtype)\n",
        "print(\"Direct access boxes:\", target[\"boxes\"])\n",
        "\n",
        "# Test with dataloader\n",
        "loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    collate_fn=custom_collate_fn\n",
        ")\n",
        "\n",
        "# Check first batch\n",
        "batch = next(iter(loader))\n",
        "print(\"\\nBatch test:\")\n",
        "print(\"Image shape:\", batch[0][0].shape)\n",
        "print(\"Batch boxes:\", batch[1][0][\"boxes\"])"
      ],
      "id": "b7e4845d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This works! Quickly adding transforms within the model getter to handle normalisation of pixel intensities (according to [ImageNet normalisation](https://discuss.pytorch.org/t/discussion-why-normalise-according-to-imagenet-mean-and-std-dev-for-transfer-learning/115670)).\n"
      ],
      "id": "8972e719"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_model_instance_object_detection(num_class: int):\n",
        "    # New weights with accuracy 80.858%\n",
        "    weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT \n",
        "    model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.5)\n",
        "    \n",
        "    # Don't use the default preprocess from weights\n",
        "    # Instead, use our custom transforms we just tested\n",
        "    transforms = T.Compose([\n",
        "        T.ConvertImageDtype(torch.float32),\n",
        "        T.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ])\n",
        "    \n",
        "    # get num of input features for classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_class)\n",
        "\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.SGD(\n",
        "        params,\n",
        "        lr=0.005,\n",
        "        momentum=0.9,\n",
        "        weight_decay=0.0005\n",
        "    )\n",
        "\n",
        "    return model, optimizer, transforms"
      ],
      "id": "b00e3754",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now I will try to train again for 15 epochs and see if (fingers crossed) the CocoEvaluator class functions correctly now.\n"
      ],
      "id": "5ece7f53"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setup data\n",
        "dataset = Plate_Image_Dataset(\n",
        "    annotations_file=str(annotations_file),\n",
        "    img_dir=str(img_dir),\n",
        "    transforms=transforms  # Using the new transforms \n",
        ")\n",
        "\n",
        "# Split dataset with correct boxes now\n",
        "dataset_size = len(dataset)\n",
        "validation_size = 50  # Or whatever size you want\n",
        "indices = [int(i) for i in torch.randperm(dataset_size).tolist()]\n",
        "\n",
        "dataset_validation = torch.utils.data.Subset(dataset, indices[-validation_size:])\n",
        "dataset_train = torch.utils.data.Subset(dataset, indices[:-validation_size])\n",
        "\n",
        "# Create data loaders with custom_collate_fn\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset_train,\n",
        "    batch_size=1,\n",
        "    shuffle=True,\n",
        "    collate_fn=custom_collate_fn  # Using new collate function\n",
        ")\n",
        "\n",
        "validation_loader = torch.utils.data.DataLoader(\n",
        "    dataset_validation,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    collate_fn=custom_collate_fn  # Using new collate function\n",
        ")\n",
        "\n",
        "# Rest of your training setup\n",
        "model, optimizer, preprocess = get_model_instance_object_detection(num_class=2)\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 15\n",
        "precedent_epoch = 0\n",
        "save_dir = '../'\n",
        "\n",
        "epoch = train(model, train_loader, validation_loader, device, num_epochs, precedent_epoch, save_dir, optimizer)"
      ],
      "id": "a1517a3b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This gave a type error, because the model is expecting float32 images in range [0,1].\n"
      ],
      "id": "6d3a030a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Plate_Image_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, annotations_file: str, img_dir: str, transforms=None):\n",
        "        self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_files = [f for f in glob(img_dir + \"/*.png\")]\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.img_files)\n",
        "    \n",
        "    def load_image(self, idx): # adding this as normalisation will mess up plotting with matplotlib\n",
        "        print(self.img_files)\n",
        "        img = read_image(self.img_files[idx])\n",
        "        return img\n",
        "\n",
        "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, Dict]:\n",
        "        # Load image \n",
        "        img = read_image(self.img_files[idx])\n",
        "        # Convert image to TVTensor\n",
        "        img = tv_tensors.Image(img)\n",
        "        \n",
        "        # Get coordinates\n",
        "        x1 = [int(item) for item in re.findall(r'\\d+', str(self.img_labels['xmins'][idx]))]\n",
        "        y1 = [int(item) for item in re.findall(r'\\d+', str(self.img_labels['ymins'][idx]))]\n",
        "        x2 = [int(item) for item in re.findall(r'\\d+', str(self.img_labels['xmaxs'][idx]))]\n",
        "        y2 = [int(item) for item in re.findall(r'\\d+', str(self.img_labels['ymaxs'][idx]))]\n",
        "        \n",
        "        # Create boxes tensor\n",
        "        num_objs = len(x1)\n",
        "        boxes = torch.tensor(\n",
        "            [[x1[i], y1[i], x2[i], y2[i]] for i in range(num_objs)],\n",
        "            dtype=torch.float32\n",
        "        )\n",
        "        \n",
        "        # Create target dictionary\n",
        "        target = {\n",
        "            \"boxes\": tv_tensors.BoundingBoxes(\n",
        "                boxes,\n",
        "                format=\"XYXY\",\n",
        "                canvas_size=F.get_size(img)\n",
        "            ),\n",
        "            \"labels\": torch.ones((num_objs,), dtype=torch.int64),\n",
        "            \"image_id\": idx,\n",
        "            \"area\": (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),\n",
        "            \"iscrowd\": torch.zeros((num_objs,), dtype=torch.int64)\n",
        "        }\n",
        "\n",
        "        # Apply transforms only to img\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "        \n",
        "        return img, target"
      ],
      "id": "63af7faa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_sample(dataset, index):\n",
        "    _, target = dataset[index]\n",
        "    img = dataset.load_image(index)\n",
        "   \n",
        "    output_image = draw_bounding_boxes(img, target[\"boxes\"], colors=\"red\")\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    plt.imshow(output_image.permute(1, 2, 0)) # permuting images as pytorch is [channels, height, width], matplotlib is [height, width, channels]"
      ],
      "id": "f9410e19",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Just check the images are being transformed properly before feed forward by plot:"
      ],
      "id": "ea3a8a17"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setup data\n",
        "model, optimizer, transforms = get_model_instance_object_detection(2)\n",
        "dataset = Plate_Image_Dataset(\n",
        "    annotations_file=str(annotations_file),\n",
        "    img_dir=str(img_dir),\n",
        "    transforms=transforms  # Using the new transforms \n",
        ")\n",
        "print(dataset[2])\n",
        "plot_sample(dataset, 2)"
      ],
      "id": "319e0239",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This revealed another problem - the image being loaded does not correspond correctly to the bounding box coordinates. This is strange as the image at line 4 (index 2) of [labels.csv](../lib/labels.csv) has the same coordinates as the at index 2 of the dataset. The associated image 1_3_0916162509.png clearly is concordant with the bounding box coordinates produced by dataset[2], but is not the one that is loaded at that index in the dataset. \\\n",
        "\n",
        "I will fix this by sorting glob, just as in xml_to_csv in [analysis 0001](analyses/0001_dataset_creation.md):\n"
      ],
      "id": "0b2aaf3f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Plate_Image_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, annotations_file: str, img_dir: str, transforms=None):\n",
        "        self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_files = [f for f in sorted(glob(img_dir + \"/*.png\"))]\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.img_files)\n",
        "    \n",
        "    def load_image(self, idx): # adding this as normalisation will mess up plotting with matplotlib\n",
        "        print(self.img_files)\n",
        "        img = read_image(self.img_files[idx])\n",
        "        return img\n",
        "\n",
        "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, Dict]:\n",
        "        # Load image \n",
        "        img = read_image(self.img_files[idx])\n",
        "        # Convert image to TVTensor\n",
        "        img = tv_tensors.Image(img)\n",
        "        \n",
        "        # Get coordinates\n",
        "        x1 = [int(item) for item in re.findall(r'\\d+', str(self.img_labels['xmins'][idx]))]\n",
        "        y1 = [int(item) for item in re.findall(r'\\d+', str(self.img_labels['ymins'][idx]))]\n",
        "        x2 = [int(item) for item in re.findall(r'\\d+', str(self.img_labels['xmaxs'][idx]))]\n",
        "        y2 = [int(item) for item in re.findall(r'\\d+', str(self.img_labels['ymaxs'][idx]))]\n",
        "        \n",
        "        # Create boxes tensor\n",
        "        num_objs = len(x1)\n",
        "        boxes = torch.tensor(\n",
        "            [[x1[i], y1[i], x2[i], y2[i]] for i in range(num_objs)],\n",
        "            dtype=torch.float32\n",
        "        )\n",
        "        \n",
        "        # Create target dictionary\n",
        "        target = {\n",
        "            \"boxes\": tv_tensors.BoundingBoxes(\n",
        "                boxes,\n",
        "                format=\"XYXY\",\n",
        "                canvas_size=F.get_size(img)\n",
        "            ),\n",
        "            \"labels\": torch.ones((num_objs,), dtype=torch.int64),\n",
        "            \"image_id\": idx,\n",
        "            \"area\": (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),\n",
        "            \"iscrowd\": torch.zeros((num_objs,), dtype=torch.int64)\n",
        "        }\n",
        "\n",
        "        # Apply transforms only to img\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "        \n",
        "        return img, target"
      ],
      "id": "e31386b1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotting again:"
      ],
      "id": "c9f0748a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setup data\n",
        "model, optimizer, transforms = get_model_instance_object_detection(2)\n",
        "dataset = Plate_Image_Dataset(\n",
        "    annotations_file=str(annotations_file),\n",
        "    img_dir=str(img_dir),\n",
        "    transforms=transforms  # Using the new transforms \n",
        ")\n",
        "print(dataset[7])\n",
        "plot_sample(dataset, 7)"
      ],
      "id": "66f2b64f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is now working! The images and associated bounding boxes are correctly ordered.\n",
        "\n",
        "Now trying to run the training script again with all these changes integrated together:"
      ],
      "id": "b14afc58"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import caffeine \n",
        "\n",
        "# Setup data\n",
        "dataset = Plate_Image_Dataset(\n",
        "    annotations_file=str(annotations_file),\n",
        "    img_dir=str(img_dir),\n",
        "    transforms=transforms  # Using the new transforms \n",
        ")\n",
        "\n",
        "# Split dataset with correct boxes now\n",
        "dataset_size = len(dataset)\n",
        "validation_size = 50  # Or whatever size you want\n",
        "indices = [int(i) for i in torch.randperm(dataset_size).tolist()]\n",
        "\n",
        "dataset_validation = torch.utils.data.Subset(dataset, indices[-validation_size:])\n",
        "dataset_train = torch.utils.data.Subset(dataset, indices[:-validation_size])\n",
        "\n",
        "# Create data loaders with custom_collate_fn\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset_train,\n",
        "    batch_size=1,\n",
        "    shuffle=True,\n",
        "    collate_fn=custom_collate_fn  # Using new collate function\n",
        ")\n",
        "\n",
        "validation_loader = torch.utils.data.DataLoader(\n",
        "    dataset_validation,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    collate_fn=custom_collate_fn  # Using new collate function\n",
        ")\n",
        "\n",
        "# Rest of your training setup\n",
        "model, optimizer, preprocess = get_model_instance_object_detection(num_class=2)\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 15\n",
        "precedent_epoch = 0\n",
        "save_dir = '../'\n",
        "\n",
        "epoch = train(model, train_loader, validation_loader, device, num_epochs, precedent_epoch, save_dir, optimizer)"
      ],
      "id": "7e4cfce5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These results will be stored in results/analysis_0002_15epochs.\n"
      ],
      "id": "a60adbf1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "save_dir = '/Users/cla24mas/Documents/SC_TSL_15092024_plate_detect/checkpoints/'\n",
        "plot_prediction(model, dataset, device, 3, save_dir, 'checkpoint_epoch_9')"
      ],
      "id": "8ef45fe4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This produced a 'tuple object is not callable error'.\n"
      ],
      "id": "37bdb314"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_prediction(model, dataset, device, index, save_dir: str, model_name):\n",
        "    # Get image and target from dataset\n",
        "    img, target = dataset[index]\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        image = img[:3, ...].to(device) # take the first 3 elements/channels (RGB) - leave the rest as the same (the '...')\n",
        "        predictions = model([image])\n",
        "        pred = predictions[0]\n",
        "    \n",
        "    # Move image back to CPU for visualization\n",
        "    image = image.cpu()\n",
        "    \n",
        "    # Normalize each channel independently\n",
        "    normalized_image = torch.zeros_like(image)\n",
        "    for c in range(3):\n",
        "        channel = image[c]\n",
        "        min_val = channel.min()\n",
        "        max_val = channel.max()\n",
        "        if max_val > min_val:\n",
        "            normalized_image[c] = (channel - min_val) / (max_val - min_val)\n",
        "    \n",
        "    # Convert to 8-bit format for visualization\n",
        "    image_uint8 = (normalized_image * 255).byte()\n",
        "    \n",
        "    # Get predicted boxes\n",
        "    pred_boxes = pred[\"boxes\"].cpu().long()\n",
        "    \n",
        "    # Draw boxes on image\n",
        "    output_image = torchvision.utils.draw_bounding_boxes(\n",
        "        image_uint8,\n",
        "        pred_boxes,\n",
        "        colors=\"red\",\n",
        "        width=3\n",
        "    )\n",
        "    \n",
        "    # Convert tensor to numpy and ensure proper format for matplotlib\n",
        "    output_image = output_image.permute(1, 2, 0).numpy()\n",
        "    \n",
        "    # Create figure and display\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    plt.imshow(output_image)\n",
        "    plt.axis('off')\n",
        "    \n",
        "    # Save the figure\n",
        "    save_path = f'{save_dir}/prediction_normalized_{index}.png'\n",
        "    plt.savefig(save_path, bbox_inches='tight', pad_inches=0, dpi=300)\n",
        "    print(f\"Saved normalized prediction to: {save_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    return output_image"
      ],
      "id": "abdcf5d0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "save_dir = '/Users/cla24mas/Documents/SC_TSL_15092024_plate_detect/results/0002_predictions/'\n",
        "load_dir = '/Users/cla24mas/Documents/SC_TSL_15092024_plate_detect/checkpoints/'\n",
        "model, _, _ = load_model(load_dir, 2, 'checkpoint_epoch_9')\n",
        "model.eval()  # Set model to evaluation mode\n",
        "for i in range(0, validation_size):\n",
        "    plot_prediction(model, dataset_validation, device, i, save_dir, 'checkpoint_epoch_9')"
      ],
      "id": "943ee195",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluation Metrics:\n",
        "\n",
        "![evaluation metrics](../results/0002_15_epochs/02_fix/evaluation_metrics_epochs_0-15.png)\n",
        "\n",
        "Example prediction:\n",
        "\n",
        "![example prediction](../results/0002_15_epochs/02_fix/holdout_predictions/prediction_normalized_24.png)\n",
        "\n",
        "To validate these results, see [analysis 0003](0003_positive_control.md)\n"
      ],
      "id": "2658111d"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/cla24mas/Documents/SC_TSL_15092024_plate_detect/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}