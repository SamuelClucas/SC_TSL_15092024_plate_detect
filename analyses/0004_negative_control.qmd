---
title: "Implementing a Negative Control"
date: 07-11-2024
date-format: short
execute:
    error: true
---
## Introduction
To complement the positive control from [analysis 0003](0003_positive_control.md), I will use the same model I trained in [analysis 0002](0002_functional_Faster_R-CNN.md) on the [plate image dataset](../raw/positives/) and evaluate it on the [Pennfudan dataset](../raw/pos_control/). The expectation is that it performs poorly, given pedestrians and plates are sufficiently distinct that a model trained to detect incubator plates should classify them as background. This would doubly confirm the model and its associated code is behaving as expected.

## Implementation

### Imports
```{python}
from plate_detect import helper_training_functions
import torchvision_deps
from torchvision.ops.boxes import masks_to_boxes
import os
import numpy as np
import pandas as pd
from torchvision.io import read_image
import torch
from torchvision.transforms.v2 import functional as F
from torchvision import tv_tensors
from typing import Dict
import torchvision_deps.T_and_utils
```

```{python}
root_dir = '/Users/cla24mas/Documents/SC_TSL_15092024_plate_detect/'
model, _, _ = helper_training_functions.load_model(root_dir, 2, 'checkpoint_epoch_9')
model.eval()  # Set model to evaluation mode
for i in range(0, validation_size):
    plot_prediction(model, dataset_test, device, i, root_dir, 'checkpoint_epoch_9')
```

Taking the evaluation code out of helper_training_functions.train() to generate evaluation metrics, again evaluating its performance on 50 samples, but fromthe PennFudan dataset instead:
```{python}
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

model, optimizer, transforms = helper_training_functions.get_model_instance_object_detection(2)

dataset_test = PennFudanDataset('/Users/cla24mas/Documents/SC_TSL_15092024_plate_detect/raw/pos_control/PennFudanPed', transforms)

# split the dataset in train and test set
indices = torch.randperm(len(dataset_test)).tolist()

dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])

data_loader_test = torch.utils.data.DataLoader(
    dataset_test,
    batch_size=1,
    shuffle=False,
    collate_fn=torchvision_deps.T_and_utils.utils.collate_fn
)

epoch=10
evaluation_across_epochs = defaultdict(list)

eval_metrics = evaluate_model(model, data_loader_test, device)
evaluation_across_epochs[f'{epoch}'] = eval_metrics

plot_eval_metrics(save_dir, precedent_epoch, num_epochs, title="Evaluation Metrics at Epoch 10", **{k: v for k, v in evaluation_across_epochs.items()}) # remember models are 0-indexed, so checkpoint_epoch_9 corresponds to the 10th epoch of training
```

As you can see in both the stdout from CocoEvaluator, as well as the plot itself, the model performed extremely poorly as expected (almost completely imprecise and unable to find the ground-truth bounding boxes at IoU thresholds >=0.50):

IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.001
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.004
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.001
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.003
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.001
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.003
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.016
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.016
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.018
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.016
